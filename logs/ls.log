[INFO][2021-06-08 20:41:00,541][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-08 20:41:00,916][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-08 20:41:00,934][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-08 20:41:00,934][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-08 20:41:00,935][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-08 20:41:00,935][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-08 20:41:00,936][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-08 20:41:02,123][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51990.
[INFO][2021-06-08 20:41:02,152][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-08 20:41:02,176][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-08 20:41:02,181][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-08 20:41:02,182][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-08 20:41:02,190][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-495ff1f3-aa01-403c-affe-e33d0b5369b4
[INFO][2021-06-08 20:41:02,204][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-08 20:41:02,240][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-08 20:41:02,303][org.spark_project.jetty.util.log:192] - Logging initialized @3491ms
[INFO][2021-06-08 20:41:02,352][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-08 20:41:02,363][org.spark_project.jetty.server.Server:403] - Started @3552ms
[INFO][2021-06-08 20:41:02,383][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@8828a70{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-08 20:41:02,384][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-08 20:41:02,404][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e094740{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,405][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@780ec4a5{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,405][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f70f32f{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,406][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72c927f1{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,407][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3dd69f5a{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,407][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ee4730{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,408][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5003041b{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,409][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23a9ba52{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,409][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@70ab80e3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,410][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@67427b69{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,411][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@544630b7{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,412][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1095f122{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,413][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3d6300e8{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,413][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24a1c17f{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,414][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73511076{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,415][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@532721fd{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,416][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7fb9f71f{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,417][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51f49060{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,418][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@617fe9e1{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,418][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1cf2fed4{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,422][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@245a26e1{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,423][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46ab18da{/,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,424][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42257bdd{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,425][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@70925b45{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,426][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@aa22f1c{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,428][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://10.168.88.20:4040
[INFO][2021-06-08 20:41:02,482][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-08 20:41:02,499][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52004.
[INFO][2021-06-08 20:41:02,500][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 10.168.88.20:52004
[INFO][2021-06-08 20:41:02,502][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-08 20:41:02,503][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 10.168.88.20, 52004, None)
[INFO][2021-06-08 20:41:02,505][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 10.168.88.20:52004 with 3.8 GB RAM, BlockManagerId(driver, 10.168.88.20, 52004, None)
[INFO][2021-06-08 20:41:02,508][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 10.168.88.20, 52004, None)
[INFO][2021-06-08 20:41:02,509][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 10.168.88.20, 52004, None)
[INFO][2021-06-08 20:41:02,655][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33a630fa{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,690][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-08 20:41:02,707][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-08 20:41:02,707][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-08 20:41:02,712][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4159e81b{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,713][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23cd4ff2{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,714][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a94b64e{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,714][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@12477988{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:02,716][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@d400943{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:41:03,211][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-08 20:41:03,709][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-08 20:41:03,748][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-08 20:41:04,511][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-08 20:41:04,595][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/ec2f1eb8-9d13-4fa0-9632-e162b787851d_resources
[INFO][2021-06-08 20:41:04,602][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/ec2f1eb8-9d13-4fa0-9632-e162b787851d
[INFO][2021-06-08 20:41:04,606][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/ec2f1eb8-9d13-4fa0-9632-e162b787851d
[INFO][2021-06-08 20:41:04,610][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/ec2f1eb8-9d13-4fa0-9632-e162b787851d/_tmp_space.db
[INFO][2021-06-08 20:41:04,614][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-08 20:41:04,751][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/99e26cf3-b921-4c38-bc46-58271ec3f183_resources
[INFO][2021-06-08 20:41:04,754][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/99e26cf3-b921-4c38-bc46-58271ec3f183
[INFO][2021-06-08 20:41:04,757][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/99e26cf3-b921-4c38-bc46-58271ec3f183
[INFO][2021-06-08 20:41:04,761][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/99e26cf3-b921-4c38-bc46-58271ec3f183/_tmp_space.db
[INFO][2021-06-08 20:41:04,762][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-08 20:41:04,801][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-08 20:41:04,807][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: myhive.parquet_test1
[INFO][2021-06-08 20:41:04,971][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:41:04,977][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:41:04,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-08 20:41:05,107][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:41:05,107][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:41:05,216][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-08 20:41:05,221][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@8828a70{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-08 20:41:05,223][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://10.168.88.20:4040
[INFO][2021-06-08 20:41:05,234][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-08 20:41:05,241][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-08 20:41:05,241][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-08 20:41:05,248][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-08 20:41:05,252][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-08 20:41:05,255][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-08 20:41:05,256][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-08 20:41:05,257][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-2f694954-a8fe-485a-82dc-fd06372f1b2c
[INFO][2021-06-08 20:43:59,923][org.apache.spark.SparkContext:54] - Running Spark version 2.1.1
[INFO][2021-06-08 20:44:00,218][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-08 20:44:00,221][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-08 20:44:00,222][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-08 20:44:00,223][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-08 20:44:00,224][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-08 20:44:01,504][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51998.
[INFO][2021-06-08 20:44:01,521][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-08 20:44:01,533][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-08 20:44:01,536][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-08 20:44:01,537][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-08 20:44:01,549][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-af6144ca-bef1-430b-9181-f2ff9c038de3
[INFO][2021-06-08 20:44:01,560][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-08 20:44:01,597][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-08 20:44:01,663][org.spark_project.jetty.util.log:186] - Logging initialized @3750ms
[INFO][2021-06-08 20:44:01,738][org.spark_project.jetty.server.Server:327] - jetty-9.2.z-SNAPSHOT
[INFO][2021-06-08 20:44:01,755][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@71b3bc45{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,755][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@a8c1f44{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,755][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,756][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3c435123{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,756][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@50fe837a{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,756][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3a62c01e{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,757][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@7a8fa663{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,757][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5ce33a58{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,758][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@78a287ed{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,758][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@546ccad7{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,758][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5357c287{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,759][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@1623134f{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,759][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@7a527389{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,759][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@485a3466{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,760][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@25748410{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,760][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@2b43529a{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,760][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4264b240{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,760][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5b04476e{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,760][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5ad10c1a{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,761][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@6bb75258{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,765][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@c260bdc{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,765][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@75e01201{/,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,766][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@2783717b{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,766][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@76f7d241{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,767][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4a335fa8{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:01,773][org.spark_project.jetty.server.ServerConnector:266] - Started Spark@74cadd41{HTTP/1.1}{0.0.0.0:4040}
[INFO][2021-06-08 20:44:01,773][org.spark_project.jetty.server.Server:379] - Started @3862ms
[INFO][2021-06-08 20:44:01,774][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-08 20:44:01,776][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://10.168.88.20:4040
[INFO][2021-06-08 20:44:01,838][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-08 20:44:01,887][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52039.
[INFO][2021-06-08 20:44:01,888][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 10.168.88.20:52039
[INFO][2021-06-08 20:44:01,890][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-08 20:44:01,892][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 10.168.88.20, 52039, None)
[INFO][2021-06-08 20:44:01,895][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 10.168.88.20:52039 with 3.8 GB RAM, BlockManagerId(driver, 10.168.88.20, 52039, None)
[INFO][2021-06-08 20:44:01,899][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 10.168.88.20, 52039, None)
[INFO][2021-06-08 20:44:01,899][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 10.168.88.20, 52039, None)
[INFO][2021-06-08 20:44:02,038][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5949eba8{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:02,070][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://192.168.52.100:8020/user/hive/warehouse'.
[INFO][2021-06-08 20:44:02,074][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@1e53135d{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:02,075][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3a7704c{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:02,075][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4acf72b6{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:02,076][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3301500b{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:02,077][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3a45c42a{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:44:02,123][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-08 20:44:02,270][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
[INFO][2021-06-08 20:44:02,271][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
[INFO][2021-06-08 20:44:02,271][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
[INFO][2021-06-08 20:44:02,272][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
[INFO][2021-06-08 20:44:02,273][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
[INFO][2021-06-08 20:44:02,273][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
[INFO][2021-06-08 20:44:02,274][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
[INFO][2021-06-08 20:44:02,274][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
[INFO][2021-06-08 20:44:02,529][hive.metastore:376] - Trying to connect to metastore with URI thrift://192.168.52.100:9083
[INFO][2021-06-08 20:44:02,575][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-08 20:44:03,363][org.apache.hadoop.hdfs.BlockReaderLocal:69] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-08 20:44:03,466][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/4a88e25b-3b11-429e-96e6-bd2643119efb_resources
[INFO][2021-06-08 20:44:03,478][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/4a88e25b-3b11-429e-96e6-bd2643119efb
[INFO][2021-06-08 20:44:03,486][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/4a88e25b-3b11-429e-96e6-bd2643119efb
[INFO][2021-06-08 20:44:03,492][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/4a88e25b-3b11-429e-96e6-bd2643119efb/_tmp_space.db
[INFO][2021-06-08 20:44:03,497][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://192.168.52.100:8020/user/hive/warehouse
[INFO][2021-06-08 20:44:03,615][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from myhive.parquet_test1
[INFO][2021-06-08 20:44:04,306][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:44:04,317][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:44:06,357][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-08 20:44:06,360][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
[INFO][2021-06-08 20:44:06,361][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<id: int, name: string>
[INFO][2021-06-08 20:44:06,362][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pushed Filters: 
[INFO][2021-06-08 20:44:07,041][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 307.388 ms
[INFO][2021-06-08 20:44:07,187][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 145.9 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:07,535][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.5 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:07,540][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 10.168.88.20:52039 (size: 16.5 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:07,545][org.apache.spark.SparkContext:54] - Created broadcast 0 from show at JavaParquetOverwrite.java:18
[INFO][2021-06-08 20:44:07,566][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-08 20:44:07,657][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:18
[INFO][2021-06-08 20:44:07,685][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at JavaParquetOverwrite.java:18) with 1 output partitions
[INFO][2021-06-08 20:44:07,686][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:44:07,687][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:44:07,689][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:44:07,699][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18), which has no missing parents
[INFO][2021-06-08 20:44:07,800][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:07,821][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:07,822][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 10.168.88.20:52039 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:07,824][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:44:07,830][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:44:07,833][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
[INFO][2021-06-08 20:44:07,925][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 6496 bytes)
[INFO][2021-06-08 20:44:07,941][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-08 20:44:08,065][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:44:09,016][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:09,985][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:10,060][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1517 bytes result sent to driver
[INFO][2021-06-08 20:44:10,079][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 2201 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:44:10,080][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:44:10,085][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at JavaParquetOverwrite.java:18) finished in 2.228 s
[INFO][2021-06-08 20:44:10,091][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at JavaParquetOverwrite.java:18, took 2.432887 s
[INFO][2021-06-08 20:44:10,098][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:18
[INFO][2021-06-08 20:44:10,100][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at JavaParquetOverwrite.java:18) with 1 output partitions
[INFO][2021-06-08 20:44:10,100][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:44:10,100][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:44:10,100][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:44:10,101][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18), which has no missing parents
[INFO][2021-06-08 20:44:10,103][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,110][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,112][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 10.168.88.20:52039 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,113][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:44:10,114][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:44:10,114][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
[INFO][2021-06-08 20:44:10,116][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 1, ANY, 6503 bytes)
[INFO][2021-06-08 20:44:10,117][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
[INFO][2021-06-08 20:44:10,124][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0_copy_1, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:44:10,133][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:10,140][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:10,145][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1348 bytes result sent to driver
[INFO][2021-06-08 20:44:10,153][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 38 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:44:10,153][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:44:10,153][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at JavaParquetOverwrite.java:18) finished in 0.039 s
[INFO][2021-06-08 20:44:10,154][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at JavaParquetOverwrite.java:18, took 0.055597 s
[INFO][2021-06-08 20:44:10,181][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 13.6998 ms
[INFO][2021-06-08 20:44:10,202][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: desc adp_bas.info_this_jjjz_etf
[INFO][2021-06-08 20:44:10,285][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:44:10,286][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:44:10,286][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:44:10,287][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:44:10,288][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-08 20:44:10,289][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:44:10,290][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:44:10,291][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:44:10,291][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:44:10,292][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:44:10,292][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:44:10,293][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:44:10,295][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:44:10,295][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:44:10,296][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:44:10,297][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-08 20:44:10,298][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-08 20:44:10,299][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:44:10,300][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:44:10,300][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:44:10,301][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:44:10,301][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:44:10,302][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:44:10,302][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-08 20:44:10,303][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-08 20:44:10,303][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-08 20:44:10,365][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
[INFO][2021-06-08 20:44:10,366][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 2
[INFO][2021-06-08 20:44:10,366][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 1
[INFO][2021-06-08 20:44:10,379][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:19
[INFO][2021-06-08 20:44:10,380][org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (show at JavaParquetOverwrite.java:19) with 1 output partitions
[INFO][2021-06-08 20:44:10,380][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at JavaParquetOverwrite.java:19)
[INFO][2021-06-08 20:44:10,380][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:44:10,380][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:44:10,381][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[5] at show at JavaParquetOverwrite.java:19), which has no missing parents
[INFO][2021-06-08 20:44:10,384][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 10.168.88.20:52039 in memory (size: 16.5 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,387][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 10.168.88.20:52039 in memory (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,389][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 10.168.88.20:52039 in memory (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,392][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 4.6 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,438][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,439][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 10.168.88.20:52039 (size: 2.6 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,439][org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:44:10,439][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at show at JavaParquetOverwrite.java:19)
[INFO][2021-06-08 20:44:10,439][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
[INFO][2021-06-08 20:44:10,446][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7403 bytes)
[INFO][2021-06-08 20:44:10,447][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
[INFO][2021-06-08 20:44:10,480][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.3762 ms
[INFO][2021-06-08 20:44:10,485][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1663 bytes result sent to driver
[INFO][2021-06-08 20:44:10,487][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 47 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:44:10,488][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:44:10,488][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at JavaParquetOverwrite.java:19) finished in 0.048 s
[INFO][2021-06-08 20:44:10,488][org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: show at JavaParquetOverwrite.java:19, took 0.109675 s
[INFO][2021-06-08 20:44:10,499][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.3159 ms
[INFO][2021-06-08 20:44:10,501][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from myhive.parquet_test1
[INFO][2021-06-08 20:44:10,526][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:44:10,526][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:44:10,545][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-08 20:44:10,545][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
[INFO][2021-06-08 20:44:10,545][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<id: int, name: string>
[INFO][2021-06-08 20:44:10,546][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pushed Filters: 
[INFO][2021-06-08 20:44:10,570][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 145.9 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,578][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.5 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,580][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 10.168.88.20:52039 (size: 16.5 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,580][org.apache.spark.SparkContext:54] - Created broadcast 4 from show at JavaParquetOverwrite.java:20
[INFO][2021-06-08 20:44:10,581][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-08 20:44:10,588][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:20
[INFO][2021-06-08 20:44:10,588][org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (show at JavaParquetOverwrite.java:20) with 1 output partitions
[INFO][2021-06-08 20:44:10,589][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:44:10,589][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:44:10,589][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:44:10,589][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20), which has no missing parents
[INFO][2021-06-08 20:44:10,591][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,594][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,595][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 10.168.88.20:52039 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,596][org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:44:10,596][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:44:10,596][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
[INFO][2021-06-08 20:44:10,598][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 6496 bytes)
[INFO][2021-06-08 20:44:10,598][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 3)
[INFO][2021-06-08 20:44:10,600][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:44:10,608][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:10,614][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:10,616][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 3). 1348 bytes result sent to driver
[INFO][2021-06-08 20:44:10,619][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 3) in 22 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:44:10,620][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:44:10,620][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at JavaParquetOverwrite.java:20) finished in 0.023 s
[INFO][2021-06-08 20:44:10,620][org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: show at JavaParquetOverwrite.java:20, took 0.032408 s
[INFO][2021-06-08 20:44:10,622][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:20
[INFO][2021-06-08 20:44:10,622][org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (show at JavaParquetOverwrite.java:20) with 1 output partitions
[INFO][2021-06-08 20:44:10,623][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:44:10,623][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:44:10,623][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:44:10,623][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20), which has no missing parents
[INFO][2021-06-08 20:44:10,624][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,627][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:44:10,628][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_6_piece0 in memory on 10.168.88.20:52039 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:44:10,629][org.apache.spark.SparkContext:54] - Created broadcast 6 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:44:10,629][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:44:10,629][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 1 tasks
[INFO][2021-06-08 20:44:10,630][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 1, ANY, 6503 bytes)
[INFO][2021-06-08 20:44:10,630][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 4)
[INFO][2021-06-08 20:44:10,633][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0_copy_1, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:44:10,639][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:10,643][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:44:10,645][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 4). 1427 bytes result sent to driver
[INFO][2021-06-08 20:44:10,646][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 4) in 17 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:44:10,646][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:44:10,646][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (show at JavaParquetOverwrite.java:20) finished in 0.017 s
[INFO][2021-06-08 20:44:10,647][org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: show at JavaParquetOverwrite.java:20, took 0.024634 s
[INFO][2021-06-08 20:44:10,650][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-08 20:44:10,653][org.spark_project.jetty.server.ServerConnector:306] - Stopped Spark@74cadd41{HTTP/1.1}{0.0.0.0:4040}
[INFO][2021-06-08 20:44:10,654][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@4a335fa8{/stages/stage/kill,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,655][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@76f7d241{/jobs/job/kill,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,655][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@2783717b{/api,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,655][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@75e01201{/,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,655][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@c260bdc{/static,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,655][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@6bb75258{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,656][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5ad10c1a{/executors/threadDump,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,656][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5b04476e{/executors/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,656][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@4264b240{/executors,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,657][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@2b43529a{/environment/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,657][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@25748410{/environment,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,657][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@485a3466{/storage/rdd/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,658][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@7a527389{/storage/rdd,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,658][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@1623134f{/storage/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,659][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5357c287{/storage,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,659][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@546ccad7{/stages/pool/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,659][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@78a287ed{/stages/pool,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,659][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5ce33a58{/stages/stage/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,659][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@7a8fa663{/stages/stage,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,659][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@3a62c01e{/stages/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,659][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@50fe837a{/stages,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,660][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@3c435123{/jobs/job/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,660][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@150ab4ed{/jobs/job,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,660][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@a8c1f44{/jobs/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,660][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@71b3bc45{/jobs,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:44:10,661][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://10.168.88.20:4040
[INFO][2021-06-08 20:44:10,669][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-08 20:44:10,681][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-08 20:44:10,682][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-08 20:44:10,683][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-08 20:44:10,685][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-08 20:44:10,688][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-08 20:44:10,688][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-08 20:44:10,688][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-4d25f59e-e757-4f52-9289-b90428b3c626
[INFO][2021-06-08 20:45:12,058][org.apache.spark.SparkContext:54] - Running Spark version 2.1.1
[INFO][2021-06-08 20:45:12,349][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-08 20:45:12,352][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-08 20:45:12,352][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-08 20:45:12,353][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-08 20:45:12,353][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-08 20:45:13,517][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 61083.
[INFO][2021-06-08 20:45:13,557][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-08 20:45:13,590][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-08 20:45:13,595][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-08 20:45:13,596][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-08 20:45:13,618][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-0d4448a1-53fb-44dc-aca5-79b042fe37b7
[INFO][2021-06-08 20:45:13,643][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-08 20:45:13,726][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-08 20:45:13,862][org.spark_project.jetty.util.log:186] - Logging initialized @3482ms
[INFO][2021-06-08 20:45:13,978][org.spark_project.jetty.server.Server:327] - jetty-9.2.z-SNAPSHOT
[INFO][2021-06-08 20:45:14,004][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4c37b5b{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,005][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@73db4768{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,005][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@71b3bc45{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,006][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@a8c1f44{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,006][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@150ab4ed{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,006][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3c435123{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,007][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@50fe837a{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,008][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3a62c01e{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,008][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@7a8fa663{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,008][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5ce33a58{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,009][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@78a287ed{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,009][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@546ccad7{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,010][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5357c287{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,010][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@1623134f{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,011][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@7a527389{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,011][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@485a3466{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,011][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@25748410{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,012][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@2b43529a{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,012][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4264b240{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,013][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5b04476e{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,020][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5ad10c1a{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,021][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@6bb75258{/,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,021][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@c260bdc{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,021][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@75e01201{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,022][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@2783717b{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,030][org.spark_project.jetty.server.ServerConnector:266] - Started Spark@62fad19{HTTP/1.1}{0.0.0.0:4040}
[INFO][2021-06-08 20:45:14,031][org.spark_project.jetty.server.Server:379] - Started @3654ms
[INFO][2021-06-08 20:45:14,031][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-08 20:45:14,034][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://10.168.88.20:4040
[INFO][2021-06-08 20:45:14,095][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-08 20:45:14,123][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61125.
[INFO][2021-06-08 20:45:14,124][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 10.168.88.20:61125
[INFO][2021-06-08 20:45:14,126][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-08 20:45:14,128][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 10.168.88.20, 61125, None)
[INFO][2021-06-08 20:45:14,131][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 10.168.88.20:61125 with 3.8 GB RAM, BlockManagerId(driver, 10.168.88.20, 61125, None)
[INFO][2021-06-08 20:45:14,133][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 10.168.88.20, 61125, None)
[INFO][2021-06-08 20:45:14,134][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 10.168.88.20, 61125, None)
[INFO][2021-06-08 20:45:14,271][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@c074c0c{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,301][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://192.168.52.100:8020/user/hive/warehouse'.
[INFO][2021-06-08 20:45:14,305][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@6ca320ab{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,306][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@1e53135d{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,306][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@323e8306{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,307][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4acf72b6{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,308][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@57a4d5ee{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-08 20:45:14,350][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-08 20:45:14,468][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
[INFO][2021-06-08 20:45:14,468][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
[INFO][2021-06-08 20:45:14,469][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
[INFO][2021-06-08 20:45:14,469][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
[INFO][2021-06-08 20:45:14,469][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
[INFO][2021-06-08 20:45:14,470][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
[INFO][2021-06-08 20:45:14,470][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
[INFO][2021-06-08 20:45:14,470][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
[INFO][2021-06-08 20:45:14,666][hive.metastore:376] - Trying to connect to metastore with URI thrift://192.168.52.100:9083
[INFO][2021-06-08 20:45:14,697][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-08 20:45:15,398][org.apache.hadoop.hdfs.BlockReaderLocal:69] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-08 20:45:15,476][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/5c6124af-dcc3-4476-ad5a-45991f33fcad_resources
[INFO][2021-06-08 20:45:15,484][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/5c6124af-dcc3-4476-ad5a-45991f33fcad
[INFO][2021-06-08 20:45:15,489][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/5c6124af-dcc3-4476-ad5a-45991f33fcad
[INFO][2021-06-08 20:45:15,492][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/5c6124af-dcc3-4476-ad5a-45991f33fcad/_tmp_space.db
[INFO][2021-06-08 20:45:15,495][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://192.168.52.100:8020/user/hive/warehouse
[INFO][2021-06-08 20:45:15,579][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from myhive.parquet_test1
[INFO][2021-06-08 20:45:16,138][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:45:16,146][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:45:18,137][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-08 20:45:18,142][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
[INFO][2021-06-08 20:45:18,144][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<id: int, name: string>
[INFO][2021-06-08 20:45:18,145][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pushed Filters: 
[INFO][2021-06-08 20:45:18,892][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 356.6084 ms
[INFO][2021-06-08 20:45:19,060][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 145.9 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:19,221][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.5 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:19,224][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 10.168.88.20:61125 (size: 16.5 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:19,227][org.apache.spark.SparkContext:54] - Created broadcast 0 from show at JavaParquetOverwrite.java:18
[INFO][2021-06-08 20:45:19,237][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-08 20:45:19,312][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:18
[INFO][2021-06-08 20:45:19,327][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at JavaParquetOverwrite.java:18) with 1 output partitions
[INFO][2021-06-08 20:45:19,328][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:45:19,328][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:45:19,330][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:45:19,335][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18), which has no missing parents
[INFO][2021-06-08 20:45:19,377][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:19,383][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:19,384][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 10.168.88.20:61125 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:19,384][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:45:19,387][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:45:19,388][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
[INFO][2021-06-08 20:45:19,426][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 6496 bytes)
[INFO][2021-06-08 20:45:19,434][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-08 20:45:19,475][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:45:20,608][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:21,581][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:21,645][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1604 bytes result sent to driver
[INFO][2021-06-08 20:45:21,660][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 2255 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:45:21,661][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:45:21,666][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at JavaParquetOverwrite.java:18) finished in 2.270 s
[INFO][2021-06-08 20:45:21,672][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at JavaParquetOverwrite.java:18, took 2.359007 s
[INFO][2021-06-08 20:45:21,682][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:18
[INFO][2021-06-08 20:45:21,683][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at JavaParquetOverwrite.java:18) with 1 output partitions
[INFO][2021-06-08 20:45:21,683][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:45:21,683][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:45:21,684][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:45:21,684][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18), which has no missing parents
[INFO][2021-06-08 20:45:21,687][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:21,696][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:21,698][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 10.168.88.20:61125 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:21,698][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:45:21,699][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at show at JavaParquetOverwrite.java:18)
[INFO][2021-06-08 20:45:21,699][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
[INFO][2021-06-08 20:45:21,701][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 1, ANY, 6503 bytes)
[INFO][2021-06-08 20:45:21,702][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
[INFO][2021-06-08 20:45:21,706][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0_copy_1, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:45:21,716][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:21,723][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:21,729][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1517 bytes result sent to driver
[INFO][2021-06-08 20:45:21,758][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 59 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:45:21,759][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:45:21,759][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at JavaParquetOverwrite.java:18) finished in 0.060 s
[INFO][2021-06-08 20:45:21,760][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at JavaParquetOverwrite.java:18, took 0.077958 s
[INFO][2021-06-08 20:45:21,773][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 10.168.88.20:61125 in memory (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:21,781][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.5848 ms
[INFO][2021-06-08 20:45:21,796][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: desc adp_bas.info_this_jjjz_etf
[INFO][2021-06-08 20:45:21,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:45:21,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:45:21,852][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:45:21,852][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:45:21,853][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-08 20:45:21,855][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:45:21,857][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:45:21,858][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:45:21,859][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:45:21,860][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:45:21,860][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:45:21,861][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:45:21,862][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:45:21,862][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:45:21,863][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:45:21,863][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-08 20:45:21,864][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-08 20:45:21,865][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:45:21,865][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:45:21,866][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:45:21,866][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-08 20:45:21,867][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-08 20:45:21,867][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-08 20:45:21,868][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-08 20:45:21,868][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-08 20:45:21,869][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-08 20:45:21,910][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:19
[INFO][2021-06-08 20:45:21,911][org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (show at JavaParquetOverwrite.java:19) with 1 output partitions
[INFO][2021-06-08 20:45:21,911][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at JavaParquetOverwrite.java:19)
[INFO][2021-06-08 20:45:21,911][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:45:21,911][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:45:21,912][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[5] at show at JavaParquetOverwrite.java:19), which has no missing parents
[INFO][2021-06-08 20:45:21,922][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 4.6 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:21,967][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:21,968][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 10.168.88.20:61125 (size: 2.6 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:21,969][org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:45:21,969][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at show at JavaParquetOverwrite.java:19)
[INFO][2021-06-08 20:45:21,969][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
[INFO][2021-06-08 20:45:21,980][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7403 bytes)
[INFO][2021-06-08 20:45:21,982][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
[INFO][2021-06-08 20:45:22,038][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 13.6743 ms
[INFO][2021-06-08 20:45:22,045][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1742 bytes result sent to driver
[INFO][2021-06-08 20:45:22,049][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 79 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:45:22,049][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:45:22,050][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at JavaParquetOverwrite.java:19) finished in 0.079 s
[INFO][2021-06-08 20:45:22,050][org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: show at JavaParquetOverwrite.java:19, took 0.140138 s
[INFO][2021-06-08 20:45:22,062][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.8125 ms
[INFO][2021-06-08 20:45:22,064][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from myhive.parquet_test1
[INFO][2021-06-08 20:45:22,096][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:45:22,096][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:45:22,115][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-08 20:45:22,115][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
[INFO][2021-06-08 20:45:22,116][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<id: int, name: string>
[INFO][2021-06-08 20:45:22,116][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pushed Filters: 
[INFO][2021-06-08 20:45:22,143][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 145.9 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,156][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.5 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,158][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 10.168.88.20:61125 (size: 16.5 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:22,159][org.apache.spark.SparkContext:54] - Created broadcast 4 from show at JavaParquetOverwrite.java:20
[INFO][2021-06-08 20:45:22,160][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-08 20:45:22,170][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:20
[INFO][2021-06-08 20:45:22,171][org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (show at JavaParquetOverwrite.java:20) with 1 output partitions
[INFO][2021-06-08 20:45:22,172][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:45:22,172][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:45:22,172][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:45:22,172][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20), which has no missing parents
[INFO][2021-06-08 20:45:22,175][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,180][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,182][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 10.168.88.20:61125 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:22,182][org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:45:22,182][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:45:22,182][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
[INFO][2021-06-08 20:45:22,184][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 6496 bytes)
[INFO][2021-06-08 20:45:22,184][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 3)
[INFO][2021-06-08 20:45:22,187][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:45:22,195][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,200][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,202][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 3). 1348 bytes result sent to driver
[INFO][2021-06-08 20:45:22,206][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 3) in 23 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:45:22,206][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:45:22,207][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at JavaParquetOverwrite.java:20) finished in 0.024 s
[INFO][2021-06-08 20:45:22,207][org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: show at JavaParquetOverwrite.java:20, took 0.036145 s
[INFO][2021-06-08 20:45:22,211][org.apache.spark.SparkContext:54] - Starting job: show at JavaParquetOverwrite.java:20
[INFO][2021-06-08 20:45:22,211][org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (show at JavaParquetOverwrite.java:20) with 1 output partitions
[INFO][2021-06-08 20:45:22,211][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:45:22,212][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:45:22,212][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:45:22,212][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20), which has no missing parents
[INFO][2021-06-08 20:45:22,214][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6 stored as values in memory (estimated size 8.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,219][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,220][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_6_piece0 in memory on 10.168.88.20:61125 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:22,221][org.apache.spark.SparkContext:54] - Created broadcast 6 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:45:22,221][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[8] at show at JavaParquetOverwrite.java:20)
[INFO][2021-06-08 20:45:22,221][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 1 tasks
[INFO][2021-06-08 20:45:22,222][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 1, ANY, 6503 bytes)
[INFO][2021-06-08 20:45:22,223][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 4)
[INFO][2021-06-08 20:45:22,228][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0_copy_1, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:45:22,240][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,249][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,254][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 4). 1427 bytes result sent to driver
[INFO][2021-06-08 20:45:22,256][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 4) in 35 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 20:45:22,257][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:45:22,257][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (show at JavaParquetOverwrite.java:20) finished in 0.036 s
[INFO][2021-06-08 20:45:22,258][org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: show at JavaParquetOverwrite.java:20, took 0.046991 s
[INFO][2021-06-08 20:45:22,261][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: insert overwrite  table myhive.parquet_test2 select id , name from myhive.parquet_test1
[INFO][2021-06-08 20:45:22,331][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:45:22,332][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:45:22,355][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 20:45:22,356][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 20:45:22,513][org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat:54] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO][2021-06-08 20:45:22,527][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-08 20:45:22,527][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
[INFO][2021-06-08 20:45:22,529][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<id: int, name: string>
[INFO][2021-06-08 20:45:22,529][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pushed Filters: 
[INFO][2021-06-08 20:45:22,533][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO][2021-06-08 20:45:22,533][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO][2021-06-08 20:45:22,534][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO][2021-06-08 20:45:22,534][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO][2021-06-08 20:45:22,534][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO][2021-06-08 20:45:22,535][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO][2021-06-08 20:45:22,537][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO][2021-06-08 20:45:22,556][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.9405 ms
[INFO][2021-06-08 20:45:22,572][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7 stored as values in memory (estimated size 145.9 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,580][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 16.5 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,581][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_7_piece0 in memory on 10.168.88.20:61125 (size: 16.5 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:22,582][org.apache.spark.SparkContext:54] - Created broadcast 7 from sql at JavaParquetOverwrite.java:21
[INFO][2021-06-08 20:45:22,582][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-08 20:45:22,606][org.apache.spark.SparkContext:54] - Starting job: sql at JavaParquetOverwrite.java:21
[INFO][2021-06-08 20:45:22,607][org.apache.spark.scheduler.DAGScheduler:54] - Got job 5 (sql at JavaParquetOverwrite.java:21) with 2 output partitions
[INFO][2021-06-08 20:45:22,607][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 5 (sql at JavaParquetOverwrite.java:21)
[INFO][2021-06-08 20:45:22,607][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 20:45:22,607][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 20:45:22,607][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 5 (MapPartitionsRDD[10] at sql at JavaParquetOverwrite.java:21), which has no missing parents
[INFO][2021-06-08 20:45:22,614][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8 stored as values in memory (estimated size 60.2 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,616][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 22.7 KB, free 3.8 GB)
[INFO][2021-06-08 20:45:22,618][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_8_piece0 in memory on 10.168.88.20:61125 (size: 22.7 KB, free: 3.8 GB)
[INFO][2021-06-08 20:45:22,618][org.apache.spark.SparkContext:54] - Created broadcast 8 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 20:45:22,619][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at sql at JavaParquetOverwrite.java:21)
[INFO][2021-06-08 20:45:22,619][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 5.0 with 2 tasks
[INFO][2021-06-08 20:45:22,620][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, ANY, 6589 bytes)
[INFO][2021-06-08 20:45:22,621][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 5.0 (TID 6, localhost, executor driver, partition 1, ANY, 6596 bytes)
[INFO][2021-06-08 20:45:22,621][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 5.0 (TID 5)
[INFO][2021-06-08 20:45:22,622][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 5.0 (TID 6)
[INFO][2021-06-08 20:45:22,632][org.apache.hadoop.conf.Configuration.deprecation:840] - mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
[INFO][2021-06-08 20:45:22,634][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
[INFO][2021-06-08 20:45:22,634][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
[INFO][2021-06-08 20:45:22,635][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
[INFO][2021-06-08 20:45:22,641][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO][2021-06-08 20:45:22,646][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO][2021-06-08 20:45:22,647][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO][2021-06-08 20:45:22,648][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO][2021-06-08 20:45:22,649][org.apache.parquet.hadoop.codec.CodecConfig:151] - Compression: SNAPPY
[INFO][2021-06-08 20:45:22,649][org.apache.parquet.hadoop.codec.CodecConfig:151] - Compression: SNAPPY
[INFO][2021-06-08 20:45:22,651][org.apache.parquet.hadoop.codec.CodecConfig:151] - Compression: SNAPPY
[INFO][2021-06-08 20:45:22,651][org.apache.parquet.hadoop.codec.CodecConfig:151] - Compression: SNAPPY
[INFO][2021-06-08 20:45:22,653][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Parquet block size to 134217728
[INFO][2021-06-08 20:45:22,653][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Parquet block size to 134217728
[INFO][2021-06-08 20:45:22,653][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Parquet page size to 1048576
[INFO][2021-06-08 20:45:22,653][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Parquet page size to 1048576
[INFO][2021-06-08 20:45:22,653][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Parquet dictionary page size to 1048576
[INFO][2021-06-08 20:45:22,653][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Parquet dictionary page size to 1048576
[INFO][2021-06-08 20:45:22,654][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Dictionary is on
[INFO][2021-06-08 20:45:22,654][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Validation is off
[INFO][2021-06-08 20:45:22,654][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Dictionary is on
[INFO][2021-06-08 20:45:22,654][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Writer version is: PARQUET_1_0
[INFO][2021-06-08 20:45:22,654][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Validation is off
[INFO][2021-06-08 20:45:22,654][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Maximum row group padding size is 0 bytes
[INFO][2021-06-08 20:45:22,654][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Writer version is: PARQUET_1_0
[INFO][2021-06-08 20:45:22,655][org.apache.parquet.hadoop.ParquetOutputFormat:151] - Maximum row group padding size is 0 bytes
[INFO][2021-06-08 20:45:22,661][org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport:54] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "int"
    }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional binary name (UTF8);
}

       
[INFO][2021-06-08 20:45:22,661][org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport:54] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "int"
    }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional binary name (UTF8);
}

       
[INFO][2021-06-08 20:45:22,683][org.apache.hadoop.io.compress.CodecPool:150] - Got brand-new compressor [.snappy]
[INFO][2021-06-08 20:45:22,683][org.apache.hadoop.io.compress.CodecPool:150] - Got brand-new compressor [.snappy]
[INFO][2021-06-08 20:45:22,722][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:45:22,722][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test1/000000_0_copy_1, range: 0-309, partition values: [empty row]
[INFO][2021-06-08 20:45:22,731][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,732][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,737][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,740][org.apache.parquet.CorruptStatistics:151] - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
[INFO][2021-06-08 20:45:22,741][org.apache.parquet.hadoop.InternalParquetRecordWriter:151] - Flushing mem columnStore to file. allocated memory: 21
[INFO][2021-06-08 20:45:22,741][org.apache.parquet.hadoop.InternalParquetRecordWriter:151] - Flushing mem columnStore to file. allocated memory: 21
[INFO][2021-06-08 20:45:22,803][org.apache.parquet.hadoop.ColumnChunkPageWriteStore:151] - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
[INFO][2021-06-08 20:45:22,803][org.apache.parquet.hadoop.ColumnChunkPageWriteStore:151] - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
[INFO][2021-06-08 20:45:22,804][org.apache.parquet.hadoop.ColumnChunkPageWriteStore:151] - written 52B for [name] BINARY: 1 values, 15B raw, 17B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
[INFO][2021-06-08 20:45:22,804][org.apache.parquet.hadoop.ColumnChunkPageWriteStore:151] - written 52B for [name] BINARY: 1 values, 15B raw, 17B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
[INFO][2021-06-08 20:45:22,903][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:439] - Saved output of task 'attempt_20210608204522_0005_m_000000_0' to hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test2/_temporary/0/task_20210608204522_0005_m_000000
[INFO][2021-06-08 20:45:22,903][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:439] - Saved output of task 'attempt_20210608204522_0005_m_000001_0' to hdfs://node01:8020/user/hive/warehouse/myhive.db/parquet_test2/_temporary/0/task_20210608204522_0005_m_000001
[INFO][2021-06-08 20:45:22,904][org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20210608204522_0005_m_000000_0: Committed
[INFO][2021-06-08 20:45:22,904][org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20210608204522_0005_m_000001_0: Committed
[INFO][2021-06-08 20:45:22,907][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 5.0 (TID 6). 1684 bytes result sent to driver
[INFO][2021-06-08 20:45:22,907][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 5.0 (TID 5). 1507 bytes result sent to driver
[INFO][2021-06-08 20:45:22,911][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 5.0 (TID 5) in 292 ms on localhost (executor driver) (1/2)
[INFO][2021-06-08 20:45:22,911][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 5.0 (TID 6) in 291 ms on localhost (executor driver) (2/2)
[INFO][2021-06-08 20:45:22,912][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 20:45:22,913][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 5 (sql at JavaParquetOverwrite.java:21) finished in 0.293 s
[INFO][2021-06-08 20:45:22,913][org.apache.spark.scheduler.DAGScheduler:54] - Job 5 finished: sql at JavaParquetOverwrite.java:21, took 0.307298 s
[INFO][2021-06-08 20:45:22,940][org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
[INFO][2021-06-08 20:45:22,950][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-08 20:45:22,953][org.spark_project.jetty.server.ServerConnector:306] - Stopped Spark@62fad19{HTTP/1.1}{0.0.0.0:4040}
[INFO][2021-06-08 20:45:22,955][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@2783717b{/stages/stage/kill,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,955][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@75e01201{/jobs/job/kill,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,955][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@c260bdc{/api,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,955][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@6bb75258{/,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,955][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5ad10c1a{/static,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,955][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5b04476e{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,956][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@4264b240{/executors/threadDump,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,956][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@2b43529a{/executors/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,956][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@25748410{/executors,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,956][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@485a3466{/environment/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,957][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@7a527389{/environment,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,957][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@1623134f{/storage/rdd/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,957][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5357c287{/storage/rdd,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,957][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@546ccad7{/storage/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,958][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@78a287ed{/storage,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,958][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5ce33a58{/stages/pool/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,958][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@7a8fa663{/stages/pool,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,958][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@3a62c01e{/stages/stage/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,958][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@50fe837a{/stages/stage,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,959][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@3c435123{/stages/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,959][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@150ab4ed{/stages,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,959][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@a8c1f44{/jobs/job/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,959][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@71b3bc45{/jobs/job,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,959][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@73db4768{/jobs/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,959][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@4c37b5b{/jobs,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 20:45:22,961][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://10.168.88.20:4040
[INFO][2021-06-08 20:45:22,970][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-08 20:45:22,988][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-08 20:45:22,988][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-08 20:45:22,990][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-08 20:45:22,992][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-08 20:45:22,994][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-08 20:45:22,995][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-08 20:45:22,996][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-1eb516fe-ed72-4c87-9743-6de5eca5b0d1
[INFO][2021-06-08 21:17:32,093][org.apache.spark.SparkContext:54] - Running Spark version 2.1.1
[INFO][2021-06-08 21:17:32,393][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-08 21:17:32,396][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-08 21:17:32,396][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-08 21:17:32,397][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-08 21:17:32,398][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-08 21:17:33,703][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 64152.
[INFO][2021-06-08 21:17:33,720][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-08 21:17:33,734][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-08 21:17:33,736][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-08 21:17:33,737][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-08 21:17:33,745][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-afae8307-8a56-4e1d-8132-75c025daf469
[INFO][2021-06-08 21:17:33,769][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-08 21:17:33,844][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-08 21:17:33,896][org.spark_project.jetty.util.log:186] - Logging initialized @4316ms
[INFO][2021-06-08 21:17:33,972][org.spark_project.jetty.server.Server:327] - jetty-9.2.z-SNAPSHOT
[INFO][2021-06-08 21:17:33,984][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4c37b5b{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,984][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@73db4768{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,985][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@71b3bc45{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,985][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@a8c1f44{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,985][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@150ab4ed{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,985][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3c435123{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,985][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@50fe837a{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,985][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3a62c01e{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,986][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@7a8fa663{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,986][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5ce33a58{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,986][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@78a287ed{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,986][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@546ccad7{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,986][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5357c287{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,987][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@1623134f{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,987][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@7a527389{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,987][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@485a3466{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,988][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@25748410{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,988][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@2b43529a{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,988][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4264b240{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,989][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5b04476e{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,993][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@5ad10c1a{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,993][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@6bb75258{/,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,994][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@c260bdc{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,994][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@75e01201{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:33,994][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@2783717b{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:34,000][org.spark_project.jetty.server.ServerConnector:266] - Started Spark@62fad19{HTTP/1.1}{0.0.0.0:4040}
[INFO][2021-06-08 21:17:34,000][org.spark_project.jetty.server.Server:379] - Started @4421ms
[INFO][2021-06-08 21:17:34,001][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-08 21:17:34,003][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-08 21:17:34,056][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-08 21:17:34,086][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64193.
[INFO][2021-06-08 21:17:34,086][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:64193
[INFO][2021-06-08 21:17:34,088][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-08 21:17:34,089][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 64193, None)
[INFO][2021-06-08 21:17:34,092][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:64193 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 64193, None)
[INFO][2021-06-08 21:17:34,096][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 64193, None)
[INFO][2021-06-08 21:17:34,097][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 64193, None)
[INFO][2021-06-08 21:17:34,239][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@c074c0c{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:34,269][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://192.168.52.100:8020/user/hive/warehouse'.
[INFO][2021-06-08 21:17:34,274][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@1e53135d{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:34,274][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3a7704c{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:34,275][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@4acf72b6{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:34,276][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3301500b{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:34,277][org.spark_project.jetty.server.handler.ContextHandler:744] - Started o.s.j.s.ServletContextHandler@3a45c42a{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-08 21:17:34,326][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-08 21:17:34,447][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
[INFO][2021-06-08 21:17:34,448][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
[INFO][2021-06-08 21:17:34,448][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
[INFO][2021-06-08 21:17:34,448][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
[INFO][2021-06-08 21:17:34,449][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
[INFO][2021-06-08 21:17:34,449][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
[INFO][2021-06-08 21:17:34,449][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
[INFO][2021-06-08 21:17:34,450][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
[INFO][2021-06-08 21:17:34,654][hive.metastore:376] - Trying to connect to metastore with URI thrift://192.168.52.100:9083
[INFO][2021-06-08 21:17:34,682][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-08 21:17:35,610][org.apache.hadoop.hdfs.BlockReaderLocal:69] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-08 21:17:35,699][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/eac4baf3-3844-40d2-9be2-036474f49f66_resources
[INFO][2021-06-08 21:17:35,706][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/eac4baf3-3844-40d2-9be2-036474f49f66
[INFO][2021-06-08 21:17:35,710][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/eac4baf3-3844-40d2-9be2-036474f49f66
[INFO][2021-06-08 21:17:35,714][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/eac4baf3-3844-40d2-9be2-036474f49f66/_tmp_space.db
[INFO][2021-06-08 21:17:35,717][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://192.168.52.100:8020/user/hive/warehouse
[INFO][2021-06-08 21:17:35,801][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from myhive.kwang_test
[INFO][2021-06-08 21:17:36,402][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 21:17:36,410][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 21:17:37,605][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkKwangTest
[INFO][2021-06-08 21:17:37,861][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from sparkKwangTest
[INFO][2021-06-08 21:17:38,249][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 140.3 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:38,517][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.2 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:38,519][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:64193 (size: 16.2 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:38,525][org.apache.spark.SparkContext:54] - Created broadcast 0 from show at SparkOverwrite.java:16
[INFO][2021-06-08 21:17:38,768][org.apache.hadoop.mapred.FileInputFormat:253] - Total input paths to process : 2
[INFO][2021-06-08 21:17:38,789][org.apache.spark.SparkContext:54] - Starting job: show at SparkOverwrite.java:16
[INFO][2021-06-08 21:17:38,817][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at SparkOverwrite.java:16) with 1 output partitions
[INFO][2021-06-08 21:17:38,818][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at SparkOverwrite.java:16)
[INFO][2021-06-08 21:17:38,819][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 21:17:38,822][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 21:17:38,835][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[5] at show at SparkOverwrite.java:16), which has no missing parents
[INFO][2021-06-08 21:17:38,881][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 7.9 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:38,892][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:38,892][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:64193 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:38,894][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 21:17:38,898][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at show at SparkOverwrite.java:16)
[INFO][2021-06-08 21:17:38,901][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
[INFO][2021-06-08 21:17:38,963][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5961 bytes)
[INFO][2021-06-08 21:17:38,978][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-08 21:17:39,060][org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test/000000_0:0+8
[INFO][2021-06-08 21:17:39,073][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO][2021-06-08 21:17:39,074][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO][2021-06-08 21:17:39,074][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO][2021-06-08 21:17:39,075][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO][2021-06-08 21:17:39,075][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO][2021-06-08 21:17:39,496][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 301.151499 ms
[INFO][2021-06-08 21:17:40,503][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1332 bytes result sent to driver
[INFO][2021-06-08 21:17:40,520][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1594 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 21:17:40,521][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 21:17:40,526][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at SparkOverwrite.java:16) finished in 1.617 s
[INFO][2021-06-08 21:17:40,532][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at SparkOverwrite.java:16, took 1.742881 s
[INFO][2021-06-08 21:17:40,542][org.apache.spark.SparkContext:54] - Starting job: show at SparkOverwrite.java:16
[INFO][2021-06-08 21:17:40,543][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SparkOverwrite.java:16) with 1 output partitions
[INFO][2021-06-08 21:17:40,543][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at SparkOverwrite.java:16)
[INFO][2021-06-08 21:17:40,544][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 21:17:40,544][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 21:17:40,545][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkOverwrite.java:16), which has no missing parents
[INFO][2021-06-08 21:17:40,546][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 7.9 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:40,552][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:40,596][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.52.10:64193 (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:40,597][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 21:17:40,597][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkOverwrite.java:16)
[INFO][2021-06-08 21:17:40,597][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
[INFO][2021-06-08 21:17:40,599][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 1, ANY, 5968 bytes)
[INFO][2021-06-08 21:17:40,599][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
[INFO][2021-06-08 21:17:40,606][org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test/000000_0_copy_1:0+8
[INFO][2021-06-08 21:17:40,617][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1163 bytes result sent to driver
[INFO][2021-06-08 21:17:40,621][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 22 ms on localhost (executor driver) (1/1)
[INFO][2021-06-08 21:17:40,621][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 21:17:40,621][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at SparkOverwrite.java:16) finished in 0.023 s
[INFO][2021-06-08 21:17:40,621][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SparkOverwrite.java:16, took 0.078644 s
[INFO][2021-06-08 21:17:40,646][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.472801 ms
[INFO][2021-06-08 21:17:40,669][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: insert overwrite  table myhive.kwang_test_2 select id , name from sparkKwangTest
[INFO][2021-06-08 21:17:40,718][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 21:17:40,719][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 21:17:40,768][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 140.3 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:40,782][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.2 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:40,783][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.52.10:64193 (size: 16.2 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:40,784][org.apache.spark.SparkContext:54] - Created broadcast 3 from sql at SparkOverwrite.java:17
[INFO][2021-06-08 21:17:40,802][org.apache.hadoop.hive.common.FileUtils:501] - Creating directory if it doesn't exist: hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test_2/.hive-staging_hive_2021-06-08_21-17-40_798_3302525573322728275-1
[INFO][2021-06-08 21:17:40,872][org.apache.hadoop.mapred.FileInputFormat:253] - Total input paths to process : 2
[INFO][2021-06-08 21:17:40,885][org.apache.spark.SparkContext:54] - Starting job: sql at SparkOverwrite.java:17
[INFO][2021-06-08 21:17:40,886][org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (sql at SparkOverwrite.java:17) with 2 output partitions
[INFO][2021-06-08 21:17:40,886][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (sql at SparkOverwrite.java:17)
[INFO][2021-06-08 21:17:40,886][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-08 21:17:40,886][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-08 21:17:40,887][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[9] at sql at SparkOverwrite.java:17), which has no missing parents
[INFO][2021-06-08 21:17:40,897][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 58.4 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:40,920][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.2 KB, free 3.8 GB)
[INFO][2021-06-08 21:17:40,921][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.52.10:64193 (size: 22.2 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:40,923][org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:996
[INFO][2021-06-08 21:17:40,924][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sql at SparkOverwrite.java:17)
[INFO][2021-06-08 21:17:40,925][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
[INFO][2021-06-08 21:17:40,927][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 6028 bytes)
[INFO][2021-06-08 21:17:40,929][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, ANY, 6035 bytes)
[INFO][2021-06-08 21:17:40,929][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
[INFO][2021-06-08 21:17:40,930][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 3)
[INFO][2021-06-08 21:17:40,946][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
[INFO][2021-06-08 21:17:40,948][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
[INFO][2021-06-08 21:17:40,948][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.52.10:64193 in memory (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:40,949][org.apache.hadoop.conf.Configuration.deprecation:840] - mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
[INFO][2021-06-08 21:17:40,950][org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test/000000_0:0+8
[INFO][2021-06-08 21:17:40,952][org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test/000000_0_copy_1:0+8
[INFO][2021-06-08 21:17:40,955][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
[INFO][2021-06-08 21:17:40,958][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.52.10:64193 in memory (size: 16.2 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:40,960][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.52.10:64193 in memory (size: 4.3 KB, free: 3.8 GB)
[INFO][2021-06-08 21:17:41,928][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:439] - Saved output of task 'attempt_20210608211740_0002_m_000000_0' to hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test_2/.hive-staging_hive_2021-06-08_21-17-40_798_3302525573322728275-1/-ext-10000/_temporary/0/task_20210608211740_0002_m_000000
[INFO][2021-06-08 21:17:41,928][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:439] - Saved output of task 'attempt_20210608211740_0002_m_000001_0' to hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test_2/.hive-staging_hive_2021-06-08_21-17-40_798_3302525573322728275-1/-ext-10000/_temporary/0/task_20210608211740_0002_m_000001
[INFO][2021-06-08 21:17:41,929][org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20210608211740_0002_m_000001_0: Committed
[INFO][2021-06-08 21:17:41,929][org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20210608211740_0002_m_000000_0: Committed
[INFO][2021-06-08 21:17:41,931][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1255 bytes result sent to driver
[INFO][2021-06-08 21:17:41,931][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 3). 1255 bytes result sent to driver
[INFO][2021-06-08 21:17:41,934][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 3) in 1006 ms on localhost (executor driver) (1/2)
[INFO][2021-06-08 21:17:41,935][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 1009 ms on localhost (executor driver) (2/2)
[INFO][2021-06-08 21:17:41,935][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-06-08 21:17:41,935][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (sql at SparkOverwrite.java:17) finished in 1.009 s
[INFO][2021-06-08 21:17:41,936][org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: sql at SparkOverwrite.java:17, took 1.050861 s
[INFO][2021-06-08 21:17:42,057][hive.ql.metadata.Hive:2641] - Replacing src:hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test_2/.hive-staging_hive_2021-06-08_21-17-40_798_3302525573322728275-1/-ext-10000/part-00000, dest: hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test_2/part-00000, Status:true
[INFO][2021-06-08 21:17:42,098][hive.ql.metadata.Hive:2641] - Replacing src:hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test_2/.hive-staging_hive_2021-06-08_21-17-40_798_3302525573322728275-1/-ext-10000/part-00001, dest: hdfs://node01:8020/user/hive/warehouse/myhive.db/kwang_test_2/part-00001, Status:true
[INFO][2021-06-08 21:17:42,431][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: `myhive`.`kwang_test_2`
[INFO][2021-06-08 21:17:42,541][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-08 21:17:42,542][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-08 21:17:42,559][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-08 21:17:42,567][org.spark_project.jetty.server.ServerConnector:306] - Stopped Spark@62fad19{HTTP/1.1}{0.0.0.0:4040}
[INFO][2021-06-08 21:17:42,571][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@2783717b{/stages/stage/kill,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,572][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@75e01201{/jobs/job/kill,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,572][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@c260bdc{/api,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,573][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@6bb75258{/,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,573][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5ad10c1a{/static,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,573][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5b04476e{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,574][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@4264b240{/executors/threadDump,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,575][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@2b43529a{/executors/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,575][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@25748410{/executors,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,576][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@485a3466{/environment/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,576][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@7a527389{/environment,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,577][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@1623134f{/storage/rdd/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,577][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5357c287{/storage/rdd,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,578][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@546ccad7{/storage/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,578][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@78a287ed{/storage,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,579][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@5ce33a58{/stages/pool/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,579][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@7a8fa663{/stages/pool,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,580][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@3a62c01e{/stages/stage/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,580][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@50fe837a{/stages/stage,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,581][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@3c435123{/stages/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,581][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@150ab4ed{/stages,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,581][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@a8c1f44{/jobs/job/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,582][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@71b3bc45{/jobs/job,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,582][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@73db4768{/jobs/json,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,583][org.spark_project.jetty.server.handler.ContextHandler:865] - Stopped o.s.j.s.ServletContextHandler@4c37b5b{/jobs,null,UNAVAILABLE,@Spark}
[INFO][2021-06-08 21:17:42,586][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-08 21:17:42,606][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-08 21:17:42,632][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-08 21:17:42,633][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-08 21:17:42,636][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-08 21:17:42,641][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-08 21:17:42,647][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-08 21:17:42,648][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-08 21:17:42,650][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-31b56ee3-6ac3-4806-afa7-eb196180b3ba
