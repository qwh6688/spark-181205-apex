[INFO][2021-06-02 10:53:34,474][com.apex.bigdata.util.test.Demo01:42] - syrq：20210530
[INFO][2021-06-02 10:53:34,477][com.apex.bigdata.util.test.Demo01:45] - j1z_qc：20210524
[INFO][2021-06-02 14:10:05,441][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 14:10:06,004][org.apache.spark.SparkContext:54] - Submitted application: Union
[INFO][2021-06-02 14:10:06,031][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 14:10:06,034][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 14:10:06,035][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 14:10:06,036][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 14:10:06,079][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 14:10:08,697][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 55655.
[INFO][2021-06-02 14:10:08,716][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 14:10:08,759][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 14:10:08,761][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 14:10:08,761][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 14:10:08,768][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-56b49077-38e2-44b7-9fb8-c8a1a70c0704
[INFO][2021-06-02 14:10:08,783][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 4.0 GB
[INFO][2021-06-02 14:10:08,823][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 14:10:08,877][org.spark_project.jetty.util.log:192] - Logging initialized @4514ms
[INFO][2021-06-02 14:10:08,916][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 14:10:08,926][org.spark_project.jetty.server.Server:403] - Started @4564ms
[INFO][2021-06-02 14:10:08,938][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@63648ee9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 14:10:08,939][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 14:10:08,953][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7fcbe147{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,954][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@89c10b7{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,955][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fe89c24{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,956][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,957][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b970f7{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,958][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b8a71{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,959][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f058b8a{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,960][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,961][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,962][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7b139eab{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,963][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@611df6e3{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,963][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6273c5a4{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,964][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53e211ee{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,965][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,966][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@117e0fe5{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,967][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@78aea4b9{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,968][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b85880b{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,969][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4215838f{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,970][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2289aca5{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,970][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@184497d1{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,978][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ffab045{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,979][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5e8f9e2d{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,981][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@fd46303{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,982][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e2822{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,983][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@29a60c27{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:08,986][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 14:10:09,075][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 14:10:09,108][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55696.
[INFO][2021-06-02 14:10:09,109][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:55696
[INFO][2021-06-02 14:10:09,110][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 14:10:09,113][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 55696, None)
[INFO][2021-06-02 14:10:09,115][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:55696 with 4.0 GB RAM, BlockManagerId(driver, 192.168.52.10, 55696, None)
[INFO][2021-06-02 14:10:09,117][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 55696, None)
[INFO][2021-06-02 14:10:09,118][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 55696, None)
[INFO][2021-06-02 14:10:09,294][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@15b986cd{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:10:09,687][org.apache.spark.SparkContext:54] - Starting job: collect at Union.scala:22
[INFO][2021-06-02 14:10:09,861][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 9 (distinct at Union.scala:22)
[INFO][2021-06-02 14:10:09,863][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at Union.scala:22) with 32 output partitions
[INFO][2021-06-02 14:10:09,864][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (collect at Union.scala:22)
[INFO][2021-06-02 14:10:09,864][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
[INFO][2021-06-02 14:10:09,865][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
[INFO][2021-06-02 14:10:09,871][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[9] at distinct at Union.scala:22), which has no missing parents
[INFO][2021-06-02 14:10:10,017][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 3.6 KB, free 4.0 GB)
[INFO][2021-06-02 14:10:10,041][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.2 KB, free 4.0 GB)
[INFO][2021-06-02 14:10:10,045][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:55696 (size: 2.2 KB, free: 4.0 GB)
[INFO][2021-06-02 14:10:10,046][org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:10:10,065][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 32 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[9] at distinct at Union.scala:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:10:10,066][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 32 tasks
[INFO][2021-06-02 14:10:10,101][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,102][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,103][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,103][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,103][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,104][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,105][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,106][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,107][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,108][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,109][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,110][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,111][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,112][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,112][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,114][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 0.0 (TID 15, localhost, executor driver, partition 15, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 0.0 (TID 8)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 0.0 (TID 6)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 0.0 (TID 11)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 0.0 (TID 5)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 0.0 (TID 7)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 0.0 (TID 4)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 0.0 (TID 3)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 0.0 (TID 9)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 0.0 (TID 10)
[INFO][2021-06-02 14:10:10,122][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-02 14:10:10,126][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 0.0 (TID 15)
[INFO][2021-06-02 14:10:10,125][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 0.0 (TID 14)
[INFO][2021-06-02 14:10:10,125][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 0.0 (TID 13)
[INFO][2021-06-02 14:10:10,123][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 0.0 (TID 12)
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 0.0 (TID 9). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 0.0 (TID 8). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 0.0 (TID 4). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 0.0 (TID 11). 1013 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 0.0 (TID 12). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 0.0 (TID 7). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 0.0 (TID 15). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 0.0 (TID 10). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 0.0 (TID 14). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 0.0 (TID 5). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 0.0 (TID 6). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 0.0 (TID 3). 1056 bytes result sent to driver
[INFO][2021-06-02 14:10:10,243][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 0.0 (TID 13). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,245][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 0.0 (TID 16, localhost, executor driver, partition 16, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,245][org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 0.0 (TID 16)
[INFO][2021-06-02 14:10:10,246][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 0.0 (TID 17, localhost, executor driver, partition 17, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,246][org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 0.0 (TID 17)
[INFO][2021-06-02 14:10:10,247][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 0.0 (TID 18, localhost, executor driver, partition 18, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,247][org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 0.0 (TID 18)
[INFO][2021-06-02 14:10:10,248][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 0.0 (TID 19, localhost, executor driver, partition 19, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,248][org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 0.0 (TID 19)
[INFO][2021-06-02 14:10:10,249][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 0.0 (TID 20, localhost, executor driver, partition 20, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,249][org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 0.0 (TID 20)
[INFO][2021-06-02 14:10:10,251][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 0.0 (TID 11) in 140 ms on localhost (executor driver) (1/32)
[INFO][2021-06-02 14:10:10,254][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 0.0 (TID 4) in 150 ms on localhost (executor driver) (2/32)
[INFO][2021-06-02 14:10:10,254][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 0.0 (TID 8) in 148 ms on localhost (executor driver) (3/32)
[INFO][2021-06-02 14:10:10,255][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 0.0 (TID 9) in 148 ms on localhost (executor driver) (4/32)
[INFO][2021-06-02 14:10:10,257][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 0.0 (TID 21, localhost, executor driver, partition 21, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,257][org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 0.0 (TID 21)
[INFO][2021-06-02 14:10:10,258][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 0.0 (TID 12) in 147 ms on localhost (executor driver) (5/32)
[INFO][2021-06-02 14:10:10,259][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 157 ms on localhost (executor driver) (6/32)
[INFO][2021-06-02 14:10:10,259][org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 0.0 (TID 18). 884 bytes result sent to driver
[INFO][2021-06-02 14:10:10,260][org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 0.0 (TID 17). 884 bytes result sent to driver
[INFO][2021-06-02 14:10:10,260][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 0.0 (TID 22, localhost, executor driver, partition 22, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,261][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 0.0 (TID 7) in 156 ms on localhost (executor driver) (7/32)
[INFO][2021-06-02 14:10:10,261][org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 0.0 (TID 16). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,261][org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 0.0 (TID 22)
[INFO][2021-06-02 14:10:10,261][org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 0.0 (TID 20). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,261][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 0.0 (TID 23, localhost, executor driver, partition 23, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,263][org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 0.0 (TID 23)
[INFO][2021-06-02 14:10:10,263][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 0.0 (TID 24, localhost, executor driver, partition 24, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,264][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 0.0 (TID 15) in 150 ms on localhost (executor driver) (8/32)
[INFO][2021-06-02 14:10:10,264][org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 0.0 (TID 24)
[INFO][2021-06-02 14:10:10,264][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 0.0 (TID 10) in 156 ms on localhost (executor driver) (9/32)
[INFO][2021-06-02 14:10:10,265][org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 0.0 (TID 19). 1013 bytes result sent to driver
[INFO][2021-06-02 14:10:10,266][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 0.0 (TID 25, localhost, executor driver, partition 25, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,267][org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 0.0 (TID 25)
[INFO][2021-06-02 14:10:10,267][org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 0.0 (TID 21). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,267][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 0.0 (TID 26, localhost, executor driver, partition 26, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,268][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 0.0 (TID 27, localhost, executor driver, partition 27, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,269][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 0.0 (TID 14) in 157 ms on localhost (executor driver) (10/32)
[INFO][2021-06-02 14:10:10,271][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 168 ms on localhost (executor driver) (11/32)
[INFO][2021-06-02 14:10:10,272][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 0.0 (TID 5) in 168 ms on localhost (executor driver) (12/32)
[INFO][2021-06-02 14:10:10,268][org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 0.0 (TID 26)
[INFO][2021-06-02 14:10:10,270][org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 0.0 (TID 27)
[INFO][2021-06-02 14:10:10,274][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 0.0 (TID 28, localhost, executor driver, partition 28, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,275][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 0.0 (TID 29, localhost, executor driver, partition 29, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,275][org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 0.0 (TID 28)
[INFO][2021-06-02 14:10:10,276][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 0.0 (TID 30, localhost, executor driver, partition 30, PROCESS_LOCAL, 4850 bytes)
[INFO][2021-06-02 14:10:10,277][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 0.0 (TID 31, localhost, executor driver, partition 31, PROCESS_LOCAL, 4854 bytes)
[INFO][2021-06-02 14:10:10,277][org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 0.0 (TID 23). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,278][org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 0.0 (TID 22). 970 bytes result sent to driver
[INFO][2021-06-02 14:10:10,277][org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 0.0 (TID 30)
[INFO][2021-06-02 14:10:10,279][org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 0.0 (TID 29)
[INFO][2021-06-02 14:10:10,278][org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 0.0 (TID 31)
[INFO][2021-06-02 14:10:10,280][org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 0.0 (TID 24). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,284][org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 0.0 (TID 26). 884 bytes result sent to driver
[INFO][2021-06-02 14:10:10,285][org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 0.0 (TID 25). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,286][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 199 ms on localhost (executor driver) (13/32)
[INFO][2021-06-02 14:10:10,288][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 0.0 (TID 6) in 184 ms on localhost (executor driver) (14/32)
[INFO][2021-06-02 14:10:10,288][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 0.0 (TID 17) in 42 ms on localhost (executor driver) (15/32)
[INFO][2021-06-02 14:10:10,289][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 0.0 (TID 13) in 178 ms on localhost (executor driver) (16/32)
[INFO][2021-06-02 14:10:10,288][org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 0.0 (TID 27). 1013 bytes result sent to driver
[INFO][2021-06-02 14:10:10,290][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 0.0 (TID 20) in 41 ms on localhost (executor driver) (17/32)
[INFO][2021-06-02 14:10:10,291][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 0.0 (TID 16) in 47 ms on localhost (executor driver) (18/32)
[INFO][2021-06-02 14:10:10,291][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 0.0 (TID 19) in 43 ms on localhost (executor driver) (19/32)
[INFO][2021-06-02 14:10:10,292][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 0.0 (TID 18) in 45 ms on localhost (executor driver) (20/32)
[INFO][2021-06-02 14:10:10,292][org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 0.0 (TID 28). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,292][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 189 ms on localhost (executor driver) (21/32)
[INFO][2021-06-02 14:10:10,293][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 0.0 (TID 21) in 37 ms on localhost (executor driver) (22/32)
[INFO][2021-06-02 14:10:10,293][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 0.0 (TID 22) in 33 ms on localhost (executor driver) (23/32)
[INFO][2021-06-02 14:10:10,294][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 0.0 (TID 24) in 31 ms on localhost (executor driver) (24/32)
[INFO][2021-06-02 14:10:10,294][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 0.0 (TID 26) in 28 ms on localhost (executor driver) (25/32)
[INFO][2021-06-02 14:10:10,295][org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 0.0 (TID 29). 927 bytes result sent to driver
[INFO][2021-06-02 14:10:10,295][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 0.0 (TID 23) in 34 ms on localhost (executor driver) (26/32)
[INFO][2021-06-02 14:10:10,295][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 0.0 (TID 25) in 29 ms on localhost (executor driver) (27/32)
[INFO][2021-06-02 14:10:10,296][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 0.0 (TID 28) in 23 ms on localhost (executor driver) (28/32)
[INFO][2021-06-02 14:10:10,297][org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 0.0 (TID 31). 1013 bytes result sent to driver
[INFO][2021-06-02 14:10:10,297][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 0.0 (TID 27) in 29 ms on localhost (executor driver) (29/32)
[INFO][2021-06-02 14:10:10,297][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 0.0 (TID 29) in 22 ms on localhost (executor driver) (30/32)
[INFO][2021-06-02 14:10:10,298][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 0.0 (TID 31) in 21 ms on localhost (executor driver) (31/32)
[INFO][2021-06-02 14:10:10,298][org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 0.0 (TID 30). 884 bytes result sent to driver
[INFO][2021-06-02 14:10:10,299][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 0.0 (TID 30) in 23 ms on localhost (executor driver) (32/32)
[INFO][2021-06-02 14:10:10,301][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:10:10,302][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (distinct at Union.scala:22) finished in 0.224 s
[INFO][2021-06-02 14:10:10,303][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 14:10:10,303][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 14:10:10,304][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
[INFO][2021-06-02 14:10:10,305][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 14:10:10,307][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[11] at distinct at Union.scala:22), which has no missing parents
[INFO][2021-06-02 14:10:10,313][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 4.0 GB)
[INFO][2021-06-02 14:10:10,314][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 4.0 GB)
[INFO][2021-06-02 14:10:10,314][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:55696 (size: 2.3 KB, free: 4.0 GB)
[INFO][2021-06-02 14:10:10,315][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:10:10,317][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 32 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at distinct at Union.scala:22) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:10:10,317][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 32 tasks
[INFO][2021-06-02 14:10:10,320][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,320][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 1.0 (TID 33, localhost, executor driver, partition 7, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,321][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 1.0 (TID 34, localhost, executor driver, partition 8, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,321][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 1.0 (TID 35, localhost, executor driver, partition 9, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,322][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 1.0 (TID 36, localhost, executor driver, partition 10, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,322][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 1.0 (TID 37, localhost, executor driver, partition 11, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,322][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 1.0 (TID 38, localhost, executor driver, partition 12, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,322][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 1.0 (TID 39, localhost, executor driver, partition 13, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,323][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 1.0 (TID 40, localhost, executor driver, partition 14, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,323][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 1.0 (TID 41, localhost, executor driver, partition 15, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,323][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 1.0 (TID 42, localhost, executor driver, partition 16, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,323][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 1.0 (TID 43, localhost, executor driver, partition 17, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,323][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 1.0 (TID 44, localhost, executor driver, partition 18, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,324][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 1.0 (TID 45, localhost, executor driver, partition 19, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,324][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 1.0 (TID 46, localhost, executor driver, partition 20, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,324][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 1.0 (TID 47, localhost, executor driver, partition 21, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 1.0 (TID 33)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 1.0 (TID 41)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 1.0 (TID 45)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 1.0 (TID 47)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 1.0 (TID 46)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 1.0 (TID 44)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 1.0 (TID 43)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 1.0 (TID 42)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 1.0 (TID 40)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 1.0 (TID 39)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 1.0 (TID 38)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 32)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 1.0 (TID 37)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 1.0 (TID 34)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 1.0 (TID 36)
[INFO][2021-06-02 14:10:10,325][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 1.0 (TID 35)
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,339][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,339][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 14:10:10,341][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 1.0 (TID 34). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 1.0 (TID 41). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,357][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 1.0 (TID 35). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 1.0 (TID 44). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,357][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 1.0 (TID 33). 1078 bytes result sent to driver
[INFO][2021-06-02 14:10:10,357][org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 1.0 (TID 43). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 32). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 1.0 (TID 40). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 1.0 (TID 37). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 1.0 (TID 36). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 1.0 (TID 38). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,356][org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 1.0 (TID 42). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,358][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 1.0 (TID 39). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,357][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 1.0 (TID 48, localhost, executor driver, partition 22, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,357][org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 1.0 (TID 47). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,357][org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 1.0 (TID 45). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,357][org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 1.0 (TID 46). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,359][org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 1.0 (TID 48)
[INFO][2021-06-02 14:10:10,359][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 1.0 (TID 49, localhost, executor driver, partition 23, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,359][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 1.0 (TID 34) in 39 ms on localhost (executor driver) (1/32)
[INFO][2021-06-02 14:10:10,359][org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 1.0 (TID 49)
[INFO][2021-06-02 14:10:10,360][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 1.0 (TID 41) in 37 ms on localhost (executor driver) (2/32)
[INFO][2021-06-02 14:10:10,360][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 1.0 (TID 50, localhost, executor driver, partition 24, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,361][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 1.0 (TID 35) in 40 ms on localhost (executor driver) (3/32)
[INFO][2021-06-02 14:10:10,361][org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 1.0 (TID 50)
[INFO][2021-06-02 14:10:10,361][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,361][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,361][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,361][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 1.0 (TID 51, localhost, executor driver, partition 25, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,361][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,362][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 1.0 (TID 44) in 39 ms on localhost (executor driver) (4/32)
[INFO][2021-06-02 14:10:10,362][org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 1.0 (TID 51)
[INFO][2021-06-02 14:10:10,363][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 1.0 (TID 52, localhost, executor driver, partition 26, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,363][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,364][org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 1.0 (TID 52)
[INFO][2021-06-02 14:10:10,364][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 1.0 (TID 53, localhost, executor driver, partition 27, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,364][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:10:10,365][org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 1.0 (TID 53)
[INFO][2021-06-02 14:10:10,365][org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 1.0 (TID 48). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,365][org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 1.0 (TID 49). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,366][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,366][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,366][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 1.0 (TID 54, localhost, executor driver, partition 28, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,367][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 1.0 (TID 33) in 47 ms on localhost (executor driver) (5/32)
[INFO][2021-06-02 14:10:10,367][org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 1.0 (TID 54)
[INFO][2021-06-02 14:10:10,368][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 1.0 (TID 55, localhost, executor driver, partition 29, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,368][org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 1.0 (TID 50). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,368][org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 1.0 (TID 55)
[INFO][2021-06-02 14:10:10,369][org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 1.0 (TID 51). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,368][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,369][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,369][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 1.0 (TID 56, localhost, executor driver, partition 30, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,369][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,369][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,370][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,369][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,370][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 32) in 51 ms on localhost (executor driver) (6/32)
[INFO][2021-06-02 14:10:10,370][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,371][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,371][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 1.0 (TID 36) in 49 ms on localhost (executor driver) (7/32)
[INFO][2021-06-02 14:10:10,371][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 1.0 (TID 40) in 48 ms on localhost (executor driver) (8/32)
[INFO][2021-06-02 14:10:10,370][org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 1.0 (TID 56)
[INFO][2021-06-02 14:10:10,373][org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 1.0 (TID 54). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,373][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,373][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 1.0 (TID 43) in 50 ms on localhost (executor driver) (9/32)
[INFO][2021-06-02 14:10:10,373][org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 1.0 (TID 52). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,374][org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 1.0 (TID 53). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,374][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 1.0 (TID 37) in 52 ms on localhost (executor driver) (10/32)
[INFO][2021-06-02 14:10:10,373][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,375][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 1.0 (TID 57, localhost, executor driver, partition 31, PROCESS_LOCAL, 4621 bytes)
[INFO][2021-06-02 14:10:10,375][org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 1.0 (TID 57)
[INFO][2021-06-02 14:10:10,375][org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 1.0 (TID 55). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,375][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 58, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-06-02 14:10:10,376][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 1.0 (TID 38) in 54 ms on localhost (executor driver) (11/32)
[INFO][2021-06-02 14:10:10,377][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,377][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,378][org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 1.0 (TID 56). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,378][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 1.0 (TID 59, localhost, executor driver, partition 2, ANY, 4621 bytes)
[INFO][2021-06-02 14:10:10,379][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 1.0 (TID 60, localhost, executor driver, partition 3, ANY, 4621 bytes)
[INFO][2021-06-02 14:10:10,379][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 1.0 (TID 60)
[INFO][2021-06-02 14:10:10,379][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 1.0 (TID 59)
[INFO][2021-06-02 14:10:10,379][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 58)
[INFO][2021-06-02 14:10:10,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 1.0 (TID 61, localhost, executor driver, partition 4, ANY, 4621 bytes)
[INFO][2021-06-02 14:10:10,381][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 1.0 (TID 61)
[INFO][2021-06-02 14:10:10,381][org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 1.0 (TID 57). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 1.0 (TID 62, localhost, executor driver, partition 5, ANY, 4621 bytes)
[INFO][2021-06-02 14:10:10,381][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,382][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 1.0 (TID 42) in 59 ms on localhost (executor driver) (12/32)
[INFO][2021-06-02 14:10:10,382][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 1.0 (TID 62)
[INFO][2021-06-02 14:10:10,381][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,383][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:10:10,382][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 1.0 (TID 45) in 59 ms on localhost (executor driver) (13/32)
[INFO][2021-06-02 14:10:10,382][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,383][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,382][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,383][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 1.0 (TID 39) in 61 ms on localhost (executor driver) (14/32)
[INFO][2021-06-02 14:10:10,383][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,384][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 1.0 (TID 46) in 60 ms on localhost (executor driver) (15/32)
[INFO][2021-06-02 14:10:10,384][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,384][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,384][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,385][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 1.0 (TID 63, localhost, executor driver, partition 6, ANY, 4621 bytes)
[INFO][2021-06-02 14:10:10,385][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 1.0 (TID 47) in 61 ms on localhost (executor driver) (16/32)
[INFO][2021-06-02 14:10:10,385][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 1.0 (TID 63)
[INFO][2021-06-02 14:10:10,386][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 1.0 (TID 48) in 29 ms on localhost (executor driver) (17/32)
[INFO][2021-06-02 14:10:10,386][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 1.0 (TID 49) in 27 ms on localhost (executor driver) (18/32)
[INFO][2021-06-02 14:10:10,387][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 32 blocks
[INFO][2021-06-02 14:10:10,388][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,388][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 1.0 (TID 50) in 28 ms on localhost (executor driver) (19/32)
[INFO][2021-06-02 14:10:10,389][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 1.0 (TID 52) in 26 ms on localhost (executor driver) (20/32)
[INFO][2021-06-02 14:10:10,390][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 1.0 (TID 51) in 28 ms on localhost (executor driver) (21/32)
[INFO][2021-06-02 14:10:10,390][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 1.0 (TID 54) in 24 ms on localhost (executor driver) (22/32)
[INFO][2021-06-02 14:10:10,390][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 1.0 (TID 53) in 26 ms on localhost (executor driver) (23/32)
[INFO][2021-06-02 14:10:10,390][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 1.0 (TID 55) in 23 ms on localhost (executor driver) (24/32)
[INFO][2021-06-02 14:10:10,391][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 1.0 (TID 56) in 22 ms on localhost (executor driver) (25/32)
[INFO][2021-06-02 14:10:10,391][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 1.0 (TID 57) in 17 ms on localhost (executor driver) (26/32)
[INFO][2021-06-02 14:10:10,406][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 58). 996 bytes result sent to driver
[INFO][2021-06-02 14:10:10,406][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 1.0 (TID 59). 1039 bytes result sent to driver
[INFO][2021-06-02 14:10:10,406][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 1.0 (TID 63). 1082 bytes result sent to driver
[INFO][2021-06-02 14:10:10,406][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 1.0 (TID 62). 996 bytes result sent to driver
[INFO][2021-06-02 14:10:10,407][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 1.0 (TID 60). 996 bytes result sent to driver
[INFO][2021-06-02 14:10:10,407][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 1.0 (TID 61). 996 bytes result sent to driver
[INFO][2021-06-02 14:10:10,407][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 58) in 32 ms on localhost (executor driver) (27/32)
[INFO][2021-06-02 14:10:10,407][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 1.0 (TID 59) in 30 ms on localhost (executor driver) (28/32)
[INFO][2021-06-02 14:10:10,408][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 1.0 (TID 63) in 24 ms on localhost (executor driver) (29/32)
[INFO][2021-06-02 14:10:10,410][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 1.0 (TID 60) in 31 ms on localhost (executor driver) (30/32)
[INFO][2021-06-02 14:10:10,410][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 1.0 (TID 62) in 29 ms on localhost (executor driver) (31/32)
[INFO][2021-06-02 14:10:10,411][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 1.0 (TID 61) in 31 ms on localhost (executor driver) (32/32)
[INFO][2021-06-02 14:10:10,411][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:10:10,412][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (collect at Union.scala:22) finished in 0.093 s
[INFO][2021-06-02 14:10:10,418][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at Union.scala:22, took 0.731102 s
[INFO][2021-06-02 14:10:10,434][org.apache.spark.SparkContext:54] - Starting job: collect at Union.scala:23
[INFO][2021-06-02 14:10:10,435][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (intersection at Union.scala:20)
[INFO][2021-06-02 14:10:10,436][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 4 (intersection at Union.scala:20)
[INFO][2021-06-02 14:10:10,436][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (collect at Union.scala:23) with 16 output partitions
[INFO][2021-06-02 14:10:10,437][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (collect at Union.scala:23)
[INFO][2021-06-02 14:10:10,437][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 2, ShuffleMapStage 3)
[INFO][2021-06-02 14:10:10,437][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 2, ShuffleMapStage 3)
[INFO][2021-06-02 14:10:10,437][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 2 (MapPartitionsRDD[3] at intersection at Union.scala:20), which has no missing parents
[INFO][2021-06-02 14:10:10,439][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 2.6 KB, free 4.0 GB)
[INFO][2021-06-02 14:10:10,441][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1664.0 B, free 4.0 GB)
[INFO][2021-06-02 14:10:10,441][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.52.10:55696 (size: 1664.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:10:10,442][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:10:10,443][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 16 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[3] at intersection at Union.scala:20) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:10:10,443][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 16 tasks
[INFO][2021-06-02 14:10:10,444][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 3 (MapPartitionsRDD[4] at intersection at Union.scala:20), which has no missing parents
[INFO][2021-06-02 14:10:10,444][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,444][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 65, localhost, executor driver, partition 1, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,445][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 2.0 (TID 66, localhost, executor driver, partition 2, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,445][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 2.0 (TID 67, localhost, executor driver, partition 3, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,446][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 2.0 (TID 68, localhost, executor driver, partition 4, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,446][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 2.6 KB, free 4.0 GB)
[INFO][2021-06-02 14:10:10,446][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 2.0 (TID 69, localhost, executor driver, partition 5, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,446][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 2.0 (TID 70, localhost, executor driver, partition 6, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,447][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 2.0 (TID 71, localhost, executor driver, partition 7, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,447][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 2.0 (TID 72, localhost, executor driver, partition 8, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,447][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 2.0 (TID 73, localhost, executor driver, partition 9, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,447][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1659.0 B, free 4.0 GB)
[INFO][2021-06-02 14:10:10,448][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 2.0 (TID 74, localhost, executor driver, partition 10, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,448][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 2.0 (TID 75, localhost, executor driver, partition 11, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,448][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.52.10:55696 (size: 1659.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:10:10,449][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 2.0 (TID 76, localhost, executor driver, partition 12, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,449][org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:10:10,449][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 2.0 (TID 77, localhost, executor driver, partition 13, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,449][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 2.0 (TID 78, localhost, executor driver, partition 14, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,449][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 16 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[4] at intersection at Union.scala:20) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:10:10,450][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 16 tasks
[INFO][2021-06-02 14:10:10,450][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 2.0 (TID 79, localhost, executor driver, partition 15, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 2.0 (TID 67)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 2.0 (TID 79)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 2.0 (TID 78)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 2.0 (TID 77)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 2.0 (TID 74)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 2.0 (TID 76)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 2.0 (TID 66)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 2.0 (TID 75)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 2.0 (TID 71)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 2.0 (TID 73)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 2.0 (TID 72)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 2.0 (TID 70)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 2.0 (TID 68)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 64)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 2.0 (TID 69)
[INFO][2021-06-02 14:10:10,450][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 65)
[INFO][2021-06-02 14:10:10,459][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 2.0 (TID 74). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,460][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 80, localhost, executor driver, partition 0, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,461][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 2.0 (TID 74) in 13 ms on localhost (executor driver) (1/16)
[INFO][2021-06-02 14:10:10,462][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 64). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,461][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 80)
[INFO][2021-06-02 14:10:10,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 3.0 (TID 81, localhost, executor driver, partition 1, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,463][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 3.0 (TID 81)
[INFO][2021-06-02 14:10:10,463][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 64) in 19 ms on localhost (executor driver) (2/16)
[INFO][2021-06-02 14:10:10,467][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 2.0 (TID 73). 739 bytes result sent to driver
[INFO][2021-06-02 14:10:10,468][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 3.0 (TID 82, localhost, executor driver, partition 2, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,469][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 2.0 (TID 73) in 21 ms on localhost (executor driver) (3/16)
[INFO][2021-06-02 14:10:10,469][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 3.0 (TID 82)
[INFO][2021-06-02 14:10:10,469][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 2.0 (TID 76). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,472][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 3.0 (TID 83, localhost, executor driver, partition 3, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,473][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 3.0 (TID 83)
[INFO][2021-06-02 14:10:10,474][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 2.0 (TID 69). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,480][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 2.0 (TID 76) in 32 ms on localhost (executor driver) (4/16)
[INFO][2021-06-02 14:10:10,481][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 3.0 (TID 84, localhost, executor driver, partition 4, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,481][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 3.0 (TID 84)
[INFO][2021-06-02 14:10:10,486][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 2.0 (TID 69) in 40 ms on localhost (executor driver) (5/16)
[INFO][2021-06-02 14:10:10,506][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 2.0 (TID 66). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,508][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 3.0 (TID 85, localhost, executor driver, partition 5, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,509][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 2.0 (TID 66) in 65 ms on localhost (executor driver) (6/16)
[INFO][2021-06-02 14:10:10,509][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 2.0 (TID 72). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,511][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 3.0 (TID 86, localhost, executor driver, partition 6, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,512][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 3.0 (TID 85)
[INFO][2021-06-02 14:10:10,513][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 2.0 (TID 72) in 66 ms on localhost (executor driver) (7/16)
[INFO][2021-06-02 14:10:10,515][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 3.0 (TID 86)
[INFO][2021-06-02 14:10:10,521][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 3.0 (TID 84). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,522][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 3.0 (TID 87, localhost, executor driver, partition 7, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,523][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 3.0 (TID 87)
[INFO][2021-06-02 14:10:10,523][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 3.0 (TID 84) in 42 ms on localhost (executor driver) (1/16)
[INFO][2021-06-02 14:10:10,528][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 2.0 (TID 75). 868 bytes result sent to driver
[INFO][2021-06-02 14:10:10,529][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 3.0 (TID 88, localhost, executor driver, partition 8, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,529][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 80). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,529][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 3.0 (TID 88)
[INFO][2021-06-02 14:10:10,530][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 3.0 (TID 89, localhost, executor driver, partition 9, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,530][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 2.0 (TID 75) in 82 ms on localhost (executor driver) (8/16)
[INFO][2021-06-02 14:10:10,530][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 3.0 (TID 89)
[INFO][2021-06-02 14:10:10,530][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 80) in 70 ms on localhost (executor driver) (2/16)
[INFO][2021-06-02 14:10:10,531][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 65). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,532][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 3.0 (TID 90, localhost, executor driver, partition 10, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,532][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 2.0 (TID 77). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,533][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 3.0 (TID 90)
[INFO][2021-06-02 14:10:10,533][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 3.0 (TID 91, localhost, executor driver, partition 11, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,533][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 2.0 (TID 77) in 84 ms on localhost (executor driver) (9/16)
[INFO][2021-06-02 14:10:10,533][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 3.0 (TID 91)
[INFO][2021-06-02 14:10:10,534][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 65) in 90 ms on localhost (executor driver) (10/16)
[INFO][2021-06-02 14:10:10,536][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 3.0 (TID 81). 739 bytes result sent to driver
[INFO][2021-06-02 14:10:10,537][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 3.0 (TID 82). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,538][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 3.0 (TID 92, localhost, executor driver, partition 12, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,539][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 3.0 (TID 92)
[INFO][2021-06-02 14:10:10,539][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 3.0 (TID 93, localhost, executor driver, partition 13, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,540][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 3.0 (TID 93)
[INFO][2021-06-02 14:10:10,540][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 3.0 (TID 81) in 78 ms on localhost (executor driver) (3/16)
[INFO][2021-06-02 14:10:10,541][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 3.0 (TID 82) in 73 ms on localhost (executor driver) (4/16)
[INFO][2021-06-02 14:10:10,544][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 3.0 (TID 87). 911 bytes result sent to driver
[INFO][2021-06-02 14:10:10,545][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 3.0 (TID 94, localhost, executor driver, partition 14, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:10:10,545][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 2.0 (TID 78). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,546][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 3.0 (TID 87) in 24 ms on localhost (executor driver) (5/16)
[INFO][2021-06-02 14:10:10,545][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 3.0 (TID 94)
[INFO][2021-06-02 14:10:10,547][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 3.0 (TID 95, localhost, executor driver, partition 15, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:10:10,547][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 2.0 (TID 68). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,547][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 2.0 (TID 70). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,547][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 2.0 (TID 78) in 98 ms on localhost (executor driver) (11/16)
[INFO][2021-06-02 14:10:10,548][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 3.0 (TID 95)
[INFO][2021-06-02 14:10:10,550][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 2.0 (TID 68) in 104 ms on localhost (executor driver) (12/16)
[INFO][2021-06-02 14:10:10,551][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 2.0 (TID 70) in 104 ms on localhost (executor driver) (13/16)
[INFO][2021-06-02 14:10:10,552][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 3.0 (TID 91). 868 bytes result sent to driver
[INFO][2021-06-02 14:10:10,554][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 3.0 (TID 93). 782 bytes result sent to driver
[INFO][2021-06-02 14:10:10,555][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 3.0 (TID 92). 739 bytes result sent to driver
[INFO][2021-06-02 14:10:10,556][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 3.0 (TID 91) in 23 ms on localhost (executor driver) (6/16)
[INFO][2021-06-02 14:10:10,556][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 3.0 (TID 93) in 17 ms on localhost (executor driver) (7/16)
[INFO][2021-06-02 14:10:10,557][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 3.0 (TID 92) in 19 ms on localhost (executor driver) (8/16)
[INFO][2021-06-02 14:10:10,559][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 2.0 (TID 79). 868 bytes result sent to driver
[INFO][2021-06-02 14:10:10,559][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 2.0 (TID 79) in 109 ms on localhost (executor driver) (14/16)
[INFO][2021-06-02 14:10:10,560][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 3.0 (TID 90). 739 bytes result sent to driver
[INFO][2021-06-02 14:10:10,561][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 3.0 (TID 89). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,561][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 3.0 (TID 90) in 29 ms on localhost (executor driver) (9/16)
[INFO][2021-06-02 14:10:10,562][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 3.0 (TID 89) in 33 ms on localhost (executor driver) (10/16)
[INFO][2021-06-02 14:10:10,562][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 3.0 (TID 88). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,563][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 3.0 (TID 88) in 35 ms on localhost (executor driver) (11/16)
[INFO][2021-06-02 14:10:10,564][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 3.0 (TID 83). 868 bytes result sent to driver
[INFO][2021-06-02 14:10:10,565][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 3.0 (TID 83) in 93 ms on localhost (executor driver) (12/16)
[INFO][2021-06-02 14:10:10,566][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 2.0 (TID 71). 868 bytes result sent to driver
[INFO][2021-06-02 14:10:10,567][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 2.0 (TID 71) in 120 ms on localhost (executor driver) (15/16)
[INFO][2021-06-02 14:10:10,569][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 2.0 (TID 67). 825 bytes result sent to driver
[INFO][2021-06-02 14:10:10,570][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 2.0 (TID 67) in 125 ms on localhost (executor driver) (16/16)
[INFO][2021-06-02 14:10:10,570][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 3.0 (TID 86). 739 bytes result sent to driver
[INFO][2021-06-02 14:10:10,570][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:10:10,570][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 2 (intersection at Union.scala:20) finished in 0.127 s
[INFO][2021-06-02 14:10:10,571][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 14:10:10,571][org.apache.spark.scheduler.DAGScheduler:54] - running: Set(ShuffleMapStage 3)
[INFO][2021-06-02 14:10:10,571][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 4)
[INFO][2021-06-02 14:10:10,571][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 14:10:10,572][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 3.0 (TID 86) in 61 ms on localhost (executor driver) (13/16)
[INFO][2021-06-02 14:10:10,573][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 3.0 (TID 85). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,573][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 3.0 (TID 85) in 66 ms on localhost (executor driver) (14/16)
[INFO][2021-06-02 14:10:10,575][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 3.0 (TID 95). 868 bytes result sent to driver
[INFO][2021-06-02 14:10:10,575][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 3.0 (TID 95) in 29 ms on localhost (executor driver) (15/16)
[INFO][2021-06-02 14:10:10,576][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 3.0 (TID 94). 696 bytes result sent to driver
[INFO][2021-06-02 14:10:10,576][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 3.0 (TID 94) in 31 ms on localhost (executor driver) (16/16)
[INFO][2021-06-02 14:10:10,577][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:10:10,577][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 3 (intersection at Union.scala:20) finished in 0.127 s
[INFO][2021-06-02 14:10:10,577][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 14:10:10,577][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 14:10:10,577][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 4)
[INFO][2021-06-02 14:10:10,577][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 14:10:10,578][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[8] at intersection at Union.scala:20), which has no missing parents
[INFO][2021-06-02 14:10:10,580][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 4.0 GB)
[INFO][2021-06-02 14:10:10,589][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2022.0 B, free 4.0 GB)
[INFO][2021-06-02 14:10:10,590][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.52.10:55696 (size: 2022.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:10:10,590][org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:10:10,591][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 16 missing tasks from ResultStage 4 (MapPartitionsRDD[8] at intersection at Union.scala:20) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:10:10,591][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 16 tasks
[INFO][2021-06-02 14:10:10,593][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 96, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,594][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 4.0 (TID 97, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,594][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 4.0 (TID 98, localhost, executor driver, partition 2, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,595][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 4.0 (TID 99, localhost, executor driver, partition 3, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,595][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 4.0 (TID 100, localhost, executor driver, partition 4, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,596][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 4.0 (TID 101, localhost, executor driver, partition 5, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,596][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 4.0 (TID 102, localhost, executor driver, partition 6, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,597][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 4.0 (TID 103, localhost, executor driver, partition 7, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,598][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 4.0 (TID 104, localhost, executor driver, partition 8, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,598][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 4.0 (TID 105, localhost, executor driver, partition 9, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,599][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 4.0 (TID 106, localhost, executor driver, partition 10, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,599][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 4.0 (TID 107, localhost, executor driver, partition 11, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,599][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 4.0 (TID 108, localhost, executor driver, partition 12, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,599][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 4.0 (TID 109, localhost, executor driver, partition 13, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,599][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 4.0 (TID 110, localhost, executor driver, partition 14, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 4.0 (TID 111, localhost, executor driver, partition 15, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 96)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 4.0 (TID 100)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 4.0 (TID 104)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 4.0 (TID 98)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 4.0 (TID 103)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 4.0 (TID 108)
[INFO][2021-06-02 14:10:10,601][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 4.0 (TID 109)
[INFO][2021-06-02 14:10:10,601][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 4.0 (TID 110)
[INFO][2021-06-02 14:10:10,602][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 4.0 (TID 111)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 4.0 (TID 99)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 4.0 (TID 106)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 4.0 (TID 107)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 4.0 (TID 105)
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 4.0 (TID 97)
[INFO][2021-06-02 14:10:10,604][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 4.0 (TID 102)
[INFO][2021-06-02 14:10:10,605][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,605][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,605][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,605][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,606][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,606][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,606][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,600][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 4.0 (TID 101)
[INFO][2021-06-02 14:10:10,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,606][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,606][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,604][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,604][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,604][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,612][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,613][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,609][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,613][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 4.0 (TID 111). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,613][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,614][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,612][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:10:10,612][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,612][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,612][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,611][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,615][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 4.0 (TID 106). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,611][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,611][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:10:10,610][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,615][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,615][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 14:10:10,615][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,615][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 4.0 (TID 108). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,615][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,614][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:10:10,614][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 4.0 (TID 111) in 14 ms on localhost (executor driver) (1/16)
[INFO][2021-06-02 14:10:10,614][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 96). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,618][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 4.0 (TID 109). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,613][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,619][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 14:10:10,620][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 4.0 (TID 110). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,620][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.52.10:55696 in memory (size: 2.3 KB, free: 4.0 GB)
[INFO][2021-06-02 14:10:10,613][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 4.0 (TID 107). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,619][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 4.0 (TID 102). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,619][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 4.0 (TID 99). 1039 bytes result sent to driver
[INFO][2021-06-02 14:10:10,618][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 4.0 (TID 103). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,617][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,617][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:10:10,621][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,621][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 4.0 (TID 104). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,621][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 4.0 (TID 105). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,620][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 4.0 (TID 108) in 21 ms on localhost (executor driver) (2/16)
[INFO][2021-06-02 14:10:10,620][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 4.0 (TID 100). 996 bytes result sent to driver
[INFO][2021-06-02 14:10:10,622][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 4.0 (TID 110) in 23 ms on localhost (executor driver) (3/16)
[INFO][2021-06-02 14:10:10,621][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 14:10:10,623][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 4.0 (TID 97). 1035 bytes result sent to driver
[INFO][2021-06-02 14:10:10,622][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 4.0 (TID 106) in 23 ms on localhost (executor driver) (4/16)
[INFO][2021-06-02 14:10:10,623][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 4.0 (TID 109) in 24 ms on localhost (executor driver) (5/16)
[INFO][2021-06-02 14:10:10,623][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 4.0 (TID 101). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,624][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 96) in 32 ms on localhost (executor driver) (6/16)
[INFO][2021-06-02 14:10:10,625][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 4.0 (TID 107) in 26 ms on localhost (executor driver) (7/16)
[INFO][2021-06-02 14:10:10,625][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 4.0 (TID 99) in 30 ms on localhost (executor driver) (8/16)
[INFO][2021-06-02 14:10:10,625][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 4.0 (TID 98). 992 bytes result sent to driver
[INFO][2021-06-02 14:10:10,625][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 4.0 (TID 102) in 29 ms on localhost (executor driver) (9/16)
[INFO][2021-06-02 14:10:10,626][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 4.0 (TID 103) in 29 ms on localhost (executor driver) (10/16)
[INFO][2021-06-02 14:10:10,627][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 4.0 (TID 104) in 29 ms on localhost (executor driver) (11/16)
[INFO][2021-06-02 14:10:10,627][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.52.10:55696 in memory (size: 1664.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:10:10,627][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 4.0 (TID 100) in 32 ms on localhost (executor driver) (12/16)
[INFO][2021-06-02 14:10:10,627][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 4.0 (TID 97) in 33 ms on localhost (executor driver) (13/16)
[INFO][2021-06-02 14:10:10,627][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 4.0 (TID 105) in 29 ms on localhost (executor driver) (14/16)
[INFO][2021-06-02 14:10:10,629][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 4.0 (TID 101) in 32 ms on localhost (executor driver) (15/16)
[INFO][2021-06-02 14:10:10,629][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 4.0 (TID 98) in 35 ms on localhost (executor driver) (16/16)
[INFO][2021-06-02 14:10:10,629][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:10:10,629][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (collect at Union.scala:23) finished in 0.037 s
[INFO][2021-06-02 14:10:10,630][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: collect at Union.scala:23, took 0.194973 s
[INFO][2021-06-02 14:10:10,634][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 14:10:10,641][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@63648ee9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 14:10:10,643][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 14:10:10,656][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 14:10:10,758][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 14:10:10,759][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 14:10:10,760][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 14:10:10,762][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 14:10:10,765][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 14:10:10,766][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 14:10:10,766][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-33b67eb7-0f6d-44cb-b028-091701f4e60c
[INFO][2021-06-02 14:13:33,475][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 14:13:34,012][org.apache.spark.SparkContext:54] - Submitted application: Union
[INFO][2021-06-02 14:13:34,041][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 14:13:34,044][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 14:13:34,044][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 14:13:34,045][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 14:13:34,045][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 14:13:37,061][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59952.
[INFO][2021-06-02 14:13:37,082][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 14:13:37,132][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 14:13:37,135][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 14:13:37,136][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 14:13:37,148][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-8f6b8def-53fb-4b97-a77f-c3a977d209e7
[INFO][2021-06-02 14:13:37,167][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 4.0 GB
[INFO][2021-06-02 14:13:37,211][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 14:13:37,284][org.spark_project.jetty.util.log:192] - Logging initialized @4868ms
[INFO][2021-06-02 14:13:37,333][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 14:13:37,344][org.spark_project.jetty.server.Server:403] - Started @4929ms
[INFO][2021-06-02 14:13:37,359][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@63648ee9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 14:13:37,360][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 14:13:37,379][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7fcbe147{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,379][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@89c10b7{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,380][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fe89c24{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,381][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@119f1f2a{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,382][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b970f7{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,382][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b8a71{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,383][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f058b8a{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,384][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@76c7beb3{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,385][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cf92cc7{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,385][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7b139eab{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,386][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@611df6e3{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,387][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6273c5a4{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,388][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53e211ee{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,389][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3d8bbcdc{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,389][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@117e0fe5{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,390][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@78aea4b9{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,391][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b85880b{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,392][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4215838f{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,393][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2289aca5{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,393][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@184497d1{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,399][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ffab045{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,400][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5e8f9e2d{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,401][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@fd46303{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,402][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e2822{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,403][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@29a60c27{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,405][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 14:13:37,489][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 14:13:37,519][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50649.
[INFO][2021-06-02 14:13:37,519][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:50649
[INFO][2021-06-02 14:13:37,521][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 14:13:37,522][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 50649, None)
[INFO][2021-06-02 14:13:37,525][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:50649 with 4.0 GB RAM, BlockManagerId(driver, 192.168.52.10, 50649, None)
[INFO][2021-06-02 14:13:37,527][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 50649, None)
[INFO][2021-06-02 14:13:37,528][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 50649, None)
[INFO][2021-06-02 14:13:37,652][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@15b986cd{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 14:13:37,923][org.apache.spark.SparkContext:54] - Starting job: collect at Union.scala:19
[INFO][2021-06-02 14:13:37,940][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at Union.scala:19) with 32 output partitions
[INFO][2021-06-02 14:13:37,941][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at Union.scala:19)
[INFO][2021-06-02 14:13:37,942][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-02 14:13:37,944][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 14:13:37,947][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (UnionRDD[2] at union at Union.scala:18), which has no missing parents
[INFO][2021-06-02 14:13:38,029][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 2000.0 B, free 4.0 GB)
[INFO][2021-06-02 14:13:38,058][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1319.0 B, free 4.0 GB)
[INFO][2021-06-02 14:13:38,060][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:50649 (size: 1319.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:13:38,063][org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:13:38,074][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 32 missing tasks from ResultStage 0 (UnionRDD[2] at union at Union.scala:18) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:13:38,075][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 32 tasks
[INFO][2021-06-02 14:13:38,106][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,107][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,108][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,108][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,109][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,109][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,110][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,111][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,111][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,112][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,112][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,113][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,114][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,114][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,115][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,117][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 0.0 (TID 15, localhost, executor driver, partition 15, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 0.0 (TID 7)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 0.0 (TID 4)
[INFO][2021-06-02 14:13:38,125][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 0.0 (TID 13)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 0.0 (TID 10)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 0.0 (TID 5)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 0.0 (TID 8)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 0.0 (TID 6)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 0.0 (TID 3)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
[INFO][2021-06-02 14:13:38,124][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 0.0 (TID 9)
[INFO][2021-06-02 14:13:38,126][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 0.0 (TID 15)
[INFO][2021-06-02 14:13:38,126][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 0.0 (TID 14)
[INFO][2021-06-02 14:13:38,125][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 0.0 (TID 12)
[INFO][2021-06-02 14:13:38,125][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 0.0 (TID 11)
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 0.0 (TID 7). 652 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 0.0 (TID 10). 691 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 0.0 (TID 4). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 691 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 0.0 (TID 9). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 0.0 (TID 5). 691 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 0.0 (TID 6). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 0.0 (TID 3). 652 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 0.0 (TID 15). 695 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 0.0 (TID 14). 691 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 0.0 (TID 13). 691 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 0.0 (TID 8). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 0.0 (TID 11). 652 bytes result sent to driver
[INFO][2021-06-02 14:13:38,208][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 0.0 (TID 12). 691 bytes result sent to driver
[INFO][2021-06-02 14:13:38,211][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 0.0 (TID 16, localhost, executor driver, partition 16, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,211][org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 0.0 (TID 16)
[INFO][2021-06-02 14:13:38,212][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 0.0 (TID 17, localhost, executor driver, partition 17, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,212][org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 0.0 (TID 17)
[INFO][2021-06-02 14:13:38,214][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 0.0 (TID 18, localhost, executor driver, partition 18, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,214][org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 0.0 (TID 18)
[INFO][2021-06-02 14:13:38,216][org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 0.0 (TID 17). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,217][org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 0.0 (TID 16). 648 bytes result sent to driver
[INFO][2021-06-02 14:13:38,217][org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 0.0 (TID 18). 562 bytes result sent to driver
[INFO][2021-06-02 14:13:38,217][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 0.0 (TID 10) in 103 ms on localhost (executor driver) (1/32)
[INFO][2021-06-02 14:13:38,219][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 0.0 (TID 7) in 108 ms on localhost (executor driver) (2/32)
[INFO][2021-06-02 14:13:38,219][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 0.0 (TID 4) in 110 ms on localhost (executor driver) (3/32)
[INFO][2021-06-02 14:13:38,220][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 0.0 (TID 19, localhost, executor driver, partition 19, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,220][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 125 ms on localhost (executor driver) (4/32)
[INFO][2021-06-02 14:13:38,220][org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 0.0 (TID 19)
[INFO][2021-06-02 14:13:38,221][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 0.0 (TID 20, localhost, executor driver, partition 20, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,221][org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 0.0 (TID 20)
[INFO][2021-06-02 14:13:38,221][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 0.0 (TID 9) in 109 ms on localhost (executor driver) (5/32)
[INFO][2021-06-02 14:13:38,222][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 0.0 (TID 21, localhost, executor driver, partition 21, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,223][org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 0.0 (TID 21)
[INFO][2021-06-02 14:13:38,223][org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 0.0 (TID 19). 609 bytes result sent to driver
[INFO][2021-06-02 14:13:38,224][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 0.0 (TID 22, localhost, executor driver, partition 22, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,224][org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 0.0 (TID 20). 605 bytes result sent to driver
[INFO][2021-06-02 14:13:38,225][org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 0.0 (TID 22)
[INFO][2021-06-02 14:13:38,225][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 0.0 (TID 23, localhost, executor driver, partition 23, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,226][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 0.0 (TID 5) in 116 ms on localhost (executor driver) (6/32)
[INFO][2021-06-02 14:13:38,226][org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 0.0 (TID 23)
[INFO][2021-06-02 14:13:38,227][org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 0.0 (TID 21). 605 bytes result sent to driver
[INFO][2021-06-02 14:13:38,227][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 0.0 (TID 24, localhost, executor driver, partition 24, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,228][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 0.0 (TID 6) in 117 ms on localhost (executor driver) (7/32)
[INFO][2021-06-02 14:13:38,228][org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 0.0 (TID 22). 605 bytes result sent to driver
[INFO][2021-06-02 14:13:38,228][org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 0.0 (TID 24)
[INFO][2021-06-02 14:13:38,228][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 0.0 (TID 25, localhost, executor driver, partition 25, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,229][org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 0.0 (TID 23). 609 bytes result sent to driver
[INFO][2021-06-02 14:13:38,229][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 122 ms on localhost (executor driver) (8/32)
[INFO][2021-06-02 14:13:38,230][org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 0.0 (TID 25)
[INFO][2021-06-02 14:13:38,230][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 122 ms on localhost (executor driver) (9/32)
[INFO][2021-06-02 14:13:38,231][org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 0.0 (TID 24). 562 bytes result sent to driver
[INFO][2021-06-02 14:13:38,231][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 0.0 (TID 26, localhost, executor driver, partition 26, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,232][org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 0.0 (TID 26)
[INFO][2021-06-02 14:13:38,232][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 0.0 (TID 27, localhost, executor driver, partition 27, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,233][org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 0.0 (TID 27)
[INFO][2021-06-02 14:13:38,233][org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 0.0 (TID 25). 562 bytes result sent to driver
[INFO][2021-06-02 14:13:38,233][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 0.0 (TID 28, localhost, executor driver, partition 28, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,234][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 0.0 (TID 14) in 119 ms on localhost (executor driver) (10/32)
[INFO][2021-06-02 14:13:38,234][org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 0.0 (TID 28)
[INFO][2021-06-02 14:13:38,235][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 0.0 (TID 15) in 120 ms on localhost (executor driver) (11/32)
[INFO][2021-06-02 14:13:38,235][org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 0.0 (TID 26). 605 bytes result sent to driver
[INFO][2021-06-02 14:13:38,236][org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 0.0 (TID 27). 609 bytes result sent to driver
[INFO][2021-06-02 14:13:38,236][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 0.0 (TID 29, localhost, executor driver, partition 29, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,236][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 0.0 (TID 13) in 122 ms on localhost (executor driver) (12/32)
[INFO][2021-06-02 14:13:38,237][org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 0.0 (TID 29)
[INFO][2021-06-02 14:13:38,237][org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 0.0 (TID 28). 605 bytes result sent to driver
[INFO][2021-06-02 14:13:38,237][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 0.0 (TID 30, localhost, executor driver, partition 30, PROCESS_LOCAL, 4861 bytes)
[INFO][2021-06-02 14:13:38,238][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 131 ms on localhost (executor driver) (13/32)
[INFO][2021-06-02 14:13:38,238][org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 0.0 (TID 30)
[INFO][2021-06-02 14:13:38,239][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 0.0 (TID 31, localhost, executor driver, partition 31, PROCESS_LOCAL, 4865 bytes)
[INFO][2021-06-02 14:13:38,240][org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 0.0 (TID 29). 562 bytes result sent to driver
[INFO][2021-06-02 14:13:38,240][org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 0.0 (TID 31)
[INFO][2021-06-02 14:13:38,241][org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 0.0 (TID 30). 605 bytes result sent to driver
[INFO][2021-06-02 14:13:38,242][org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 0.0 (TID 31). 609 bytes result sent to driver
[INFO][2021-06-02 14:13:38,243][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 0.0 (TID 8) in 132 ms on localhost (executor driver) (14/32)
[INFO][2021-06-02 14:13:38,244][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 0.0 (TID 11) in 131 ms on localhost (executor driver) (15/32)
[INFO][2021-06-02 14:13:38,244][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 0.0 (TID 17) in 32 ms on localhost (executor driver) (16/32)
[INFO][2021-06-02 14:13:38,244][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 0.0 (TID 12) in 131 ms on localhost (executor driver) (17/32)
[INFO][2021-06-02 14:13:38,245][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 0.0 (TID 16) in 35 ms on localhost (executor driver) (18/32)
[INFO][2021-06-02 14:13:38,246][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 0.0 (TID 18) in 33 ms on localhost (executor driver) (19/32)
[INFO][2021-06-02 14:13:38,246][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 0.0 (TID 19) in 27 ms on localhost (executor driver) (20/32)
[INFO][2021-06-02 14:13:38,247][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 0.0 (TID 20) in 25 ms on localhost (executor driver) (21/32)
[INFO][2021-06-02 14:13:38,247][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 0.0 (TID 21) in 25 ms on localhost (executor driver) (22/32)
[INFO][2021-06-02 14:13:38,248][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 0.0 (TID 22) in 25 ms on localhost (executor driver) (23/32)
[INFO][2021-06-02 14:13:38,248][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 0.0 (TID 23) in 23 ms on localhost (executor driver) (24/32)
[INFO][2021-06-02 14:13:38,249][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 0.0 (TID 24) in 23 ms on localhost (executor driver) (25/32)
[INFO][2021-06-02 14:13:38,250][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 0.0 (TID 25) in 22 ms on localhost (executor driver) (26/32)
[INFO][2021-06-02 14:13:38,250][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 0.0 (TID 26) in 19 ms on localhost (executor driver) (27/32)
[INFO][2021-06-02 14:13:38,251][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 0.0 (TID 27) in 18 ms on localhost (executor driver) (28/32)
[INFO][2021-06-02 14:13:38,251][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 0.0 (TID 28) in 18 ms on localhost (executor driver) (29/32)
[INFO][2021-06-02 14:13:38,251][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 0.0 (TID 29) in 15 ms on localhost (executor driver) (30/32)
[INFO][2021-06-02 14:13:38,252][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 0.0 (TID 30) in 15 ms on localhost (executor driver) (31/32)
[INFO][2021-06-02 14:13:38,253][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 0.0 (TID 31) in 14 ms on localhost (executor driver) (32/32)
[INFO][2021-06-02 14:13:38,255][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:13:38,256][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at Union.scala:19) finished in 0.169 s
[INFO][2021-06-02 14:13:38,262][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at Union.scala:19, took 0.337656 s
[INFO][2021-06-02 14:13:38,300][org.apache.spark.SparkContext:54] - Starting job: collect at Union.scala:22
[INFO][2021-06-02 14:13:38,303][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 4 (intersection at Union.scala:21)
[INFO][2021-06-02 14:13:38,303][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (intersection at Union.scala:21)
[INFO][2021-06-02 14:13:38,304][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (collect at Union.scala:22) with 16 output partitions
[INFO][2021-06-02 14:13:38,304][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (collect at Union.scala:22)
[INFO][2021-06-02 14:13:38,304][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO][2021-06-02 14:13:38,304][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO][2021-06-02 14:13:38,305][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at intersection at Union.scala:21), which has no missing parents
[INFO][2021-06-02 14:13:38,307][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 2.6 KB, free 4.0 GB)
[INFO][2021-06-02 14:13:38,308][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1659.0 B, free 4.0 GB)
[INFO][2021-06-02 14:13:38,309][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:50649 (size: 1659.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:13:38,310][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:13:38,311][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 16 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at intersection at Union.scala:21) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:13:38,312][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 16 tasks
[INFO][2021-06-02 14:13:38,312][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 2 (MapPartitionsRDD[3] at intersection at Union.scala:21), which has no missing parents
[INFO][2021-06-02 14:13:38,313][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,314][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 33, localhost, executor driver, partition 1, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,314][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 2.6 KB, free 4.0 GB)
[INFO][2021-06-02 14:13:38,314][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 1.0 (TID 34, localhost, executor driver, partition 2, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,315][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 1.0 (TID 35, localhost, executor driver, partition 3, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,315][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 1.0 (TID 36, localhost, executor driver, partition 4, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,315][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 1.0 (TID 37, localhost, executor driver, partition 5, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,316][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1666.0 B, free 4.0 GB)
[INFO][2021-06-02 14:13:38,316][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 1.0 (TID 38, localhost, executor driver, partition 6, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,316][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 1.0 (TID 39, localhost, executor driver, partition 7, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,317][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 1.0 (TID 40, localhost, executor driver, partition 8, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,317][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.52.10:50649 (size: 1666.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:13:38,317][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 1.0 (TID 41, localhost, executor driver, partition 9, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,317][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 1.0 (TID 42, localhost, executor driver, partition 10, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,317][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:13:38,317][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 1.0 (TID 43, localhost, executor driver, partition 11, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,318][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 1.0 (TID 44, localhost, executor driver, partition 12, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,318][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 16 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[3] at intersection at Union.scala:21) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:13:38,318][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 16 tasks
[INFO][2021-06-02 14:13:38,318][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 1.0 (TID 45, localhost, executor driver, partition 13, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 1.0 (TID 46, localhost, executor driver, partition 14, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 1.0 (TID 47, localhost, executor driver, partition 15, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 32)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 1.0 (TID 41)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 1.0 (TID 44)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 1.0 (TID 43)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 1.0 (TID 40)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 1.0 (TID 42)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 1.0 (TID 35)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 1.0 (TID 34)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 33)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 1.0 (TID 37)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 1.0 (TID 39)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 1.0 (TID 38)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 1.0 (TID 36)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 1.0 (TID 47)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 1.0 (TID 46)
[INFO][2021-06-02 14:13:38,319][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 1.0 (TID 45)
[INFO][2021-06-02 14:13:38,380][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 1.0 (TID 41). 782 bytes result sent to driver
[INFO][2021-06-02 14:13:38,380][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 32). 782 bytes result sent to driver
[INFO][2021-06-02 14:13:38,380][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 1.0 (TID 36). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,381][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 48)
[INFO][2021-06-02 14:13:38,382][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 49, localhost, executor driver, partition 1, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,383][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 1.0 (TID 38). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,383][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 49)
[INFO][2021-06-02 14:13:38,383][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 2.0 (TID 50, localhost, executor driver, partition 2, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,384][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 32) in 71 ms on localhost (executor driver) (1/16)
[INFO][2021-06-02 14:13:38,384][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 2.0 (TID 50)
[INFO][2021-06-02 14:13:38,384][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 1.0 (TID 41) in 67 ms on localhost (executor driver) (2/16)
[INFO][2021-06-02 14:13:38,384][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 1.0 (TID 36) in 69 ms on localhost (executor driver) (3/16)
[INFO][2021-06-02 14:13:38,385][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 1.0 (TID 44). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,386][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 2.0 (TID 51, localhost, executor driver, partition 3, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,386][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 1.0 (TID 40). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,386][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 2.0 (TID 51)
[INFO][2021-06-02 14:13:38,387][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 2.0 (TID 52, localhost, executor driver, partition 4, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,387][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 2.0 (TID 52)
[INFO][2021-06-02 14:13:38,387][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 1.0 (TID 38) in 72 ms on localhost (executor driver) (4/16)
[INFO][2021-06-02 14:13:38,388][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 2.0 (TID 53, localhost, executor driver, partition 5, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,388][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 1.0 (TID 45). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,389][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 1.0 (TID 44) in 71 ms on localhost (executor driver) (5/16)
[INFO][2021-06-02 14:13:38,389][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 2.0 (TID 53)
[INFO][2021-06-02 14:13:38,390][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 2.0 (TID 54, localhost, executor driver, partition 6, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,391][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 1.0 (TID 40) in 75 ms on localhost (executor driver) (6/16)
[INFO][2021-06-02 14:13:38,391][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 2.0 (TID 54)
[INFO][2021-06-02 14:13:38,391][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 33). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,393][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 1.0 (TID 45) in 74 ms on localhost (executor driver) (7/16)
[INFO][2021-06-02 14:13:38,394][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 2.0 (TID 55, localhost, executor driver, partition 7, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,395][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 33) in 81 ms on localhost (executor driver) (8/16)
[INFO][2021-06-02 14:13:38,395][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 2.0 (TID 55)
[INFO][2021-06-02 14:13:38,395][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 1.0 (TID 42). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,396][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 2.0 (TID 56, localhost, executor driver, partition 8, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,396][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 2.0 (TID 56)
[INFO][2021-06-02 14:13:38,397][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 1.0 (TID 42) in 79 ms on localhost (executor driver) (9/16)
[INFO][2021-06-02 14:13:38,397][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 1.0 (TID 37). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,398][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 1.0 (TID 34). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,399][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 2.0 (TID 57, localhost, executor driver, partition 9, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,399][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 2.0 (TID 57)
[INFO][2021-06-02 14:13:38,399][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 2.0 (TID 58, localhost, executor driver, partition 10, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,400][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 2.0 (TID 58)
[INFO][2021-06-02 14:13:38,400][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 1.0 (TID 37) in 85 ms on localhost (executor driver) (10/16)
[INFO][2021-06-02 14:13:38,401][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 1.0 (TID 46). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,401][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 1.0 (TID 34) in 87 ms on localhost (executor driver) (11/16)
[INFO][2021-06-02 14:13:38,402][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 2.0 (TID 59, localhost, executor driver, partition 11, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,403][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 2.0 (TID 59)
[INFO][2021-06-02 14:13:38,403][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 1.0 (TID 46) in 85 ms on localhost (executor driver) (12/16)
[INFO][2021-06-02 14:13:38,405][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 49). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,405][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 2.0 (TID 53). 782 bytes result sent to driver
[INFO][2021-06-02 14:13:38,407][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 2.0 (TID 50). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,408][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 2.0 (TID 60, localhost, executor driver, partition 12, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,408][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 2.0 (TID 60)
[INFO][2021-06-02 14:13:38,410][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 2.0 (TID 61, localhost, executor driver, partition 13, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,410][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 2.0 (TID 58). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,410][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 2.0 (TID 52). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,411][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 49) in 29 ms on localhost (executor driver) (1/16)
[INFO][2021-06-02 14:13:38,411][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 2.0 (TID 61)
[INFO][2021-06-02 14:13:38,411][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 2.0 (TID 53) in 23 ms on localhost (executor driver) (2/16)
[INFO][2021-06-02 14:13:38,413][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 2.0 (TID 62, localhost, executor driver, partition 14, PROCESS_LOCAL, 4741 bytes)
[INFO][2021-06-02 14:13:38,413][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 48). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,414][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 2.0 (TID 62)
[INFO][2021-06-02 14:13:38,414][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 2.0 (TID 63, localhost, executor driver, partition 15, PROCESS_LOCAL, 4745 bytes)
[INFO][2021-06-02 14:13:38,415][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 2.0 (TID 56). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,415][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 2.0 (TID 63)
[INFO][2021-06-02 14:13:38,415][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 2.0 (TID 50) in 32 ms on localhost (executor driver) (3/16)
[INFO][2021-06-02 14:13:38,416][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 2.0 (TID 58) in 17 ms on localhost (executor driver) (4/16)
[INFO][2021-06-02 14:13:38,416][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 2.0 (TID 57). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,417][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 2.0 (TID 52) in 30 ms on localhost (executor driver) (5/16)
[INFO][2021-06-02 14:13:38,418][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 2.0 (TID 57) in 20 ms on localhost (executor driver) (6/16)
[INFO][2021-06-02 14:13:38,418][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 48) in 37 ms on localhost (executor driver) (7/16)
[INFO][2021-06-02 14:13:38,418][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 2.0 (TID 54). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,419][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 2.0 (TID 56) in 24 ms on localhost (executor driver) (8/16)
[INFO][2021-06-02 14:13:38,420][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 2.0 (TID 60). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,420][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 2.0 (TID 54) in 30 ms on localhost (executor driver) (9/16)
[INFO][2021-06-02 14:13:38,421][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 2.0 (TID 60) in 14 ms on localhost (executor driver) (10/16)
[INFO][2021-06-02 14:13:38,422][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 2.0 (TID 61). 739 bytes result sent to driver
[INFO][2021-06-02 14:13:38,422][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 2.0 (TID 61) in 13 ms on localhost (executor driver) (11/16)
[INFO][2021-06-02 14:13:38,426][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 2.0 (TID 62). 696 bytes result sent to driver
[INFO][2021-06-02 14:13:38,427][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 2.0 (TID 62) in 14 ms on localhost (executor driver) (12/16)
[INFO][2021-06-02 14:13:38,433][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 1.0 (TID 47). 911 bytes result sent to driver
[INFO][2021-06-02 14:13:38,434][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 1.0 (TID 47) in 115 ms on localhost (executor driver) (13/16)
[INFO][2021-06-02 14:13:38,435][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 1.0 (TID 35). 868 bytes result sent to driver
[INFO][2021-06-02 14:13:38,436][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 1.0 (TID 35) in 122 ms on localhost (executor driver) (14/16)
[INFO][2021-06-02 14:13:38,438][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 1.0 (TID 39). 825 bytes result sent to driver
[INFO][2021-06-02 14:13:38,439][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 1.0 (TID 39) in 122 ms on localhost (executor driver) (15/16)
[INFO][2021-06-02 14:13:38,440][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 2.0 (TID 51). 868 bytes result sent to driver
[INFO][2021-06-02 14:13:38,441][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 2.0 (TID 51) in 56 ms on localhost (executor driver) (13/16)
[INFO][2021-06-02 14:13:38,442][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 2.0 (TID 63). 825 bytes result sent to driver
[INFO][2021-06-02 14:13:38,443][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 2.0 (TID 63) in 29 ms on localhost (executor driver) (14/16)
[INFO][2021-06-02 14:13:38,444][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 2.0 (TID 59). 825 bytes result sent to driver
[INFO][2021-06-02 14:13:38,445][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 2.0 (TID 59) in 43 ms on localhost (executor driver) (15/16)
[INFO][2021-06-02 14:13:38,447][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 1.0 (TID 43). 868 bytes result sent to driver
[INFO][2021-06-02 14:13:38,447][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 1.0 (TID 43) in 130 ms on localhost (executor driver) (16/16)
[INFO][2021-06-02 14:13:38,448][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:13:38,448][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (intersection at Union.scala:21) finished in 0.136 s
[INFO][2021-06-02 14:13:38,449][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 2.0 (TID 55). 825 bytes result sent to driver
[INFO][2021-06-02 14:13:38,449][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 14:13:38,449][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 2.0 (TID 55) in 55 ms on localhost (executor driver) (16/16)
[INFO][2021-06-02 14:13:38,450][org.apache.spark.scheduler.DAGScheduler:54] - running: Set(ShuffleMapStage 2)
[INFO][2021-06-02 14:13:38,450][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:13:38,450][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 3)
[INFO][2021-06-02 14:13:38,451][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 14:13:38,455][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 2 (intersection at Union.scala:21) finished in 0.135 s
[INFO][2021-06-02 14:13:38,455][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 14:13:38,455][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 14:13:38,456][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 3)
[INFO][2021-06-02 14:13:38,456][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 14:13:38,456][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[8] at intersection at Union.scala:21), which has no missing parents
[INFO][2021-06-02 14:13:38,458][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 4.0 GB)
[INFO][2021-06-02 14:13:38,459][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 4.0 GB)
[INFO][2021-06-02 14:13:38,460][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.52.10:50649 (size: 2022.0 B, free: 4.0 GB)
[INFO][2021-06-02 14:13:38,460][org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 14:13:38,461][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 16 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at intersection at Union.scala:21) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO][2021-06-02 14:13:38,461][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 16 tasks
[INFO][2021-06-02 14:13:38,462][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 3.0 (TID 65, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 3.0 (TID 66, localhost, executor driver, partition 2, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 3.0 (TID 67, localhost, executor driver, partition 3, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 3.0 (TID 68, localhost, executor driver, partition 4, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 3.0 (TID 69, localhost, executor driver, partition 5, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 3.0 (TID 70, localhost, executor driver, partition 6, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,463][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 3.0 (TID 71, localhost, executor driver, partition 7, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,464][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 3.0 (TID 72, localhost, executor driver, partition 8, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,464][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 3.0 (TID 73, localhost, executor driver, partition 9, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,464][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 3.0 (TID 74, localhost, executor driver, partition 10, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,464][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 3.0 (TID 75, localhost, executor driver, partition 11, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,464][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 3.0 (TID 76, localhost, executor driver, partition 12, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 3.0 (TID 77, localhost, executor driver, partition 13, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 3.0 (TID 78, localhost, executor driver, partition 14, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 3.0 (TID 79, localhost, executor driver, partition 15, PROCESS_LOCAL, 4684 bytes)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 3.0 (TID 65)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 3.0 (TID 79)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 3.0 (TID 77)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 3.0 (TID 78)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 3.0 (TID 69)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 3.0 (TID 72)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 3.0 (TID 71)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 3.0 (TID 74)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 3.0 (TID 76)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 3.0 (TID 75)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 3.0 (TID 70)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 3.0 (TID 67)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 3.0 (TID 73)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 3.0 (TID 68)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 3.0 (TID 66)
[INFO][2021-06-02 14:13:38,465][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 64)
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,485][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,484][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 14:13:38,487][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,491][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:13:38,491][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:13:38,491][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:13:38,491][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,490][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:13:38,492][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:13:38,492][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:13:38,492][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:13:38,492][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 14:13:38,491][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:13:38,496][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,496][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,496][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,496][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 16 blocks
[INFO][2021-06-02 14:13:38,497][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:13:38,496][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:13:38,496][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 14:13:38,497][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 3.0 (TID 75). 992 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 3.0 (TID 74). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 3.0 (TID 79). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 3.0 (TID 72). 992 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 3.0 (TID 71). 992 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 3.0 (TID 76). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 3.0 (TID 78). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 3.0 (TID 77). 992 bytes result sent to driver
[INFO][2021-06-02 14:13:38,509][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 3.0 (TID 75) in 45 ms on localhost (executor driver) (1/16)
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 64). 992 bytes result sent to driver
[INFO][2021-06-02 14:13:38,508][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 3.0 (TID 73). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,510][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 3.0 (TID 74) in 46 ms on localhost (executor driver) (2/16)
[INFO][2021-06-02 14:13:38,510][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 3.0 (TID 79) in 45 ms on localhost (executor driver) (3/16)
[INFO][2021-06-02 14:13:38,511][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 3.0 (TID 72) in 47 ms on localhost (executor driver) (4/16)
[INFO][2021-06-02 14:13:38,511][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 3.0 (TID 71) in 48 ms on localhost (executor driver) (5/16)
[INFO][2021-06-02 14:13:38,512][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 3.0 (TID 78) in 47 ms on localhost (executor driver) (6/16)
[INFO][2021-06-02 14:13:38,513][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 3.0 (TID 76) in 49 ms on localhost (executor driver) (7/16)
[INFO][2021-06-02 14:13:38,514][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 3.0 (TID 65). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,514][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 3.0 (TID 69). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,514][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 3.0 (TID 77) in 50 ms on localhost (executor driver) (8/16)
[INFO][2021-06-02 14:13:38,514][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 3.0 (TID 68). 1039 bytes result sent to driver
[INFO][2021-06-02 14:13:38,515][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 64) in 53 ms on localhost (executor driver) (9/16)
[INFO][2021-06-02 14:13:38,514][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 3.0 (TID 67). 1039 bytes result sent to driver
[INFO][2021-06-02 14:13:38,515][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 3.0 (TID 66). 1035 bytes result sent to driver
[INFO][2021-06-02 14:13:38,515][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 3.0 (TID 70). 992 bytes result sent to driver
[INFO][2021-06-02 14:13:38,516][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 3.0 (TID 73) in 52 ms on localhost (executor driver) (10/16)
[INFO][2021-06-02 14:13:38,517][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 3.0 (TID 65) in 55 ms on localhost (executor driver) (11/16)
[INFO][2021-06-02 14:13:38,517][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 3.0 (TID 69) in 54 ms on localhost (executor driver) (12/16)
[INFO][2021-06-02 14:13:38,518][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 3.0 (TID 68) in 55 ms on localhost (executor driver) (13/16)
[INFO][2021-06-02 14:13:38,518][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 3.0 (TID 67) in 55 ms on localhost (executor driver) (14/16)
[INFO][2021-06-02 14:13:38,519][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 3.0 (TID 66) in 56 ms on localhost (executor driver) (15/16)
[INFO][2021-06-02 14:13:38,519][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 3.0 (TID 70) in 56 ms on localhost (executor driver) (16/16)
[INFO][2021-06-02 14:13:38,519][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 14:13:38,520][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (collect at Union.scala:22) finished in 0.059 s
[INFO][2021-06-02 14:13:38,520][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: collect at Union.scala:22, took 0.219142 s
[INFO][2021-06-02 14:13:38,524][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 14:13:38,528][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@63648ee9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 14:13:38,530][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 14:13:38,539][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 14:13:38,606][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 14:13:38,606][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 14:13:38,610][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 14:13:38,612][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 14:13:38,615][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 14:13:38,616][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 14:13:38,616][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-b8c423b2-7982-4a35-9dc8-7c06a4ff820b
[INFO][2021-06-02 15:43:00,789][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 15:43:01,220][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 15:43:01,234][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 15:43:01,235][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 15:43:01,235][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 15:43:01,236][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 15:43:01,236][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 15:43:02,660][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60225.
[INFO][2021-06-02 15:43:02,686][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 15:43:02,700][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 15:43:02,703][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 15:43:02,703][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 15:43:02,710][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-ebc8dfe9-2598-47e6-b6ca-5e6ba63660c0
[INFO][2021-06-02 15:43:02,722][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 15:43:02,757][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 15:43:02,820][org.spark_project.jetty.util.log:192] - Logging initialized @4413ms
[INFO][2021-06-02 15:43:02,872][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 15:43:02,888][org.spark_project.jetty.server.Server:403] - Started @4481ms
[INFO][2021-06-02 15:43:02,910][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@c9413d8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 15:43:02,911][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 15:43:02,935][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@552518c3{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,936][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44a2b17b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,936][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2a76b80a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,937][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61d9efe0{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,939][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e6516e{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,939][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43ed0ff3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,940][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a50b09c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,941][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e5c7f0b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,942][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4de025bf{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,942][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1eef9aef{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,943][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5db99216{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,944][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c1bd44c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,945][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18cc679e{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,946][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c4ca0f9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,947][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7df587ef{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,948][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2755d705{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,948][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@740abb5{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,949][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fe8b721{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,950][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@578524c3{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,950][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e094740{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,959][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc547a{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,959][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61a002b1{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,960][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@780ec4a5{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,961][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ac85b0c{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,962][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3aa3193a{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:02,966][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 15:43:03,021][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 15:43:03,046][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60266.
[INFO][2021-06-02 15:43:03,047][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:60266
[INFO][2021-06-02 15:43:03,048][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 15:43:03,049][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 60266, None)
[INFO][2021-06-02 15:43:03,052][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:60266 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 60266, None)
[INFO][2021-06-02 15:43:03,054][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 60266, None)
[INFO][2021-06-02 15:43:03,055][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 60266, None)
[INFO][2021-06-02 15:43:03,220][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@66d57c1b{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:03,255][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 15:43:03,316][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 15:43:03,316][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 15:43:03,321][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5488b5c5{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:03,321][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@712ca57b{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:03,322][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27abb83e{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:03,322][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1a1ed4e5{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:03,323][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7af1cd63{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:43:03,818][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 15:43:04,273][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 15:43:04,347][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 15:43:05,585][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 15:43:05,730][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/addcde6f-a7bd-47dc-be06-5be5064a2a53_resources
[INFO][2021-06-02 15:43:05,738][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/addcde6f-a7bd-47dc-be06-5be5064a2a53
[INFO][2021-06-02 15:43:05,741][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/addcde6f-a7bd-47dc-be06-5be5064a2a53
[INFO][2021-06-02 15:43:05,744][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/addcde6f-a7bd-47dc-be06-5be5064a2a53/_tmp_space.db
[INFO][2021-06-02 15:43:05,747][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 15:43:05,882][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/91a25475-915b-45d1-8202-fceba76ad652_resources
[INFO][2021-06-02 15:43:05,886][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/91a25475-915b-45d1-8202-fceba76ad652
[INFO][2021-06-02 15:43:05,889][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/91a25475-915b-45d1-8202-fceba76ad652
[INFO][2021-06-02 15:43:05,893][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/91a25475-915b-45d1-8202-fceba76ad652/_tmp_space.db
[INFO][2021-06-02 15:43:05,895][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 15:43:05,939][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 15:43:05,943][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: SELECT  * from adp_cfg.`info_this_jjjz_etf` WHERE rq = '20210419' AND `jys` ='SH' AND `jjdm` ='510010' LIMIT 1 
[INFO][2021-06-02 15:43:06,361][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:06,389][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:06,390][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:06,392][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:06,393][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:43:06,396][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:06,396][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:06,397][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:06,398][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:06,398][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:06,399][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:06,399][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:06,400][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:06,401][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:06,401][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:06,402][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:06,402][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:43:06,403][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:06,404][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:06,404][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:06,405][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:06,406][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:06,406][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:06,407][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:43:06,408][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:06,409][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:43:06,424][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 15:43:08,198][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf1
[INFO][2021-06-02 15:43:08,288][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: SELECT  * from adp_cfg.`info_this_jjjz_etf` WHERE rq = '20210419' AND `jys` ='SH' AND `jjdm` ='510010' LIMIT 1 
[INFO][2021-06-02 15:43:08,314][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,315][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,316][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,317][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,317][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:43:08,318][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,318][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,319][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,320][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,320][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,321][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,321][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,322][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,322][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,323][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,324][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,324][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:43:08,324][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,325][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,325][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,326][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,326][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,327][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,327][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:43:08,328][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,329][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:43:08,365][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf1
[INFO][2021-06-02 15:43:08,392][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: unionTemp
[INFO][2021-06-02 15:43:08,397][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from unionTemp 
[INFO][2021-06-02 15:43:08,491][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-02 15:43:08,493][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: isnotnull(jjdm#2),isnotnull(rq#0),isnotnull(jys#1),(rq#0 = 20210419),(jys#1 = SH),(jjdm#2 = 510010)
[INFO][2021-06-02 15:43:08,496][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<rq: int, jys: string, jjdm: string, jjlx: decimal(12,0), jjjz: decimal(10,4) ... 23 more fields>
[INFO][2021-06-02 15:43:08,504][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: IsNotNull(jjdm),IsNotNull(rq),IsNotNull(jys),EqualTo(rq,20210419),EqualTo(jys,SH),EqualTo(jjdm,510010)
[INFO][2021-06-02 15:43:08,536][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,537][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,537][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,537][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,538][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:43:08,538][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,538][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,538][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,538][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,538][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,539][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,539][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,539][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,539][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,539][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,539][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,540][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:43:08,540][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,540][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,540][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,540][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,541][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,541][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,541][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:43:08,541][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,541][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:43:08,724][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-02 15:43:08,725][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: isnotnull(jys#81),isnotnull(rq#80),isnotnull(jjdm#82),(rq#80 = 20210419),(jys#81 = SH),(jjdm#82 = 510010)
[INFO][2021-06-02 15:43:08,726][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<rq: int, jys: string, jjdm: string, jjlx: decimal(12,0), jjjz: decimal(10,4) ... 23 more fields>
[INFO][2021-06-02 15:43:08,726][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: IsNotNull(jys),IsNotNull(rq),IsNotNull(jjdm),EqualTo(rq,20210419),EqualTo(jys,SH),EqualTo(jjdm,510010)
[INFO][2021-06-02 15:43:08,817][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,817][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,818][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,818][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,819][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:43:08,819][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,820][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,820][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,821][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,822][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,822][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,823][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,823][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,824][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,825][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,825][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,826][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:43:08,827][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,827][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,828][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,828][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,829][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,829][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,830][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:43:08,830][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,831][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:43:08,885][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: EqualTo(none,20210419),IsNotNull(none),EqualTo(none,SH),EqualTo(none,510010),IsNotNull(none),IsNotNull(none)
[INFO][2021-06-02 15:43:08,919][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,920][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:08,920][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,921][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:08,921][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:43:08,922][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,923][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,923][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,924][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,924][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,925][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,926][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,926][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,927][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,927][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,927][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,927][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:43:08,928][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,929][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,929][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,930][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:08,930][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:08,931][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:08,931][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:43:08,932][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:08,933][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:43:08,959][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: EqualTo(none,20210419),IsNotNull(none),EqualTo(none,SH),EqualTo(none,510010),IsNotNull(none),IsNotNull(none)
[INFO][2021-06-02 15:43:09,001][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:09,001][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:43:09,002][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:09,002][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:43:09,003][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:43:09,004][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:09,004][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:09,005][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:09,005][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:09,006][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:09,006][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:09,007][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:09,007][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:09,008][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:09,008][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:09,008][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:09,009][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:43:09,009][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:09,009][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:09,010][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:09,010][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:43:09,010][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:43:09,011][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:43:09,011][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:43:09,011][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:43:09,011][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:43:09,380][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 215.5093 ms
[INFO][2021-06-02 15:43:09,585][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 32.7409 ms
[INFO][2021-06-02 15:43:09,651][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 11
[INFO][2021-06-02 15:43:09,653][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 8
[INFO][2021-06-02 15:43:09,658][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 16
[INFO][2021-06-02 15:43:09,660][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 9
[INFO][2021-06-02 15:43:09,677][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 64.7736 ms
[INFO][2021-06-02 15:43:09,762][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 242.5 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:10,037][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.2 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:10,043][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:60266 (size: 24.2 KB, free: 3.8 GB)
[INFO][2021-06-02 15:43:10,050][org.apache.spark.SparkContext:54] - Created broadcast 0 from show at UnionALLRegisterTempView.java:23
[INFO][2021-06-02 15:43:10,063][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-02 15:43:10,272][org.apache.spark.SparkContext:54] - Starting job: show at UnionALLRegisterTempView.java:23
[INFO][2021-06-02 15:43:10,285][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 2 (show at UnionALLRegisterTempView.java:23)
[INFO][2021-06-02 15:43:10,287][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at UnionALLRegisterTempView.java:23) with 1 output partitions
[INFO][2021-06-02 15:43:10,287][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at UnionALLRegisterTempView.java:23)
[INFO][2021-06-02 15:43:10,287][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
[INFO][2021-06-02 15:43:10,289][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
[INFO][2021-06-02 15:43:10,292][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at UnionALLRegisterTempView.java:23), which has no missing parents
[INFO][2021-06-02 15:43:10,392][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 29.2 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:10,409][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.0 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:10,410][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:60266 (size: 10.0 KB, free: 3.8 GB)
[INFO][2021-06-02 15:43:10,411][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:43:10,423][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at UnionALLRegisterTempView.java:23) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[INFO][2021-06-02 15:43:10,424][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 4 tasks
[INFO][2021-06-02 15:43:10,477][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5327 bytes)
[INFO][2021-06-02 15:43:10,480][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5327 bytes)
[INFO][2021-06-02 15:43:10,481][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5327 bytes)
[INFO][2021-06-02 15:43:10,482][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5327 bytes)
[INFO][2021-06-02 15:43:10,492][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
[INFO][2021-06-02 15:43:10,492][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-02 15:43:10,492][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 0.0 (TID 3)
[INFO][2021-06-02 15:43:10,492][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-06-02 15:43:10,571][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210421/000000_0, range: 0-61127, partition values: [20210421]
[INFO][2021-06-02 15:43:10,571][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210422/000000_0, range: 0-61127, partition values: [20210422]
[INFO][2021-06-02 15:43:10,571][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210420/000000_0, range: 0-66403, partition values: [20210420]
[INFO][2021-06-02 15:43:10,571][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210419/000000_0, range: 0-71668, partition values: [20210419]
[INFO][2021-06-02 15:43:12,005][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(jjdm, null), noteq(rq, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:43:12,006][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(jjdm, null), noteq(rq, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:43:12,005][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(jjdm, null), noteq(rq, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:43:12,006][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(jjdm, null), noteq(rq, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:43:12,105][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 0.0 (TID 3). 1343 bytes result sent to driver
[INFO][2021-06-02 15:43:12,105][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1300 bytes result sent to driver
[INFO][2021-06-02 15:43:12,105][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 1300 bytes result sent to driver
[INFO][2021-06-02 15:43:12,143][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 1660 ms on localhost (executor driver) (1/4)
[INFO][2021-06-02 15:43:12,145][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 1664 ms on localhost (executor driver) (2/4)
[INFO][2021-06-02 15:43:12,146][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1666 ms on localhost (executor driver) (3/4)
[INFO][2021-06-02 15:43:12,260][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1515 bytes result sent to driver
[INFO][2021-06-02 15:43:12,263][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1805 ms on localhost (executor driver) (4/4)
[INFO][2021-06-02 15:43:12,265][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:43:12,266][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at UnionALLRegisterTempView.java:23) finished in 1.824 s
[INFO][2021-06-02 15:43:12,266][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 15:43:12,267][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 15:43:12,267][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
[INFO][2021-06-02 15:43:12,268][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 15:43:12,271][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[7] at show at UnionALLRegisterTempView.java:23), which has no missing parents
[INFO][2021-06-02 15:43:12,280][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 23.6 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:12,285][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:12,286][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.52.10:60266 (size: 5.1 KB, free: 3.8 GB)
[INFO][2021-06-02 15:43:12,287][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:43:12,288][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at UnionALLRegisterTempView.java:23) (first 15 tasks are for partitions Vector(0))
[INFO][2021-06-02 15:43:12,288][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
[INFO][2021-06-02 15:43:12,292][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4835 bytes)
[INFO][2021-06-02 15:43:12,292][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 4)
[INFO][2021-06-02 15:43:12,304][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 4 blocks
[INFO][2021-06-02 15:43:12,305][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:43:12,323][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 4). 1551 bytes result sent to driver
[INFO][2021-06-02 15:43:12,323][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 4) in 34 ms on localhost (executor driver) (1/1)
[INFO][2021-06-02 15:43:12,324][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:43:12,324][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at UnionALLRegisterTempView.java:23) finished in 0.035 s
[INFO][2021-06-02 15:43:12,328][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at UnionALLRegisterTempView.java:23, took 2.055310 s
[INFO][2021-06-02 15:43:12,333][org.apache.spark.SparkContext:54] - Starting job: show at UnionALLRegisterTempView.java:23
[INFO][2021-06-02 15:43:12,338][org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 160 bytes
[INFO][2021-06-02 15:43:12,340][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at UnionALLRegisterTempView.java:23) with 1 output partitions
[INFO][2021-06-02 15:43:12,340][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at UnionALLRegisterTempView.java:23)
[INFO][2021-06-02 15:43:12,340][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 2)
[INFO][2021-06-02 15:43:12,340][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 15:43:12,340][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[7] at show at UnionALLRegisterTempView.java:23), which has no missing parents
[INFO][2021-06-02 15:43:12,342][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 23.6 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:12,346][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.1 KB, free 3.8 GB)
[INFO][2021-06-02 15:43:12,347][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.52.10:60266 (size: 5.1 KB, free: 3.8 GB)
[INFO][2021-06-02 15:43:12,348][org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:43:12,349][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at show at UnionALLRegisterTempView.java:23) (first 15 tasks are for partitions Vector(1))
[INFO][2021-06-02 15:43:12,349][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
[INFO][2021-06-02 15:43:12,349][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 4835 bytes)
[INFO][2021-06-02 15:43:12,350][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 5)
[INFO][2021-06-02 15:43:12,351][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 4 blocks
[INFO][2021-06-02 15:43:12,352][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:43:12,355][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 5). 1422 bytes result sent to driver
[INFO][2021-06-02 15:43:12,355][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
[INFO][2021-06-02 15:43:12,355][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:43:12,356][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at UnionALLRegisterTempView.java:23) finished in 0.007 s
[INFO][2021-06-02 15:43:12,356][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at UnionALLRegisterTempView.java:23, took 0.022319 s
[INFO][2021-06-02 15:43:12,368][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 15:43:12,373][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@c9413d8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 15:43:12,375][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 15:43:12,382][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 15:43:12,397][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 15:43:12,398][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 15:43:12,401][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 15:43:12,404][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 15:43:12,407][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 15:43:12,408][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 15:43:12,409][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-17c3949e-4179-483b-8b88-dc2a7729097b
[INFO][2021-06-02 15:46:18,836][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 15:46:19,347][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 15:46:19,378][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 15:46:19,379][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 15:46:19,379][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 15:46:19,380][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 15:46:19,381][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 15:46:20,832][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 49767.
[INFO][2021-06-02 15:46:20,871][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 15:46:20,902][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 15:46:20,907][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 15:46:20,908][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 15:46:20,921][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-9a461978-b8b5-4954-9018-0a17c8030078
[INFO][2021-06-02 15:46:20,932][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 15:46:20,971][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 15:46:21,029][org.spark_project.jetty.util.log:192] - Logging initialized @4858ms
[INFO][2021-06-02 15:46:21,099][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 15:46:21,109][org.spark_project.jetty.server.Server:403] - Started @4938ms
[INFO][2021-06-02 15:46:21,130][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@c9413d8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 15:46:21,130][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 15:46:21,148][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@552518c3{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,149][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44a2b17b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,150][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2a76b80a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,151][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61d9efe0{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,152][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e6516e{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,152][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43ed0ff3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,153][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a50b09c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,154][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e5c7f0b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,155][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4de025bf{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,156][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1eef9aef{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,156][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5db99216{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,157][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c1bd44c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,157][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18cc679e{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,158][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c4ca0f9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,159][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7df587ef{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,160][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2755d705{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,160][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@740abb5{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,161][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fe8b721{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,163][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@578524c3{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,163][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e094740{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,170][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc547a{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,171][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61a002b1{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,172][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@780ec4a5{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,172][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ac85b0c{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,173][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3aa3193a{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,175][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 15:46:21,234][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 15:46:21,259][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49811.
[INFO][2021-06-02 15:46:21,260][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:49811
[INFO][2021-06-02 15:46:21,261][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 15:46:21,262][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 49811, None)
[INFO][2021-06-02 15:46:21,266][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:49811 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 49811, None)
[INFO][2021-06-02 15:46:21,269][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 49811, None)
[INFO][2021-06-02 15:46:21,269][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 49811, None)
[INFO][2021-06-02 15:46:21,415][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@66d57c1b{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,445][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 15:46:21,465][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 15:46:21,465][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 15:46:21,473][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@712ca57b{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,473][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@54534abf{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,474][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1a1ed4e5{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,475][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c6e0a08{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,477][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3c2772d1{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 15:46:21,963][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 15:46:22,441][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 15:46:22,484][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 15:46:23,507][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 15:46:23,594][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/6df8c2b0-485d-4b8c-9c61-e6e0009fcc04_resources
[INFO][2021-06-02 15:46:23,601][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/6df8c2b0-485d-4b8c-9c61-e6e0009fcc04
[INFO][2021-06-02 15:46:23,604][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/6df8c2b0-485d-4b8c-9c61-e6e0009fcc04
[INFO][2021-06-02 15:46:23,607][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/6df8c2b0-485d-4b8c-9c61-e6e0009fcc04/_tmp_space.db
[INFO][2021-06-02 15:46:23,610][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 15:46:23,744][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/aa6a2117-fcb4-4f57-bee3-4900bd52c5b2_resources
[INFO][2021-06-02 15:46:23,748][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/aa6a2117-fcb4-4f57-bee3-4900bd52c5b2
[INFO][2021-06-02 15:46:23,752][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/aa6a2117-fcb4-4f57-bee3-4900bd52c5b2
[INFO][2021-06-02 15:46:23,755][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/aa6a2117-fcb4-4f57-bee3-4900bd52c5b2/_tmp_space.db
[INFO][2021-06-02 15:46:23,757][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 15:46:23,796][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 15:46:23,800][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: SELECT  * from adp_cfg.`info_this_jjjz_etf` WHERE rq = '20210419' AND `jys` ='SH' AND `jjdm` ='510010' LIMIT 1 
[INFO][2021-06-02 15:46:24,160][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:24,185][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:24,185][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:24,186][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:24,186][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:46:24,187][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:24,188][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:24,188][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:24,188][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:24,189][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:24,189][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:24,190][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:24,190][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:24,190][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:24,191][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:24,191][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:24,191][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:46:24,192][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:24,192][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:24,192][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:24,192][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:24,193][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:24,193][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:24,194][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:46:24,195][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:24,195][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:46:24,208][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 15:46:25,926][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf1
[INFO][2021-06-02 15:46:26,061][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: SELECT  * from adp_cfg.`info_this_jjjz_etf` WHERE rq = '20210419' AND `jys` ='SH' AND `jjdm` ='510010' LIMIT 1 
[INFO][2021-06-02 15:46:26,155][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:26,156][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:26,157][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:26,158][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:26,159][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:46:26,160][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,161][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,162][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,163][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,164][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,165][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,166][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,167][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,168][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,169][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,170][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:26,172][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:46:26,172][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,173][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,174][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,175][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,175][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,176][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,177][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:46:26,178][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:26,178][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:46:26,245][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf1
[INFO][2021-06-02 15:46:26,293][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: unionTemp
[INFO][2021-06-02 15:46:26,300][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from unionTemp 
[INFO][2021-06-02 15:46:26,438][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-02 15:46:26,441][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: isnotnull(rq#0),isnotnull(jjdm#2),isnotnull(jys#1),(rq#0 = 20210419),(jys#1 = SH),(jjdm#2 = 510010)
[INFO][2021-06-02 15:46:26,444][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<rq: int, jys: string, jjdm: string, jjlx: decimal(12,0), jjjz: decimal(10,4) ... 23 more fields>
[INFO][2021-06-02 15:46:26,452][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: IsNotNull(rq),IsNotNull(jjdm),IsNotNull(jys),EqualTo(rq,20210419),EqualTo(jys,SH),EqualTo(jjdm,510010)
[INFO][2021-06-02 15:46:26,601][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:26,602][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:26,602][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:26,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:26,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:46:26,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,605][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,605][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,605][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,605][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,606][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:26,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:46:26,608][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,608][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,609][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,609][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,609][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,610][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,610][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:46:26,610][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:26,611][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:46:26,773][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-02 15:46:26,773][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: isnotnull(jjdm#82),isnotnull(jys#81),isnotnull(rq#80),(rq#80 = 20210419),(jys#81 = SH),(jjdm#82 = 510010)
[INFO][2021-06-02 15:46:26,774][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<rq: int, jys: string, jjdm: string, jjlx: decimal(12,0), jjjz: decimal(10,4) ... 23 more fields>
[INFO][2021-06-02 15:46:26,774][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: IsNotNull(jjdm),IsNotNull(jys),IsNotNull(rq),EqualTo(rq,20210419),EqualTo(jys,SH),EqualTo(jjdm,510010)
[INFO][2021-06-02 15:46:26,873][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:26,874][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:26,875][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:26,876][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:26,876][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:46:26,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,879][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,879][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,880][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,881][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,881][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,882][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,883][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,884][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:26,885][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:46:26,886][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,887][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,887][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,888][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:26,890][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:26,892][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:26,893][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:46:26,894][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:26,895][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:46:26,988][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: EqualTo(none,20210419),EqualTo(none,510010),EqualTo(none,SH),IsNotNull(none),IsNotNull(none),IsNotNull(none)
[INFO][2021-06-02 15:46:27,060][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:27,061][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:27,062][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:27,063][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:27,064][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:46:27,064][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,065][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,067][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,068][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,068][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,069][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,070][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,071][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,072][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,072][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,073][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:27,074][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:46:27,075][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,076][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,078][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,079][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,080][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,080][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,081][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:46:27,082][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:27,083][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:46:27,125][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: EqualTo(none,20210419),EqualTo(none,510010),EqualTo(none,SH),IsNotNull(none),IsNotNull(none),IsNotNull(none)
[INFO][2021-06-02 15:46:27,190][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:27,191][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 15:46:27,192][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:27,193][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 15:46:27,194][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 15:46:27,195][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,196][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,197][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,198][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,199][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,199][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,200][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,201][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,202][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,203][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,204][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:27,205][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 15:46:27,205][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,206][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,207][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,208][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 15:46:27,209][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 15:46:27,210][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 15:46:27,211][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 15:46:27,212][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 15:46:27,213][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 15:46:27,763][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 292.5676 ms
[INFO][2021-06-02 15:46:27,978][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 12
[INFO][2021-06-02 15:46:27,978][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 10
[INFO][2021-06-02 15:46:27,979][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 8
[INFO][2021-06-02 15:46:27,979][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 9
[INFO][2021-06-02 15:46:27,979][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 18
[INFO][2021-06-02 15:46:27,979][org.apache.spark.ContextCleaner:54] - Cleaned accumulator 13
[INFO][2021-06-02 15:46:28,132][org.apache.spark.sql.execution.aggregate.HashAggregateExec:54] - spark.sql.codegen.aggregate.map.twolevel.enable is set to true, but current version of codegened fast hashmap does not support this aggregate.
[INFO][2021-06-02 15:46:28,300][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 91.0066 ms
[INFO][2021-06-02 15:46:28,302][org.apache.spark.sql.execution.aggregate.HashAggregateExec:54] - spark.sql.codegen.aggregate.map.twolevel.enable is set to true, but current version of codegened fast hashmap does not support this aggregate.
[INFO][2021-06-02 15:46:28,350][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 33.6474 ms
[INFO][2021-06-02 15:46:28,368][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.7369 ms
[INFO][2021-06-02 15:46:28,405][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 20.5505 ms
[INFO][2021-06-02 15:46:28,498][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 242.5 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:28,810][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.2 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:28,816][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:49811 (size: 24.2 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:28,825][org.apache.spark.SparkContext:54] - Created broadcast 0 from show at UnionALLRegisterTempView.java:29
[INFO][2021-06-02 15:46:28,837][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-02 15:46:29,142][org.apache.spark.SparkContext:54] - Starting job: show at UnionALLRegisterTempView.java:29
[INFO][2021-06-02 15:46:29,158][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 2 (show at UnionALLRegisterTempView.java:29)
[INFO][2021-06-02 15:46:29,159][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 8 (show at UnionALLRegisterTempView.java:29)
[INFO][2021-06-02 15:46:29,161][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at UnionALLRegisterTempView.java:29) with 1 output partitions
[INFO][2021-06-02 15:46:29,162][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at UnionALLRegisterTempView.java:29)
[INFO][2021-06-02 15:46:29,162][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
[INFO][2021-06-02 15:46:29,164][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
[INFO][2021-06-02 15:46:29,173][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at UnionALLRegisterTempView.java:29), which has no missing parents
[INFO][2021-06-02 15:46:29,220][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 29.2 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:29,239][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.0 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:29,240][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:49811 (size: 10.0 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:29,240][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:46:29,250][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at UnionALLRegisterTempView.java:29) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[INFO][2021-06-02 15:46:29,251][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 4 tasks
[INFO][2021-06-02 15:46:29,282][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5327 bytes)
[INFO][2021-06-02 15:46:29,284][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5327 bytes)
[INFO][2021-06-02 15:46:29,285][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5327 bytes)
[INFO][2021-06-02 15:46:29,286][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5327 bytes)
[INFO][2021-06-02 15:46:29,291][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-02 15:46:29,291][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-06-02 15:46:29,291][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
[INFO][2021-06-02 15:46:29,291][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 0.0 (TID 3)
[INFO][2021-06-02 15:46:29,358][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210419/000000_0, range: 0-71668, partition values: [20210419]
[INFO][2021-06-02 15:46:29,358][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210422/000000_0, range: 0-61127, partition values: [20210422]
[INFO][2021-06-02 15:46:29,358][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210420/000000_0, range: 0-66403, partition values: [20210420]
[INFO][2021-06-02 15:46:29,358][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210421/000000_0, range: 0-61127, partition values: [20210421]
[INFO][2021-06-02 15:46:30,786][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(rq, null), noteq(jjdm, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:46:30,786][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(rq, null), noteq(jjdm, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:46:30,786][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(rq, null), noteq(jjdm, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:46:30,787][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(and(and(and(and(noteq(rq, null), noteq(jjdm, null)), noteq(jys, null)), eq(rq, 20210419)), eq(jys, Binary{"SH"})), eq(jjdm, Binary{"510010"}))
[INFO][2021-06-02 15:46:30,880][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 1343 bytes result sent to driver
[INFO][2021-06-02 15:46:30,880][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 0.0 (TID 3). 1343 bytes result sent to driver
[INFO][2021-06-02 15:46:30,880][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1386 bytes result sent to driver
[INFO][2021-06-02 15:46:30,907][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 1621 ms on localhost (executor driver) (1/4)
[INFO][2021-06-02 15:46:30,908][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1624 ms on localhost (executor driver) (2/4)
[INFO][2021-06-02 15:46:30,908][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 1623 ms on localhost (executor driver) (3/4)
[INFO][2021-06-02 15:46:30,972][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1515 bytes result sent to driver
[INFO][2021-06-02 15:46:30,976][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1702 ms on localhost (executor driver) (4/4)
[INFO][2021-06-02 15:46:30,977][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:46:30,978][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at UnionALLRegisterTempView.java:29) finished in 1.711 s
[INFO][2021-06-02 15:46:30,978][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 15:46:30,978][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 15:46:30,979][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ShuffleMapStage 1, ResultStage 2)
[INFO][2021-06-02 15:46:30,979][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 15:46:30,981][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at show at UnionALLRegisterTempView.java:29), which has no missing parents
[INFO][2021-06-02 15:46:30,986][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 67.8 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:30,989][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.2 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:30,990][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.52.10:49811 (size: 19.2 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:30,991][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:46:30,992][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at show at UnionALLRegisterTempView.java:29) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-06-02 15:46:30,992][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
[INFO][2021-06-02 15:46:30,994][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4824 bytes)
[INFO][2021-06-02 15:46:30,995][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 5, localhost, executor driver, partition 1, ANY, 4824 bytes)
[INFO][2021-06-02 15:46:30,995][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 4)
[INFO][2021-06-02 15:46:30,995][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 5)
[INFO][2021-06-02 15:46:31,011][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 4 blocks
[INFO][2021-06-02 15:46:31,011][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 4 blocks
[INFO][2021-06-02 15:46:31,013][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,013][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,087][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.6262 ms
[INFO][2021-06-02 15:46:31,095][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 5.0415 ms
[INFO][2021-06-02 15:46:31,130][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 32.1495 ms
[INFO][2021-06-02 15:46:31,141][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.52.10:49811 in memory (size: 10.0 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,255][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 4). 3064 bytes result sent to driver
[INFO][2021-06-02 15:46:31,257][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 4) in 265 ms on localhost (executor driver) (1/2)
[INFO][2021-06-02 15:46:31,258][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 5). 3107 bytes result sent to driver
[INFO][2021-06-02 15:46:31,259][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 5) in 265 ms on localhost (executor driver) (2/2)
[INFO][2021-06-02 15:46:31,260][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:46:31,260][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (show at UnionALLRegisterTempView.java:29) finished in 0.268 s
[INFO][2021-06-02 15:46:31,260][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 15:46:31,260][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 15:46:31,260][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
[INFO][2021-06-02 15:46:31,261][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 15:46:31,261][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29), which has no missing parents
[INFO][2021-06-02 15:46:31,265][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 51.0 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,269][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.7 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,270][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.52.10:49811 (size: 18.7 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,270][org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:46:31,272][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29) (first 15 tasks are for partitions Vector(0))
[INFO][2021-06-02 15:46:31,272][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
[INFO][2021-06-02 15:46:31,274][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,275][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 6)
[INFO][2021-06-02 15:46:31,278][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,278][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,287][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 6). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,287][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 6) in 14 ms on localhost (executor driver) (1/1)
[INFO][2021-06-02 15:46:31,288][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:46:31,288][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at UnionALLRegisterTempView.java:29) finished in 0.016 s
[INFO][2021-06-02 15:46:31,296][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at UnionALLRegisterTempView.java:29, took 2.154573 s
[INFO][2021-06-02 15:46:31,302][org.apache.spark.SparkContext:54] - Starting job: show at UnionALLRegisterTempView.java:29
[INFO][2021-06-02 15:46:31,308][org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 160 bytes
[INFO][2021-06-02 15:46:31,311][org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 1 is 165 bytes
[INFO][2021-06-02 15:46:31,311][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at UnionALLRegisterTempView.java:29) with 4 output partitions
[INFO][2021-06-02 15:46:31,311][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 5 (show at UnionALLRegisterTempView.java:29)
[INFO][2021-06-02 15:46:31,311][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 4)
[INFO][2021-06-02 15:46:31,312][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 15:46:31,312][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 5 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29), which has no missing parents
[INFO][2021-06-02 15:46:31,314][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 51.0 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,319][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.7 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,320][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.52.10:49811 (size: 18.7 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,320][org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:46:31,321][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[INFO][2021-06-02 15:46:31,321][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 5.0 with 4 tasks
[INFO][2021-06-02 15:46:31,321][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 5.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,322][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 5.0 (TID 8, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,323][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 5.0 (TID 9, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,323][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 5.0 (TID 10, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,324][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 5.0 (TID 7)
[INFO][2021-06-02 15:46:31,324][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 5.0 (TID 9)
[INFO][2021-06-02 15:46:31,324][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 5.0 (TID 10)
[INFO][2021-06-02 15:46:31,324][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 5.0 (TID 8)
[INFO][2021-06-02 15:46:31,329][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,330][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,334][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 5.0 (TID 8). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,338][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,339][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,339][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,340][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 5.0 (TID 8) in 18 ms on localhost (executor driver) (1/4)
[INFO][2021-06-02 15:46:31,343][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,343][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,349][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 5.0 (TID 10). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,350][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 5.0 (TID 10) in 27 ms on localhost (executor driver) (2/4)
[INFO][2021-06-02 15:46:31,352][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 5.0 (TID 7). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,352][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 5.0 (TID 9). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,354][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 5.0 (TID 7) in 32 ms on localhost (executor driver) (3/4)
[INFO][2021-06-02 15:46:31,355][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 5.0 (TID 9) in 33 ms on localhost (executor driver) (4/4)
[INFO][2021-06-02 15:46:31,355][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:46:31,357][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 5 (show at UnionALLRegisterTempView.java:29) finished in 0.036 s
[INFO][2021-06-02 15:46:31,358][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at UnionALLRegisterTempView.java:29, took 0.055525 s
[INFO][2021-06-02 15:46:31,363][org.apache.spark.SparkContext:54] - Starting job: show at UnionALLRegisterTempView.java:29
[INFO][2021-06-02 15:46:31,366][org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (show at UnionALLRegisterTempView.java:29) with 20 output partitions
[INFO][2021-06-02 15:46:31,366][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 8 (show at UnionALLRegisterTempView.java:29)
[INFO][2021-06-02 15:46:31,366][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 7)
[INFO][2021-06-02 15:46:31,366][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 15:46:31,367][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 8 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29), which has no missing parents
[INFO][2021-06-02 15:46:31,372][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 51.0 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,375][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 18.7 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,377][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 192.168.52.10:49811 (size: 18.7 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,377][org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:46:31,378][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 20 missing tasks from ResultStage 8 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))
[INFO][2021-06-02 15:46:31,378][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 8.0 with 20 tasks
[INFO][2021-06-02 15:46:31,379][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 8.0 (TID 11, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,379][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 8.0 (TID 12, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,380][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 8.0 (TID 13, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,380][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 8.0 (TID 14, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,380][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 8.0 (TID 15, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,380][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 8.0 (TID 16, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 8.0 (TID 17, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 8.0 (TID 18, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 8.0 (TID 19, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 8.0 (TID 20, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,381][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 8.0 (TID 21, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,382][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 8.0 (TID 22, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,382][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 8.0 (TID 23, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,382][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 8.0 (TID 24, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,383][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 8.0 (TID 25, localhost, executor driver, partition 19, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,383][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 8.0 (TID 26, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,383][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 8.0 (TID 11)
[INFO][2021-06-02 15:46:31,383][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 8.0 (TID 12)
[INFO][2021-06-02 15:46:31,384][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 8.0 (TID 13)
[INFO][2021-06-02 15:46:31,384][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 8.0 (TID 14)
[INFO][2021-06-02 15:46:31,386][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 8.0 (TID 16)
[INFO][2021-06-02 15:46:31,387][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 8.0 (TID 18)
[INFO][2021-06-02 15:46:31,387][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 8.0 (TID 19)
[INFO][2021-06-02 15:46:31,387][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 8.0 (TID 17)
[INFO][2021-06-02 15:46:31,389][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 8.0 (TID 15)
[INFO][2021-06-02 15:46:31,393][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,393][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,394][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,395][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,395][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,396][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,396][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 8.0 (TID 20)
[INFO][2021-06-02 15:46:31,397][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 8.0 (TID 24)
[INFO][2021-06-02 15:46:31,398][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 8.0 (TID 23)
[INFO][2021-06-02 15:46:31,396][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 8.0 (TID 26)
[INFO][2021-06-02 15:46:31,402][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,402][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,397][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 8.0 (TID 21)
[INFO][2021-06-02 15:46:31,396][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 8.0 (TID 25)
[INFO][2021-06-02 15:46:31,408][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 8.0 (TID 17). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,409][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,409][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 8.0 (TID 27, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,410][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,411][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,411][org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 8.0 (TID 27)
[INFO][2021-06-02 15:46:31,411][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 8.0 (TID 17) in 30 ms on localhost (executor driver) (1/20)
[INFO][2021-06-02 15:46:31,397][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 8.0 (TID 22)
[INFO][2021-06-02 15:46:31,415][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,415][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,416][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,416][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,416][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 8.0 (TID 13). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,417][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 8.0 (TID 28, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,411][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,418][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 10 ms
[INFO][2021-06-02 15:46:31,420][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 8.0 (TID 20). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,423][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 8.0 (TID 22). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,424][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 8.0 (TID 16). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,425][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 8.0 (TID 26). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,425][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 8.0 (TID 29, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,409][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,426][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 8.0 (TID 30, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,427][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,427][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,427][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 8.0 (TID 13) in 47 ms on localhost (executor driver) (2/20)
[INFO][2021-06-02 15:46:31,428][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 8.0 (TID 20) in 47 ms on localhost (executor driver) (3/20)
[INFO][2021-06-02 15:46:31,428][org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 8.0 (TID 28)
[INFO][2021-06-02 15:46:31,429][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 8.0 (TID 16) in 49 ms on localhost (executor driver) (4/20)
[INFO][2021-06-02 15:46:31,402][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 8.0 (TID 11). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,429][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 8.0 (TID 22) in 48 ms on localhost (executor driver) (5/20)
[INFO][2021-06-02 15:46:31,430][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,430][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,399][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,431][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 33 ms
[INFO][2021-06-02 15:46:31,432][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,432][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,430][org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 8.0 (TID 30)
[INFO][2021-06-02 15:46:31,433][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,434][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,435][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 8.0 (TID 25). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,435][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,436][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,430][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,437][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 15:46:31,430][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 8.0 (TID 26) in 47 ms on localhost (executor driver) (6/20)
[INFO][2021-06-02 15:46:31,438][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,438][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,429][org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 8.0 (TID 29)
[INFO][2021-06-02 15:46:31,440][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 8.0 (TID 18). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,427][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,442][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 15 ms
[INFO][2021-06-02 15:46:31,417][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 8.0 (TID 14). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,444][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 8.0 (TID 11) in 65 ms on localhost (executor driver) (7/20)
[INFO][2021-06-02 15:46:31,433][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,447][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 14 ms
[INFO][2021-06-02 15:46:31,447][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 8.0 (TID 12). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,448][org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 8.0 (TID 30). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,432][org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 8.0 (TID 27). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,450][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 8.0 (TID 18) in 69 ms on localhost (executor driver) (8/20)
[INFO][2021-06-02 15:46:31,451][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 8.0 (TID 12) in 72 ms on localhost (executor driver) (9/20)
[INFO][2021-06-02 15:46:31,451][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 8.0 (TID 30) in 25 ms on localhost (executor driver) (10/20)
[INFO][2021-06-02 15:46:31,451][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 8.0 (TID 21). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,452][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 8.0 (TID 15). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,452][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 8.0 (TID 24). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,454][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 8.0 (TID 21) in 73 ms on localhost (executor driver) (11/20)
[INFO][2021-06-02 15:46:31,454][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,455][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,455][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 8.0 (TID 15) in 75 ms on localhost (executor driver) (12/20)
[INFO][2021-06-02 15:46:31,454][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 8.0 (TID 19). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,455][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 8.0 (TID 14) in 75 ms on localhost (executor driver) (13/20)
[INFO][2021-06-02 15:46:31,456][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 8.0 (TID 25) in 74 ms on localhost (executor driver) (14/20)
[INFO][2021-06-02 15:46:31,457][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 8.0 (TID 19) in 76 ms on localhost (executor driver) (15/20)
[INFO][2021-06-02 15:46:31,457][org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 8.0 (TID 28). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,459][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 8.0 (TID 23). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,461][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 8.0 (TID 27) in 52 ms on localhost (executor driver) (16/20)
[INFO][2021-06-02 15:46:31,462][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 8.0 (TID 24) in 80 ms on localhost (executor driver) (17/20)
[INFO][2021-06-02 15:46:31,462][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 8.0 (TID 28) in 46 ms on localhost (executor driver) (18/20)
[INFO][2021-06-02 15:46:31,462][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 8.0 (TID 23) in 80 ms on localhost (executor driver) (19/20)
[INFO][2021-06-02 15:46:31,465][org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 8.0 (TID 29). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,466][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 8.0 (TID 29) in 41 ms on localhost (executor driver) (20/20)
[INFO][2021-06-02 15:46:31,467][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:46:31,467][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 8 (show at UnionALLRegisterTempView.java:29) finished in 0.088 s
[INFO][2021-06-02 15:46:31,468][org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: show at UnionALLRegisterTempView.java:29, took 0.104274 s
[INFO][2021-06-02 15:46:31,473][org.apache.spark.SparkContext:54] - Starting job: show at UnionALLRegisterTempView.java:29
[INFO][2021-06-02 15:46:31,476][org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (show at UnionALLRegisterTempView.java:29) with 100 output partitions
[INFO][2021-06-02 15:46:31,476][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 11 (show at UnionALLRegisterTempView.java:29)
[INFO][2021-06-02 15:46:31,476][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 10)
[INFO][2021-06-02 15:46:31,476][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 15:46:31,477][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 11 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29), which has no missing parents
[INFO][2021-06-02 15:46:31,484][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6 stored as values in memory (estimated size 51.0 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,487][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 18.7 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,488][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_6_piece0 in memory on 192.168.52.10:49811 (size: 18.7 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,489][org.apache.spark.SparkContext:54] - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:46:31,489][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 100 missing tasks from ResultStage 11 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29) (first 15 tasks are for partitions Vector(25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39))
[INFO][2021-06-02 15:46:31,490][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 11.0 with 100 tasks
[INFO][2021-06-02 15:46:31,491][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 11.0 (TID 31, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,491][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 11.0 (TID 32, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,491][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 11.0 (TID 33, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,492][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 11.0 (TID 34, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,492][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 11.0 (TID 35, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,492][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 11.0 (TID 36, localhost, executor driver, partition 30, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,492][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 11.0 (TID 37, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,493][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 11.0 (TID 38, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,493][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 11.0 (TID 39, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,493][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 11.0 (TID 40, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,494][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 11.0 (TID 41, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,494][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 11.0 (TID 42, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,494][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 11.0 (TID 43, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,494][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 11.0 (TID 44, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 11.0 (TID 45, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 11.0 (TID 46, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 11.0 (TID 33)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 11.0 (TID 46)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 11.0 (TID 45)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 11.0 (TID 44)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 11.0 (TID 40)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 11.0 (TID 43)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 11.0 (TID 41)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 11.0 (TID 42)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 11.0 (TID 39)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 11.0 (TID 38)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 11.0 (TID 37)
[INFO][2021-06-02 15:46:31,501][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,501][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,501][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,502][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,502][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,502][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,504][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,504][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,504][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,504][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,506][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 11.0 (TID 41). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,507][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 11.0 (TID 39). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,507][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 11.0 (TID 47, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,509][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,509][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,509][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 11.0 (TID 43). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,510][org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 11.0 (TID 47)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 11.0 (TID 36)
[INFO][2021-06-02 15:46:31,513][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,513][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 11.0 (TID 35)
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 11.0 (TID 32)
[INFO][2021-06-02 15:46:31,515][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 11.0 (TID 44). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,516][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,516][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 11.0 (TID 34)
[INFO][2021-06-02 15:46:31,518][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 11.0 (TID 33). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,518][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 11.0 (TID 45). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,495][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 11.0 (TID 31)
[INFO][2021-06-02 15:46:31,519][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,519][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,517][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,520][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,521][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,522][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,516][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,523][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 15:46:31,513][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,523][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 10 ms
[INFO][2021-06-02 15:46:31,509][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 11.0 (TID 48, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,527][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 11.0 (TID 49, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,528][org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 11.0 (TID 47). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,509][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,529][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,529][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,507][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 11.0 (TID 37). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,501][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,529][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 28 ms
[INFO][2021-06-02 15:46:31,530][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 11.0 (TID 35). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,501][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,531][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 30 ms
[INFO][2021-06-02 15:46:31,532][org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 11.0 (TID 49)
[INFO][2021-06-02 15:46:31,532][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 11.0 (TID 40). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,531][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 11.0 (TID 34). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,529][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 20 ms
[INFO][2021-06-02 15:46:31,529][org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 11.0 (TID 48)
[INFO][2021-06-02 15:46:31,528][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 11.0 (TID 50, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,536][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 11.0 (TID 41) in 42 ms on localhost (executor driver) (1/100)
[INFO][2021-06-02 15:46:31,536][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 11.0 (TID 31). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,526][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 11.0 (TID 32). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,537][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,537][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,538][org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 11.0 (TID 50)
[INFO][2021-06-02 15:46:31,521][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 11.0 (TID 36). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,537][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 11.0 (TID 42). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,537][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 11.0 (TID 46). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,536][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 11.0 (TID 33) in 45 ms on localhost (executor driver) (2/100)
[INFO][2021-06-02 15:46:31,539][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,539][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,540][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 11.0 (TID 44) in 46 ms on localhost (executor driver) (3/100)
[INFO][2021-06-02 15:46:31,540][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 11.0 (TID 43) in 46 ms on localhost (executor driver) (4/100)
[INFO][2021-06-02 15:46:31,541][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 11.0 (TID 38). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,542][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 11.0 (TID 51, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,543][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 11.0 (TID 52, localhost, executor driver, partition 46, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,543][org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 11.0 (TID 51)
[INFO][2021-06-02 15:46:31,544][org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 11.0 (TID 49). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,543][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 11.0 (TID 53, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,545][org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 11.0 (TID 48). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,543][org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 11.0 (TID 52)
[INFO][2021-06-02 15:46:31,545][org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 11.0 (TID 53)
[INFO][2021-06-02 15:46:31,545][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,545][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 11.0 (TID 54, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,546][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,547][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 11.0 (TID 55, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,548][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 11.0 (TID 56, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,546][org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 11.0 (TID 54)
[INFO][2021-06-02 15:46:31,549][org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 11.0 (TID 56)
[INFO][2021-06-02 15:46:31,550][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,550][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,551][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,551][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,549][org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 11.0 (TID 55)
[INFO][2021-06-02 15:46:31,552][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,550][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,553][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,553][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,554][org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 11.0 (TID 50). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,550][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 11.0 (TID 57, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,555][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 11.0 (TID 58, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,554][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,555][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,556][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 11.0 (TID 59, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,556][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 11.0 (TID 60, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,557][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,557][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,559][org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 11.0 (TID 58)
[INFO][2021-06-02 15:46:31,560][org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 11.0 (TID 54). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,555][org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 11.0 (TID 57)
[INFO][2021-06-02 15:46:31,561][org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 11.0 (TID 56). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,560][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 11.0 (TID 61, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,562][org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 11.0 (TID 52). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,557][org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 11.0 (TID 51). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,563][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,557][org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 11.0 (TID 60)
[INFO][2021-06-02 15:46:31,557][org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 11.0 (TID 59)
[INFO][2021-06-02 15:46:31,564][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,564][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 11.0 (TID 62, localhost, executor driver, partition 56, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,564][org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 11.0 (TID 55). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,565][org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 11.0 (TID 62)
[INFO][2021-06-02 15:46:31,563][org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 11.0 (TID 61)
[INFO][2021-06-02 15:46:31,562][org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 11.0 (TID 53). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,565][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 11.0 (TID 39) in 72 ms on localhost (executor driver) (5/100)
[INFO][2021-06-02 15:46:31,566][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 11.0 (TID 63, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,567][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 11.0 (TID 64, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,566][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,568][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,568][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,568][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 11.0 (TID 47) in 61 ms on localhost (executor driver) (6/100)
[INFO][2021-06-02 15:46:31,568][org.apache.spark.executor.Executor:54] - Running task 32.0 in stage 11.0 (TID 63)
[INFO][2021-06-02 15:46:31,569][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 11.0 (TID 45) in 74 ms on localhost (executor driver) (7/100)
[INFO][2021-06-02 15:46:31,568][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,569][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 11.0 (TID 34) in 77 ms on localhost (executor driver) (8/100)
[INFO][2021-06-02 15:46:31,570][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 11.0 (TID 31) in 80 ms on localhost (executor driver) (9/100)
[INFO][2021-06-02 15:46:31,570][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 11.0 (TID 32) in 79 ms on localhost (executor driver) (10/100)
[INFO][2021-06-02 15:46:31,571][org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 11.0 (TID 58). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,568][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,568][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,572][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,568][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,568][org.apache.spark.executor.Executor:54] - Running task 33.0 in stage 11.0 (TID 64)
[INFO][2021-06-02 15:46:31,573][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,573][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,573][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,572][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,571][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 11.0 (TID 36) in 79 ms on localhost (executor driver) (11/100)
[INFO][2021-06-02 15:46:31,576][org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 11.0 (TID 57). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,576][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 11.0 (TID 42) in 82 ms on localhost (executor driver) (12/100)
[INFO][2021-06-02 15:46:31,577][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 11.0 (TID 46) in 82 ms on localhost (executor driver) (13/100)
[INFO][2021-06-02 15:46:31,577][org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 11.0 (TID 62). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,578][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,578][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,578][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 11.0 (TID 65, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,579][org.apache.spark.executor.Executor:54] - Running task 34.0 in stage 11.0 (TID 65)
[INFO][2021-06-02 15:46:31,579][org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 11.0 (TID 60). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,579][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 11.0 (TID 40) in 86 ms on localhost (executor driver) (14/100)
[INFO][2021-06-02 15:46:31,580][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 11.0 (TID 35) in 88 ms on localhost (executor driver) (15/100)
[INFO][2021-06-02 15:46:31,580][org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 11.0 (TID 59). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,581][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 11.0 (TID 49) in 54 ms on localhost (executor driver) (16/100)
[INFO][2021-06-02 15:46:31,581][org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 11.0 (TID 61). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,582][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 11.0 (TID 66, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,582][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,582][org.apache.spark.executor.Executor:54] - Finished task 32.0 in stage 11.0 (TID 63). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,582][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 11.0 (TID 38) in 89 ms on localhost (executor driver) (17/100)
[INFO][2021-06-02 15:46:31,582][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,582][org.apache.spark.executor.Executor:54] - Running task 35.0 in stage 11.0 (TID 66)
[INFO][2021-06-02 15:46:31,583][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 11.0 (TID 37) in 91 ms on localhost (executor driver) (18/100)
[INFO][2021-06-02 15:46:31,583][org.apache.spark.executor.Executor:54] - Finished task 33.0 in stage 11.0 (TID 64). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,583][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 11.0 (TID 67, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,584][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 11.0 (TID 50) in 56 ms on localhost (executor driver) (19/100)
[INFO][2021-06-02 15:46:31,584][org.apache.spark.executor.Executor:54] - Running task 36.0 in stage 11.0 (TID 67)
[INFO][2021-06-02 15:46:31,584][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 11.0 (TID 54) in 39 ms on localhost (executor driver) (20/100)
[INFO][2021-06-02 15:46:31,584][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 11.0 (TID 48) in 75 ms on localhost (executor driver) (21/100)
[INFO][2021-06-02 15:46:31,585][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 11.0 (TID 68, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,586][org.apache.spark.executor.Executor:54] - Running task 37.0 in stage 11.0 (TID 68)
[INFO][2021-06-02 15:46:31,586][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 11.0 (TID 69, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,586][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,586][org.apache.spark.executor.Executor:54] - Running task 38.0 in stage 11.0 (TID 69)
[INFO][2021-06-02 15:46:31,586][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,587][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 11.0 (TID 70, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,587][org.apache.spark.executor.Executor:54] - Running task 39.0 in stage 11.0 (TID 70)
[INFO][2021-06-02 15:46:31,587][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 11.0 (TID 71, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,587][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,588][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,588][org.apache.spark.executor.Executor:54] - Finished task 34.0 in stage 11.0 (TID 65). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,588][org.apache.spark.executor.Executor:54] - Running task 40.0 in stage 11.0 (TID 71)
[INFO][2021-06-02 15:46:31,588][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 11.0 (TID 72, localhost, executor driver, partition 66, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,589][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 11.0 (TID 52) in 47 ms on localhost (executor driver) (22/100)
[INFO][2021-06-02 15:46:31,589][org.apache.spark.executor.Executor:54] - Running task 41.0 in stage 11.0 (TID 72)
[INFO][2021-06-02 15:46:31,589][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 11.0 (TID 56) in 41 ms on localhost (executor driver) (23/100)
[INFO][2021-06-02 15:46:31,589][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,590][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,590][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 11.0 (TID 51) in 49 ms on localhost (executor driver) (24/100)
[INFO][2021-06-02 15:46:31,590][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,591][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,591][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,591][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 11.0 (TID 55) in 45 ms on localhost (executor driver) (25/100)
[INFO][2021-06-02 15:46:31,591][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,592][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 11.0 (TID 73, localhost, executor driver, partition 67, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,592][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,593][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,593][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 11.0 (TID 74, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,594][org.apache.spark.executor.Executor:54] - Running task 42.0 in stage 11.0 (TID 73)
[INFO][2021-06-02 15:46:31,594][org.apache.spark.executor.Executor:54] - Running task 43.0 in stage 11.0 (TID 74)
[INFO][2021-06-02 15:46:31,594][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 11.0 (TID 75, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,594][org.apache.spark.executor.Executor:54] - Finished task 36.0 in stage 11.0 (TID 67). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,594][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,595][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,594][org.apache.spark.executor.Executor:54] - Running task 44.0 in stage 11.0 (TID 75)
[INFO][2021-06-02 15:46:31,594][org.apache.spark.executor.Executor:54] - Finished task 35.0 in stage 11.0 (TID 66). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,596][org.apache.spark.executor.Executor:54] - Finished task 37.0 in stage 11.0 (TID 68). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,595][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 11.0 (TID 76, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,596][org.apache.spark.executor.Executor:54] - Finished task 39.0 in stage 11.0 (TID 70). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,597][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 11.0 (TID 53) in 54 ms on localhost (executor driver) (26/100)
[INFO][2021-06-02 15:46:31,597][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,597][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,598][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 11.0 (TID 77, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,597][org.apache.spark.executor.Executor:54] - Running task 45.0 in stage 11.0 (TID 76)
[INFO][2021-06-02 15:46:31,598][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 11.0 (TID 78, localhost, executor driver, partition 72, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,598][org.apache.spark.executor.Executor:54] - Running task 46.0 in stage 11.0 (TID 77)
[INFO][2021-06-02 15:46:31,598][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,598][org.apache.spark.executor.Executor:54] - Finished task 38.0 in stage 11.0 (TID 69). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,599][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,599][org.apache.spark.executor.Executor:54] - Finished task 40.0 in stage 11.0 (TID 71). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,599][org.apache.spark.executor.Executor:54] - Running task 47.0 in stage 11.0 (TID 78)
[INFO][2021-06-02 15:46:31,599][org.apache.spark.executor.Executor:54] - Finished task 41.0 in stage 11.0 (TID 72). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,599][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 11.0 (TID 79, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,600][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,601][org.apache.spark.executor.Executor:54] - Running task 48.0 in stage 11.0 (TID 79)
[INFO][2021-06-02 15:46:31,601][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 11.0 (TID 80, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,601][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,603][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,603][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,603][org.apache.spark.executor.Executor:54] - Finished task 42.0 in stage 11.0 (TID 73). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,602][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 11.0 (TID 57) in 53 ms on localhost (executor driver) (27/100)
[INFO][2021-06-02 15:46:31,604][org.apache.spark.executor.Executor:54] - Running task 49.0 in stage 11.0 (TID 80)
[INFO][2021-06-02 15:46:31,604][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 11.0 (TID 59) in 49 ms on localhost (executor driver) (28/100)
[INFO][2021-06-02 15:46:31,603][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,604][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,605][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,605][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,604][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 11.0 (TID 62) in 41 ms on localhost (executor driver) (29/100)
[INFO][2021-06-02 15:46:31,606][org.apache.spark.executor.Executor:54] - Finished task 43.0 in stage 11.0 (TID 74). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,606][org.apache.spark.executor.Executor:54] - Finished task 44.0 in stage 11.0 (TID 75). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,606][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 11.0 (TID 63) in 40 ms on localhost (executor driver) (30/100)
[INFO][2021-06-02 15:46:31,606][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 11.0 (TID 60) in 50 ms on localhost (executor driver) (31/100)
[INFO][2021-06-02 15:46:31,606][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,607][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,608][org.apache.spark.executor.Executor:54] - Finished task 46.0 in stage 11.0 (TID 77). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,608][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 11.0 (TID 81, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,608][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,608][org.apache.spark.executor.Executor:54] - Running task 50.0 in stage 11.0 (TID 81)
[INFO][2021-06-02 15:46:31,608][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 11.0 (TID 58) in 54 ms on localhost (executor driver) (32/100)
[INFO][2021-06-02 15:46:31,610][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 11.0 (TID 82, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,610][org.apache.spark.executor.Executor:54] - Finished task 47.0 in stage 11.0 (TID 78). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,610][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 11.0 (TID 61) in 53 ms on localhost (executor driver) (33/100)
[INFO][2021-06-02 15:46:31,610][org.apache.spark.executor.Executor:54] - Running task 51.0 in stage 11.0 (TID 82)
[INFO][2021-06-02 15:46:31,611][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 11.0 (TID 83, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,611][org.apache.spark.executor.Executor:54] - Finished task 45.0 in stage 11.0 (TID 76). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,612][org.apache.spark.executor.Executor:54] - Running task 52.0 in stage 11.0 (TID 83)
[INFO][2021-06-02 15:46:31,612][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 11.0 (TID 84, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,612][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,612][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 11.0 (TID 65) in 34 ms on localhost (executor driver) (34/100)
[INFO][2021-06-02 15:46:31,612][org.apache.spark.executor.Executor:54] - Running task 53.0 in stage 11.0 (TID 84)
[INFO][2021-06-02 15:46:31,612][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,612][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 11.0 (TID 67) in 29 ms on localhost (executor driver) (35/100)
[INFO][2021-06-02 15:46:31,613][org.apache.spark.executor.Executor:54] - Finished task 48.0 in stage 11.0 (TID 79). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,614][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,614][org.apache.spark.executor.Executor:54] - Finished task 49.0 in stage 11.0 (TID 80). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,614][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,614][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 11.0 (TID 85, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,614][org.apache.spark.executor.Executor:54] - Running task 54.0 in stage 11.0 (TID 85)
[INFO][2021-06-02 15:46:31,615][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 11.0 (TID 86, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,615][org.apache.spark.executor.Executor:54] - Running task 55.0 in stage 11.0 (TID 86)
[INFO][2021-06-02 15:46:31,615][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,615][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 11.0 (TID 87, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,616][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,616][org.apache.spark.executor.Executor:54] - Running task 56.0 in stage 11.0 (TID 87)
[INFO][2021-06-02 15:46:31,616][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,617][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,616][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 11.0 (TID 88, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,617][org.apache.spark.executor.Executor:54] - Finished task 50.0 in stage 11.0 (TID 81). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,618][org.apache.spark.executor.Executor:54] - Running task 57.0 in stage 11.0 (TID 88)
[INFO][2021-06-02 15:46:31,619][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,619][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,618][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 11.0 (TID 89, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,619][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,619][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,620][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 11.0 (TID 90, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,619][org.apache.spark.executor.Executor:54] - Running task 58.0 in stage 11.0 (TID 89)
[INFO][2021-06-02 15:46:31,620][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,621][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,619][org.apache.spark.executor.Executor:54] - Finished task 51.0 in stage 11.0 (TID 82). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,621][org.apache.spark.executor.Executor:54] - Finished task 53.0 in stage 11.0 (TID 84). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,622][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,622][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,621][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 11.0 (TID 91, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,623][org.apache.spark.executor.Executor:54] - Finished task 52.0 in stage 11.0 (TID 83). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,623][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,623][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,620][org.apache.spark.executor.Executor:54] - Running task 59.0 in stage 11.0 (TID 90)
[INFO][2021-06-02 15:46:31,623][org.apache.spark.executor.Executor:54] - Running task 60.0 in stage 11.0 (TID 91)
[INFO][2021-06-02 15:46:31,623][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 11.0 (TID 92, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,624][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 11.0 (TID 93, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,625][org.apache.spark.executor.Executor:54] - Running task 61.0 in stage 11.0 (TID 92)
[INFO][2021-06-02 15:46:31,626][org.apache.spark.executor.Executor:54] - Running task 62.0 in stage 11.0 (TID 93)
[INFO][2021-06-02 15:46:31,627][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 11.0 (TID 94, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,627][org.apache.spark.executor.Executor:54] - Finished task 55.0 in stage 11.0 (TID 86). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,628][org.apache.spark.executor.Executor:54] - Running task 64.0 in stage 11.0 (TID 94)
[INFO][2021-06-02 15:46:31,627][org.apache.spark.executor.Executor:54] - Finished task 58.0 in stage 11.0 (TID 89). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,629][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,629][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,629][org.apache.spark.executor.Executor:54] - Finished task 54.0 in stage 11.0 (TID 85). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,628][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 11.0 (TID 95, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,629][org.apache.spark.executor.Executor:54] - Finished task 57.0 in stage 11.0 (TID 88). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,630][org.apache.spark.executor.Executor:54] - Running task 65.0 in stage 11.0 (TID 95)
[INFO][2021-06-02 15:46:31,630][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 11.0 (TID 96, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,631][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 11.0 (TID 64) in 64 ms on localhost (executor driver) (36/100)
[INFO][2021-06-02 15:46:31,631][org.apache.spark.executor.Executor:54] - Running task 66.0 in stage 11.0 (TID 96)
[INFO][2021-06-02 15:46:31,631][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 11.0 (TID 69) in 45 ms on localhost (executor driver) (37/100)
[INFO][2021-06-02 15:46:31,631][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,632][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,632][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 11.0 (TID 97, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,633][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,633][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,633][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,633][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,634][org.apache.spark.executor.Executor:54] - Finished task 59.0 in stage 11.0 (TID 90). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,630][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,630][org.apache.spark.executor.Executor:54] - Finished task 56.0 in stage 11.0 (TID 87). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,634][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,634][org.apache.spark.executor.Executor:54] - Running task 67.0 in stage 11.0 (TID 97)
[INFO][2021-06-02 15:46:31,634][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,634][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,635][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,633][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 11.0 (TID 98, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,635][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 11.0 (TID 66) in 54 ms on localhost (executor driver) (38/100)
[INFO][2021-06-02 15:46:31,635][org.apache.spark.executor.Executor:54] - Running task 68.0 in stage 11.0 (TID 98)
[INFO][2021-06-02 15:46:31,637][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 41.0 in stage 11.0 (TID 72) in 49 ms on localhost (executor driver) (39/100)
[INFO][2021-06-02 15:46:31,635][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,637][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.0 in stage 11.0 (TID 73) in 46 ms on localhost (executor driver) (40/100)
[INFO][2021-06-02 15:46:31,639][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,639][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,640][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 43.0 in stage 11.0 (TID 74) in 47 ms on localhost (executor driver) (41/100)
[INFO][2021-06-02 15:46:31,640][org.apache.spark.executor.Executor:54] - Finished task 62.0 in stage 11.0 (TID 93). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,640][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,640][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,639][org.apache.spark.executor.Executor:54] - Finished task 60.0 in stage 11.0 (TID 91). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,641][org.apache.spark.executor.Executor:54] - Finished task 61.0 in stage 11.0 (TID 92). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,640][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 11.0 (TID 70) in 54 ms on localhost (executor driver) (42/100)
[INFO][2021-06-02 15:46:31,640][org.apache.spark.executor.Executor:54] - Finished task 65.0 in stage 11.0 (TID 95). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,642][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 46.0 in stage 11.0 (TID 77) in 45 ms on localhost (executor driver) (43/100)
[INFO][2021-06-02 15:46:31,642][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 47.0 in stage 11.0 (TID 78) in 44 ms on localhost (executor driver) (44/100)
[INFO][2021-06-02 15:46:31,643][org.apache.spark.executor.Executor:54] - Finished task 64.0 in stage 11.0 (TID 94). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,643][org.apache.spark.executor.Executor:54] - Finished task 66.0 in stage 11.0 (TID 96). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,643][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 11.0 (TID 99, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,644][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 40.0 in stage 11.0 (TID 71) in 57 ms on localhost (executor driver) (45/100)
[INFO][2021-06-02 15:46:31,644][org.apache.spark.executor.Executor:54] - Running task 69.0 in stage 11.0 (TID 99)
[INFO][2021-06-02 15:46:31,644][org.apache.spark.executor.Executor:54] - Finished task 67.0 in stage 11.0 (TID 97). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,644][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 45.0 in stage 11.0 (TID 76) in 49 ms on localhost (executor driver) (46/100)
[INFO][2021-06-02 15:46:31,644][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.0 in stage 11.0 (TID 79) in 45 ms on localhost (executor driver) (47/100)
[INFO][2021-06-02 15:46:31,645][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 44.0 in stage 11.0 (TID 75) in 51 ms on localhost (executor driver) (48/100)
[INFO][2021-06-02 15:46:31,645][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 11.0 (TID 68) in 60 ms on localhost (executor driver) (49/100)
[INFO][2021-06-02 15:46:31,645][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.0 in stage 11.0 (TID 82) in 35 ms on localhost (executor driver) (50/100)
[INFO][2021-06-02 15:46:31,646][org.apache.spark.executor.Executor:54] - Finished task 68.0 in stage 11.0 (TID 98). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,646][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 49.0 in stage 11.0 (TID 80) in 45 ms on localhost (executor driver) (51/100)
[INFO][2021-06-02 15:46:31,646][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 11.0 (TID 100, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,646][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 50.0 in stage 11.0 (TID 81) in 38 ms on localhost (executor driver) (52/100)
[INFO][2021-06-02 15:46:31,647][org.apache.spark.executor.Executor:54] - Running task 70.0 in stage 11.0 (TID 100)
[INFO][2021-06-02 15:46:31,647][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,647][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 52.0 in stage 11.0 (TID 83) in 36 ms on localhost (executor driver) (53/100)
[INFO][2021-06-02 15:46:31,647][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,647][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 53.0 in stage 11.0 (TID 84) in 36 ms on localhost (executor driver) (54/100)
[INFO][2021-06-02 15:46:31,648][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 11.0 (TID 101, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,648][org.apache.spark.executor.Executor:54] - Running task 71.0 in stage 11.0 (TID 101)
[INFO][2021-06-02 15:46:31,648][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 55.0 in stage 11.0 (TID 86) in 34 ms on localhost (executor driver) (55/100)
[INFO][2021-06-02 15:46:31,649][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 11.0 (TID 102, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,649][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 58.0 in stage 11.0 (TID 89) in 31 ms on localhost (executor driver) (56/100)
[INFO][2021-06-02 15:46:31,650][org.apache.spark.executor.Executor:54] - Running task 72.0 in stage 11.0 (TID 102)
[INFO][2021-06-02 15:46:31,650][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 11.0 (TID 103, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,650][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,651][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,651][org.apache.spark.executor.Executor:54] - Running task 73.0 in stage 11.0 (TID 103)
[INFO][2021-06-02 15:46:31,651][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 11.0 (TID 104, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,651][org.apache.spark.executor.Executor:54] - Running task 74.0 in stage 11.0 (TID 104)
[INFO][2021-06-02 15:46:31,652][org.apache.spark.executor.Executor:54] - Finished task 69.0 in stage 11.0 (TID 99). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,652][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 75.0 in stage 11.0 (TID 105, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,653][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,653][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,653][org.apache.spark.executor.Executor:54] - Running task 75.0 in stage 11.0 (TID 105)
[INFO][2021-06-02 15:46:31,654][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 76.0 in stage 11.0 (TID 106, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,654][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,655][org.apache.spark.executor.Executor:54] - Running task 76.0 in stage 11.0 (TID 106)
[INFO][2021-06-02 15:46:31,655][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.0 in stage 11.0 (TID 85) in 41 ms on localhost (executor driver) (57/100)
[INFO][2021-06-02 15:46:31,655][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,656][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,656][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 77.0 in stage 11.0 (TID 107, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,657][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 78.0 in stage 11.0 (TID 108, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,655][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,658][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 79.0 in stage 11.0 (TID 109, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,658][org.apache.spark.executor.Executor:54] - Running task 78.0 in stage 11.0 (TID 108)
[INFO][2021-06-02 15:46:31,659][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 57.0 in stage 11.0 (TID 88) in 43 ms on localhost (executor driver) (58/100)
[INFO][2021-06-02 15:46:31,659][org.apache.spark.executor.Executor:54] - Running task 79.0 in stage 11.0 (TID 109)
[INFO][2021-06-02 15:46:31,659][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 60.0 in stage 11.0 (TID 91) in 39 ms on localhost (executor driver) (59/100)
[INFO][2021-06-02 15:46:31,660][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 61.0 in stage 11.0 (TID 92) in 37 ms on localhost (executor driver) (60/100)
[INFO][2021-06-02 15:46:31,661][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 56.0 in stage 11.0 (TID 87) in 46 ms on localhost (executor driver) (61/100)
[INFO][2021-06-02 15:46:31,658][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,662][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,658][org.apache.spark.executor.Executor:54] - Running task 77.0 in stage 11.0 (TID 107)
[INFO][2021-06-02 15:46:31,664][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,662][org.apache.spark.executor.Executor:54] - Finished task 73.0 in stage 11.0 (TID 103). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,662][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 59.0 in stage 11.0 (TID 90) in 42 ms on localhost (executor driver) (62/100)
[INFO][2021-06-02 15:46:31,662][org.apache.spark.executor.Executor:54] - Finished task 71.0 in stage 11.0 (TID 101). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,662][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,660][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,665][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,664][org.apache.spark.executor.Executor:54] - Finished task 72.0 in stage 11.0 (TID 102). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,664][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 62.0 in stage 11.0 (TID 93) in 40 ms on localhost (executor driver) (63/100)
[INFO][2021-06-02 15:46:31,664][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,665][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 65.0 in stage 11.0 (TID 95) in 38 ms on localhost (executor driver) (64/100)
[INFO][2021-06-02 15:46:31,664][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,666][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,666][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 80.0 in stage 11.0 (TID 110, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,667][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 81.0 in stage 11.0 (TID 111, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,668][org.apache.spark.executor.Executor:54] - Running task 81.0 in stage 11.0 (TID 111)
[INFO][2021-06-02 15:46:31,665][org.apache.spark.executor.Executor:54] - Finished task 70.0 in stage 11.0 (TID 100). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,665][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,669][org.apache.spark.executor.Executor:54] - Finished task 75.0 in stage 11.0 (TID 105). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,668][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 82.0 in stage 11.0 (TID 112, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,667][org.apache.spark.executor.Executor:54] - Running task 80.0 in stage 11.0 (TID 110)
[INFO][2021-06-02 15:46:31,670][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 83.0 in stage 11.0 (TID 113, localhost, executor driver, partition 108, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,667][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,671][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,671][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 84.0 in stage 11.0 (TID 114, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,672][org.apache.spark.executor.Executor:54] - Running task 82.0 in stage 11.0 (TID 112)
[INFO][2021-06-02 15:46:31,672][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 85.0 in stage 11.0 (TID 115, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,675][org.apache.spark.executor.Executor:54] - Finished task 79.0 in stage 11.0 (TID 109). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,676][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 86.0 in stage 11.0 (TID 116, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,677][org.apache.spark.executor.Executor:54] - Running task 84.0 in stage 11.0 (TID 114)
[INFO][2021-06-02 15:46:31,677][org.apache.spark.executor.Executor:54] - Running task 86.0 in stage 11.0 (TID 116)
[INFO][2021-06-02 15:46:31,678][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 87.0 in stage 11.0 (TID 117, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,679][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 88.0 in stage 11.0 (TID 118, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,680][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 89.0 in stage 11.0 (TID 119, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,682][org.apache.spark.executor.Executor:54] - Finished task 74.0 in stage 11.0 (TID 104). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,682][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,682][org.apache.spark.executor.Executor:54] - Running task 88.0 in stage 11.0 (TID 118)
[INFO][2021-06-02 15:46:31,677][org.apache.spark.executor.Executor:54] - Running task 83.0 in stage 11.0 (TID 113)
[INFO][2021-06-02 15:46:31,682][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,683][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,676][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,685][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 15:46:31,676][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,685][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 9 ms
[INFO][2021-06-02 15:46:31,676][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,688][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 12 ms
[INFO][2021-06-02 15:46:31,676][org.apache.spark.executor.Executor:54] - Finished task 78.0 in stage 11.0 (TID 108). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,688][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,684][org.apache.spark.executor.Executor:54] - Running task 89.0 in stage 11.0 (TID 119)
[INFO][2021-06-02 15:46:31,682][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,682][org.apache.spark.executor.Executor:54] - Running task 87.0 in stage 11.0 (TID 117)
[INFO][2021-06-02 15:46:31,695][org.apache.spark.executor.Executor:54] - Finished task 80.0 in stage 11.0 (TID 110). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,695][org.apache.spark.executor.Executor:54] - Finished task 86.0 in stage 11.0 (TID 116). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,696][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,696][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,682][org.apache.spark.executor.Executor:54] - Running task 85.0 in stage 11.0 (TID 115)
[INFO][2021-06-02 15:46:31,682][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 64.0 in stage 11.0 (TID 94) in 55 ms on localhost (executor driver) (65/100)
[INFO][2021-06-02 15:46:31,680][org.apache.spark.executor.Executor:54] - Finished task 77.0 in stage 11.0 (TID 107). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,697][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.0 in stage 11.0 (TID 96) in 67 ms on localhost (executor driver) (66/100)
[INFO][2021-06-02 15:46:31,697][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 73.0 in stage 11.0 (TID 103) in 47 ms on localhost (executor driver) (67/100)
[INFO][2021-06-02 15:46:31,698][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 71.0 in stage 11.0 (TID 101) in 50 ms on localhost (executor driver) (68/100)
[INFO][2021-06-02 15:46:31,698][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,698][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,698][org.apache.spark.executor.Executor:54] - Finished task 82.0 in stage 11.0 (TID 112). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,678][org.apache.spark.executor.Executor:54] - Finished task 76.0 in stage 11.0 (TID 106). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,699][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 90.0 in stage 11.0 (TID 120, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,689][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,689][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,701][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 91.0 in stage 11.0 (TID 121, localhost, executor driver, partition 116, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,700][org.apache.spark.executor.Executor:54] - Running task 90.0 in stage 11.0 (TID 120)
[INFO][2021-06-02 15:46:31,699][org.apache.spark.executor.Executor:54] - Finished task 81.0 in stage 11.0 (TID 111). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,702][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 92.0 in stage 11.0 (TID 122, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,699][org.apache.spark.executor.Executor:54] - Finished task 84.0 in stage 11.0 (TID 114). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,704][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 93.0 in stage 11.0 (TID 123, localhost, executor driver, partition 118, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,704][org.apache.spark.executor.Executor:54] - Running task 92.0 in stage 11.0 (TID 122)
[INFO][2021-06-02 15:46:31,702][org.apache.spark.executor.Executor:54] - Running task 91.0 in stage 11.0 (TID 121)
[INFO][2021-06-02 15:46:31,701][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 12 ms
[INFO][2021-06-02 15:46:31,706][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 68.0 in stage 11.0 (TID 98) in 73 ms on localhost (executor driver) (69/100)
[INFO][2021-06-02 15:46:31,706][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,706][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,710][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,705][org.apache.spark.executor.Executor:54] - Running task 93.0 in stage 11.0 (TID 123)
[INFO][2021-06-02 15:46:31,709][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 67.0 in stage 11.0 (TID 97) in 77 ms on localhost (executor driver) (70/100)
[INFO][2021-06-02 15:46:31,708][org.apache.spark.executor.Executor:54] - Finished task 89.0 in stage 11.0 (TID 119). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,712][org.apache.spark.executor.Executor:54] - Finished task 88.0 in stage 11.0 (TID 118). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,711][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 75.0 in stage 11.0 (TID 105) in 59 ms on localhost (executor driver) (71/100)
[INFO][2021-06-02 15:46:31,714][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 79.0 in stage 11.0 (TID 109) in 56 ms on localhost (executor driver) (72/100)
[INFO][2021-06-02 15:46:31,715][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,715][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,711][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 15:46:31,716][org.apache.spark.executor.Executor:54] - Finished task 83.0 in stage 11.0 (TID 113). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,716][org.apache.spark.executor.Executor:54] - Finished task 87.0 in stage 11.0 (TID 117). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,715][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 74.0 in stage 11.0 (TID 104) in 64 ms on localhost (executor driver) (73/100)
[INFO][2021-06-02 15:46:31,714][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,717][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,713][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,717][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.0 in stage 11.0 (TID 102) in 68 ms on localhost (executor driver) (74/100)
[INFO][2021-06-02 15:46:31,718][org.apache.spark.executor.Executor:54] - Finished task 90.0 in stage 11.0 (TID 120). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,718][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,718][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 69.0 in stage 11.0 (TID 99) in 75 ms on localhost (executor driver) (75/100)
[INFO][2021-06-02 15:46:31,721][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 78.0 in stage 11.0 (TID 108) in 64 ms on localhost (executor driver) (76/100)
[INFO][2021-06-02 15:46:31,721][org.apache.spark.executor.Executor:54] - Finished task 93.0 in stage 11.0 (TID 123). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,722][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 94.0 in stage 11.0 (TID 124, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,722][org.apache.spark.executor.Executor:54] - Running task 94.0 in stage 11.0 (TID 124)
[INFO][2021-06-02 15:46:31,723][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 95.0 in stage 11.0 (TID 125, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,723][org.apache.spark.executor.Executor:54] - Finished task 85.0 in stage 11.0 (TID 115). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,724][org.apache.spark.executor.Executor:54] - Running task 95.0 in stage 11.0 (TID 125)
[INFO][2021-06-02 15:46:31,724][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 96.0 in stage 11.0 (TID 126, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,725][org.apache.spark.executor.Executor:54] - Running task 96.0 in stage 11.0 (TID 126)
[INFO][2021-06-02 15:46:31,726][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 97.0 in stage 11.0 (TID 127, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,726][org.apache.spark.executor.Executor:54] - Finished task 91.0 in stage 11.0 (TID 121). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,727][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 98.0 in stage 11.0 (TID 128, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,728][org.apache.spark.executor.Executor:54] - Running task 97.0 in stage 11.0 (TID 127)
[INFO][2021-06-02 15:46:31,728][org.apache.spark.executor.Executor:54] - Finished task 92.0 in stage 11.0 (TID 122). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,729][org.apache.spark.executor.Executor:54] - Running task 98.0 in stage 11.0 (TID 128)
[INFO][2021-06-02 15:46:31,728][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,728][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 99.0 in stage 11.0 (TID 129, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,730][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,731][org.apache.spark.executor.Executor:54] - Running task 99.0 in stage 11.0 (TID 129)
[INFO][2021-06-02 15:46:31,730][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 11.0 (TID 130, localhost, executor driver, partition 88, ANY, 4726 bytes)
[INFO][2021-06-02 15:46:31,730][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,733][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,730][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,733][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,732][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 86.0 in stage 11.0 (TID 116) in 59 ms on localhost (executor driver) (77/100)
[INFO][2021-06-02 15:46:31,734][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 82.0 in stage 11.0 (TID 112) in 66 ms on localhost (executor driver) (78/100)
[INFO][2021-06-02 15:46:31,735][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 76.0 in stage 11.0 (TID 106) in 82 ms on localhost (executor driver) (79/100)
[INFO][2021-06-02 15:46:31,736][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 81.0 in stage 11.0 (TID 111) in 69 ms on localhost (executor driver) (80/100)
[INFO][2021-06-02 15:46:31,732][org.apache.spark.executor.Executor:54] - Running task 63.0 in stage 11.0 (TID 130)
[INFO][2021-06-02 15:46:31,737][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 84.0 in stage 11.0 (TID 114) in 66 ms on localhost (executor driver) (81/100)
[INFO][2021-06-02 15:46:31,737][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,737][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,734][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,738][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,737][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 89.0 in stage 11.0 (TID 119) in 57 ms on localhost (executor driver) (82/100)
[INFO][2021-06-02 15:46:31,740][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 70.0 in stage 11.0 (TID 100) in 94 ms on localhost (executor driver) (83/100)
[INFO][2021-06-02 15:46:31,739][org.apache.spark.executor.Executor:54] - Finished task 94.0 in stage 11.0 (TID 124). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,742][org.apache.spark.executor.Executor:54] - Finished task 96.0 in stage 11.0 (TID 126). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,739][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,743][org.apache.spark.executor.Executor:54] - Finished task 95.0 in stage 11.0 (TID 125). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,742][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 77.0 in stage 11.0 (TID 107) in 86 ms on localhost (executor driver) (84/100)
[INFO][2021-06-02 15:46:31,743][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,743][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,744][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,744][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 80.0 in stage 11.0 (TID 110) in 78 ms on localhost (executor driver) (85/100)
[INFO][2021-06-02 15:46:31,745][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 88.0 in stage 11.0 (TID 118) in 66 ms on localhost (executor driver) (86/100)
[INFO][2021-06-02 15:46:31,746][org.apache.spark.executor.Executor:54] - Finished task 98.0 in stage 11.0 (TID 128). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,747][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 83.0 in stage 11.0 (TID 113) in 77 ms on localhost (executor driver) (87/100)
[INFO][2021-06-02 15:46:31,748][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 87.0 in stage 11.0 (TID 117) in 71 ms on localhost (executor driver) (88/100)
[INFO][2021-06-02 15:46:31,761][org.apache.spark.executor.Executor:54] - Finished task 97.0 in stage 11.0 (TID 127). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,761][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 93.0 in stage 11.0 (TID 123) in 57 ms on localhost (executor driver) (89/100)
[INFO][2021-06-02 15:46:31,763][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 90.0 in stage 11.0 (TID 120) in 64 ms on localhost (executor driver) (90/100)
[INFO][2021-06-02 15:46:31,763][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_3_piece0 on 192.168.52.10:49811 in memory (size: 18.7 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,764][org.apache.spark.executor.Executor:54] - Finished task 99.0 in stage 11.0 (TID 129). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,764][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 85.0 in stage 11.0 (TID 115) in 92 ms on localhost (executor driver) (91/100)
[INFO][2021-06-02 15:46:31,766][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 91.0 in stage 11.0 (TID 121) in 66 ms on localhost (executor driver) (92/100)
[INFO][2021-06-02 15:46:31,767][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 92.0 in stage 11.0 (TID 122) in 65 ms on localhost (executor driver) (93/100)
[INFO][2021-06-02 15:46:31,767][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_4_piece0 on 192.168.52.10:49811 in memory (size: 18.7 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,768][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 95.0 in stage 11.0 (TID 125) in 45 ms on localhost (executor driver) (94/100)
[INFO][2021-06-02 15:46:31,768][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 96.0 in stage 11.0 (TID 126) in 45 ms on localhost (executor driver) (95/100)
[INFO][2021-06-02 15:46:31,769][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.52.10:49811 in memory (size: 19.2 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,769][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 94.0 in stage 11.0 (TID 124) in 47 ms on localhost (executor driver) (96/100)
[INFO][2021-06-02 15:46:31,769][org.apache.spark.executor.Executor:54] - Finished task 63.0 in stage 11.0 (TID 130). 3306 bytes result sent to driver
[INFO][2021-06-02 15:46:31,770][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 97.0 in stage 11.0 (TID 127) in 45 ms on localhost (executor driver) (97/100)
[INFO][2021-06-02 15:46:31,770][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 98.0 in stage 11.0 (TID 128) in 43 ms on localhost (executor driver) (98/100)
[INFO][2021-06-02 15:46:31,771][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 99.0 in stage 11.0 (TID 129) in 42 ms on localhost (executor driver) (99/100)
[INFO][2021-06-02 15:46:31,771][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 63.0 in stage 11.0 (TID 130) in 41 ms on localhost (executor driver) (100/100)
[INFO][2021-06-02 15:46:31,771][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:46:31,772][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 11 (show at UnionALLRegisterTempView.java:29) finished in 0.282 s
[INFO][2021-06-02 15:46:31,773][org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: show at UnionALLRegisterTempView.java:29, took 0.299464 s
[INFO][2021-06-02 15:46:31,789][org.apache.spark.SparkContext:54] - Starting job: show at UnionALLRegisterTempView.java:29
[INFO][2021-06-02 15:46:31,791][org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (show at UnionALLRegisterTempView.java:29) with 75 output partitions
[INFO][2021-06-02 15:46:31,791][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 14 (show at UnionALLRegisterTempView.java:29)
[INFO][2021-06-02 15:46:31,791][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 13)
[INFO][2021-06-02 15:46:31,792][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 15:46:31,792][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 14 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29), which has no missing parents
[INFO][2021-06-02 15:46:31,796][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7 stored as values in memory (estimated size 51.0 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,799][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KB, free 3.8 GB)
[INFO][2021-06-02 15:46:31,800][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_7_piece0 in memory on 192.168.52.10:49811 (size: 18.7 KB, free: 3.8 GB)
[INFO][2021-06-02 15:46:31,801][org.apache.spark.SparkContext:54] - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 15:46:31,801][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 75 missing tasks from ResultStage 14 (MapPartitionsRDD[11] at show at UnionALLRegisterTempView.java:29) (first 15 tasks are for partitions Vector(125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139))
[INFO][2021-06-02 15:46:31,802][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 14.0 with 75 tasks
[INFO][2021-06-02 15:46:31,802][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 14.0 (TID 131, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,803][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 14.0 (TID 132, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,803][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 14.0 (TID 133, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,803][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 14.0 (TID 134, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,803][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 14.0 (TID 135, localhost, executor driver, partition 129, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,804][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 14.0 (TID 136, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,804][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 14.0 (TID 137, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,804][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 14.0 (TID 138, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,804][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 14.0 (TID 139, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,805][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 14.0 (TID 140, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,805][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 14.0 (TID 141, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,805][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 14.0 (TID 142, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,805][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 14.0 (TID 143, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,805][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 14.0 (TID 144, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,805][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 14.0 (TID 145, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 14.0 (TID 146, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 14.0 (TID 137)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 14.0 (TID 132)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 14.0 (TID 136)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 14.0 (TID 138)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 14.0 (TID 139)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 14.0 (TID 140)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 14.0 (TID 134)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 14.0 (TID 131)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 14.0 (TID 144)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 14.0 (TID 133)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 14.0 (TID 145)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 14.0 (TID 146)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 14.0 (TID 141)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 14.0 (TID 143)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 14.0 (TID 142)
[INFO][2021-06-02 15:46:31,806][org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 14.0 (TID 135)
[INFO][2021-06-02 15:46:31,811][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,812][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,812][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,812][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,813][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,813][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,813][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,813][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,814][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,814][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,812][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,814][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,815][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,815][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,814][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,814][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,817][org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 14.0 (TID 141). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,813][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,813][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,813][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,818][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,818][org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 14.0 (TID 140). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,818][org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 14.0 (TID 142). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,818][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 14.0 (TID 132). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,818][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,818][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 14.0 (TID 147, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,820][org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 14.0 (TID 146). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,820][org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 14.0 (TID 147)
[INFO][2021-06-02 15:46:31,817][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,817][org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 14.0 (TID 145). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,817][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 14.0 (TID 131). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,816][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,815][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,815][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,814][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,821][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 15:46:31,821][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 15:46:31,821][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,821][org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 14.0 (TID 137). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,820][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,820][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 14.0 (TID 148, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,819][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
[INFO][2021-06-02 15:46:31,825][org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 14.0 (TID 148)
[INFO][2021-06-02 15:46:31,824][org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 14.0 (TID 143). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,823][org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 14.0 (TID 136). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,823][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,821][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 15:46:31,827][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,827][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 14.0 (TID 141) in 22 ms on localhost (executor driver) (1/75)
[INFO][2021-06-02 15:46:31,826][org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 14.0 (TID 135). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,828][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 14.0 (TID 140) in 23 ms on localhost (executor driver) (2/75)
[INFO][2021-06-02 15:46:31,826][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 14.0 (TID 134). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,826][org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 14.0 (TID 139). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,829][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 14.0 (TID 133). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,829][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 14.0 (TID 142) in 24 ms on localhost (executor driver) (3/75)
[INFO][2021-06-02 15:46:31,829][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,830][org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 14.0 (TID 144). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,830][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,830][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 14.0 (TID 149, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,831][org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 14.0 (TID 149)
[INFO][2021-06-02 15:46:31,831][org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 14.0 (TID 138). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,831][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 14.0 (TID 150, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,831][org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 14.0 (TID 147). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,832][org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 14.0 (TID 150)
[INFO][2021-06-02 15:46:31,832][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 14.0 (TID 151, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,832][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 14.0 (TID 132) in 29 ms on localhost (executor driver) (4/75)
[INFO][2021-06-02 15:46:31,832][org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 14.0 (TID 151)
[INFO][2021-06-02 15:46:31,833][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 14.0 (TID 152, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,833][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,833][org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 14.0 (TID 152)
[INFO][2021-06-02 15:46:31,833][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 14.0 (TID 153, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,833][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,834][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,834][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,834][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 14.0 (TID 154, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,834][org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 14.0 (TID 153)
[INFO][2021-06-02 15:46:31,834][org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 14.0 (TID 154)
[INFO][2021-06-02 15:46:31,834][org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 14.0 (TID 148). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,835][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 14.0 (TID 155, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,835][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 14.0 (TID 146) in 30 ms on localhost (executor driver) (5/75)
[INFO][2021-06-02 15:46:31,835][org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 14.0 (TID 155)
[INFO][2021-06-02 15:46:31,835][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 14.0 (TID 143) in 30 ms on localhost (executor driver) (6/75)
[INFO][2021-06-02 15:46:31,835][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,836][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,836][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,836][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,837][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 14.0 (TID 145) in 32 ms on localhost (executor driver) (7/75)
[INFO][2021-06-02 15:46:31,837][org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 14.0 (TID 149). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,838][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 14.0 (TID 156, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,837][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,838][org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 14.0 (TID 156)
[INFO][2021-06-02 15:46:31,838][org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 14.0 (TID 150). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,838][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,839][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 14.0 (TID 157, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,838][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,838][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,839][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,839][org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 14.0 (TID 157)
[INFO][2021-06-02 15:46:31,839][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 14.0 (TID 131) in 37 ms on localhost (executor driver) (8/75)
[INFO][2021-06-02 15:46:31,840][org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 14.0 (TID 152). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,840][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 14.0 (TID 158, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,841][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 14.0 (TID 159, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,839][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,842][org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 14.0 (TID 159)
[INFO][2021-06-02 15:46:31,841][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 14.0 (TID 160, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,843][org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 14.0 (TID 153). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,841][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,843][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,841][org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 14.0 (TID 158)
[INFO][2021-06-02 15:46:31,840][org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 14.0 (TID 151). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,843][org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 14.0 (TID 160)
[INFO][2021-06-02 15:46:31,843][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 14.0 (TID 161, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,844][org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 14.0 (TID 161)
[INFO][2021-06-02 15:46:31,843][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,846][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,843][org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 14.0 (TID 154). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,846][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,846][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,844][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 14.0 (TID 162, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,847][org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 14.0 (TID 155). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,847][org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 14.0 (TID 156). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,847][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,847][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,847][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,847][org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 14.0 (TID 162)
[INFO][2021-06-02 15:46:31,847][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 14.0 (TID 137) in 43 ms on localhost (executor driver) (9/75)
[INFO][2021-06-02 15:46:31,847][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,847][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,848][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,848][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 14.0 (TID 136) in 44 ms on localhost (executor driver) (10/75)
[INFO][2021-06-02 15:46:31,849][org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 14.0 (TID 157). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,850][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 14.0 (TID 133) in 46 ms on localhost (executor driver) (11/75)
[INFO][2021-06-02 15:46:31,850][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 14.0 (TID 144) in 45 ms on localhost (executor driver) (12/75)
[INFO][2021-06-02 15:46:31,850][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 14.0 (TID 138) in 46 ms on localhost (executor driver) (13/75)
[INFO][2021-06-02 15:46:31,851][org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 14.0 (TID 161). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,850][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,851][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 14.0 (TID 135) in 48 ms on localhost (executor driver) (14/75)
[INFO][2021-06-02 15:46:31,851][org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 14.0 (TID 158). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,851][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,851][org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 14.0 (TID 160). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,852][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 14.0 (TID 163, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,852][org.apache.spark.executor.Executor:54] - Running task 32.0 in stage 14.0 (TID 163)
[INFO][2021-06-02 15:46:31,852][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 14.0 (TID 164, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,852][org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 14.0 (TID 159). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,852][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 14.0 (TID 147) in 34 ms on localhost (executor driver) (15/75)
[INFO][2021-06-02 15:46:31,852][org.apache.spark.executor.Executor:54] - Running task 33.0 in stage 14.0 (TID 164)
[INFO][2021-06-02 15:46:31,853][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 14.0 (TID 139) in 48 ms on localhost (executor driver) (16/75)
[INFO][2021-06-02 15:46:31,853][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 14.0 (TID 134) in 50 ms on localhost (executor driver) (17/75)
[INFO][2021-06-02 15:46:31,853][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 14.0 (TID 148) in 33 ms on localhost (executor driver) (18/75)
[INFO][2021-06-02 15:46:31,853][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 14.0 (TID 165, localhost, executor driver, partition 159, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,854][org.apache.spark.executor.Executor:54] - Running task 34.0 in stage 14.0 (TID 165)
[INFO][2021-06-02 15:46:31,854][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 14.0 (TID 166, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,854][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 14.0 (TID 149) in 24 ms on localhost (executor driver) (19/75)
[INFO][2021-06-02 15:46:31,854][org.apache.spark.executor.Executor:54] - Running task 35.0 in stage 14.0 (TID 166)
[INFO][2021-06-02 15:46:31,854][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 14.0 (TID 150) in 23 ms on localhost (executor driver) (20/75)
[INFO][2021-06-02 15:46:31,855][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 14.0 (TID 167, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,855][org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 14.0 (TID 162). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,855][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,855][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 14.0 (TID 152) in 22 ms on localhost (executor driver) (21/75)
[INFO][2021-06-02 15:46:31,855][org.apache.spark.executor.Executor:54] - Running task 36.0 in stage 14.0 (TID 167)
[INFO][2021-06-02 15:46:31,855][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,855][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 14.0 (TID 168, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,855][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,856][org.apache.spark.executor.Executor:54] - Running task 37.0 in stage 14.0 (TID 168)
[INFO][2021-06-02 15:46:31,856][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,856][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,858][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,856][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 14.0 (TID 169, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,856][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,859][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 14.0 (TID 170, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,858][org.apache.spark.executor.Executor:54] - Running task 38.0 in stage 14.0 (TID 169)
[INFO][2021-06-02 15:46:31,860][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 14.0 (TID 171, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,858][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,861][org.apache.spark.executor.Executor:54] - Running task 40.0 in stage 14.0 (TID 171)
[INFO][2021-06-02 15:46:31,857][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,861][org.apache.spark.executor.Executor:54] - Finished task 33.0 in stage 14.0 (TID 164). 3183 bytes result sent to driver
[INFO][2021-06-02 15:46:31,861][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 14.0 (TID 172, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,861][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,860][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,862][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,860][org.apache.spark.executor.Executor:54] - Running task 39.0 in stage 14.0 (TID 170)
[INFO][2021-06-02 15:46:31,862][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 14.0 (TID 173, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,862][org.apache.spark.executor.Executor:54] - Running task 41.0 in stage 14.0 (TID 172)
[INFO][2021-06-02 15:46:31,863][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,864][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,864][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 14.0 (TID 174, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,864][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,865][org.apache.spark.executor.Executor:54] - Finished task 36.0 in stage 14.0 (TID 167). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,865][org.apache.spark.executor.Executor:54] - Running task 43.0 in stage 14.0 (TID 174)
[INFO][2021-06-02 15:46:31,865][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,865][org.apache.spark.executor.Executor:54] - Finished task 34.0 in stage 14.0 (TID 165). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,865][org.apache.spark.executor.Executor:54] - Finished task 32.0 in stage 14.0 (TID 163). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,865][org.apache.spark.executor.Executor:54] - Running task 42.0 in stage 14.0 (TID 173)
[INFO][2021-06-02 15:46:31,864][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 14.0 (TID 175, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,866][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,867][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 14.0 (TID 176, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,868][org.apache.spark.executor.Executor:54] - Finished task 37.0 in stage 14.0 (TID 168). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,868][org.apache.spark.executor.Executor:54] - Running task 44.0 in stage 14.0 (TID 175)
[INFO][2021-06-02 15:46:31,869][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,866][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,865][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,869][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
[INFO][2021-06-02 15:46:31,869][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,869][org.apache.spark.executor.Executor:54] - Finished task 35.0 in stage 14.0 (TID 166). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,869][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,871][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
[INFO][2021-06-02 15:46:31,868][org.apache.spark.executor.Executor:54] - Running task 45.0 in stage 14.0 (TID 176)
[INFO][2021-06-02 15:46:31,868][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 14.0 (TID 177, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,872][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 14.0 (TID 178, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,872][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 14.0 (TID 179, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,873][org.apache.spark.executor.Executor:54] - Finished task 41.0 in stage 14.0 (TID 172). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,873][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 14.0 (TID 180, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,874][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,868][org.apache.spark.executor.Executor:54] - Finished task 38.0 in stage 14.0 (TID 169). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,874][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 14.0 (TID 181, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,874][org.apache.spark.executor.Executor:54] - Finished task 43.0 in stage 14.0 (TID 174). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,875][org.apache.spark.executor.Executor:54] - Running task 50.0 in stage 14.0 (TID 181)
[INFO][2021-06-02 15:46:31,874][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,874][org.apache.spark.executor.Executor:54] - Running task 49.0 in stage 14.0 (TID 180)
[INFO][2021-06-02 15:46:31,873][org.apache.spark.executor.Executor:54] - Running task 48.0 in stage 14.0 (TID 179)
[INFO][2021-06-02 15:46:31,873][org.apache.spark.executor.Executor:54] - Running task 47.0 in stage 14.0 (TID 178)
[INFO][2021-06-02 15:46:31,877][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,878][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,878][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,873][org.apache.spark.executor.Executor:54] - Finished task 40.0 in stage 14.0 (TID 171). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,872][org.apache.spark.executor.Executor:54] - Running task 46.0 in stage 14.0 (TID 177)
[INFO][2021-06-02 15:46:31,871][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,879][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 8 ms
[INFO][2021-06-02 15:46:31,871][org.apache.spark.executor.Executor:54] - Finished task 39.0 in stage 14.0 (TID 170). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,881][org.apache.spark.executor.Executor:54] - Finished task 45.0 in stage 14.0 (TID 176). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,881][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,882][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,879][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,878][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,875][org.apache.spark.executor.Executor:54] - Finished task 42.0 in stage 14.0 (TID 173). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,875][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 14.0 (TID 182, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,882][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,883][org.apache.spark.executor.Executor:54] - Running task 51.0 in stage 14.0 (TID 182)
[INFO][2021-06-02 15:46:31,883][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 14.0 (TID 183, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,884][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 14.0 (TID 184, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,884][org.apache.spark.executor.Executor:54] - Running task 52.0 in stage 14.0 (TID 183)
[INFO][2021-06-02 15:46:31,885][org.apache.spark.executor.Executor:54] - Running task 53.0 in stage 14.0 (TID 184)
[INFO][2021-06-02 15:46:31,882][org.apache.spark.executor.Executor:54] - Finished task 50.0 in stage 14.0 (TID 181). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,881][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,886][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,886][org.apache.spark.executor.Executor:54] - Finished task 47.0 in stage 14.0 (TID 178). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,886][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,887][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,887][org.apache.spark.executor.Executor:54] - Finished task 49.0 in stage 14.0 (TID 180). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,886][org.apache.spark.executor.Executor:54] - Finished task 48.0 in stage 14.0 (TID 179). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,884][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 14.0 (TID 185, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,888][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,888][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,888][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 14.0 (TID 186, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,889][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 14.0 (TID 187, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,890][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 14.0 (TID 188, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,891][org.apache.spark.executor.Executor:54] - Running task 57.0 in stage 14.0 (TID 188)
[INFO][2021-06-02 15:46:31,891][org.apache.spark.executor.Executor:54] - Finished task 46.0 in stage 14.0 (TID 177). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,883][org.apache.spark.executor.Executor:54] - Finished task 44.0 in stage 14.0 (TID 175). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,891][org.apache.spark.executor.Executor:54] - Running task 56.0 in stage 14.0 (TID 187)
[INFO][2021-06-02 15:46:31,891][org.apache.spark.executor.Executor:54] - Finished task 51.0 in stage 14.0 (TID 182). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,891][org.apache.spark.executor.Executor:54] - Running task 55.0 in stage 14.0 (TID 186)
[INFO][2021-06-02 15:46:31,891][org.apache.spark.executor.Executor:54] - Running task 54.0 in stage 14.0 (TID 185)
[INFO][2021-06-02 15:46:31,891][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 14.0 (TID 189, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,888][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,893][org.apache.spark.executor.Executor:54] - Finished task 53.0 in stage 14.0 (TID 184). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,893][org.apache.spark.executor.Executor:54] - Running task 58.0 in stage 14.0 (TID 189)
[INFO][2021-06-02 15:46:31,893][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 14.0 (TID 154) in 59 ms on localhost (executor driver) (22/75)
[INFO][2021-06-02 15:46:31,893][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,894][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,893][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 15:46:31,894][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,894][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,895][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,894][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 14.0 (TID 156) in 55 ms on localhost (executor driver) (23/75)
[INFO][2021-06-02 15:46:31,895][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,895][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,896][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,897][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,895][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,898][org.apache.spark.executor.Executor:54] - Finished task 57.0 in stage 14.0 (TID 188). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,895][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 14.0 (TID 157) in 57 ms on localhost (executor driver) (24/75)
[INFO][2021-06-02 15:46:31,898][org.apache.spark.executor.Executor:54] - Finished task 54.0 in stage 14.0 (TID 185). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,898][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 14.0 (TID 161) in 55 ms on localhost (executor driver) (25/75)
[INFO][2021-06-02 15:46:31,899][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 14.0 (TID 151) in 67 ms on localhost (executor driver) (26/75)
[INFO][2021-06-02 15:46:31,898][org.apache.spark.executor.Executor:54] - Finished task 56.0 in stage 14.0 (TID 187). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,899][org.apache.spark.executor.Executor:54] - Finished task 52.0 in stage 14.0 (TID 183). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,899][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 14.0 (TID 158) in 59 ms on localhost (executor driver) (27/75)
[INFO][2021-06-02 15:46:31,899][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 14.0 (TID 155) in 65 ms on localhost (executor driver) (28/75)
[INFO][2021-06-02 15:46:31,900][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 14.0 (TID 159) in 59 ms on localhost (executor driver) (29/75)
[INFO][2021-06-02 15:46:31,900][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 14.0 (TID 153) in 67 ms on localhost (executor driver) (30/75)
[INFO][2021-06-02 15:46:31,900][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 14.0 (TID 164) in 48 ms on localhost (executor driver) (31/75)
[INFO][2021-06-02 15:46:31,900][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 14.0 (TID 190, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,901][org.apache.spark.executor.Executor:54] - Finished task 58.0 in stage 14.0 (TID 189). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,901][org.apache.spark.executor.Executor:54] - Finished task 55.0 in stage 14.0 (TID 186). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,901][org.apache.spark.executor.Executor:54] - Running task 59.0 in stage 14.0 (TID 190)
[INFO][2021-06-02 15:46:31,901][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 14.0 (TID 160) in 60 ms on localhost (executor driver) (32/75)
[INFO][2021-06-02 15:46:31,902][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 14.0 (TID 191, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,902][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 14.0 (TID 162) in 58 ms on localhost (executor driver) (33/75)
[INFO][2021-06-02 15:46:31,902][org.apache.spark.executor.Executor:54] - Running task 60.0 in stage 14.0 (TID 191)
[INFO][2021-06-02 15:46:31,902][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 14.0 (TID 167) in 48 ms on localhost (executor driver) (34/75)
[INFO][2021-06-02 15:46:31,903][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 14.0 (TID 165) in 50 ms on localhost (executor driver) (35/75)
[INFO][2021-06-02 15:46:31,903][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 14.0 (TID 166) in 49 ms on localhost (executor driver) (36/75)
[INFO][2021-06-02 15:46:31,903][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 14.0 (TID 168) in 48 ms on localhost (executor driver) (37/75)
[INFO][2021-06-02 15:46:31,904][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,904][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 14.0 (TID 163) in 53 ms on localhost (executor driver) (38/75)
[INFO][2021-06-02 15:46:31,904][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,904][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 41.0 in stage 14.0 (TID 172) in 43 ms on localhost (executor driver) (39/75)
[INFO][2021-06-02 15:46:31,905][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 14.0 (TID 192, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,905][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,905][org.apache.spark.executor.Executor:54] - Running task 61.0 in stage 14.0 (TID 192)
[INFO][2021-06-02 15:46:31,905][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 14.0 (TID 169) in 49 ms on localhost (executor driver) (40/75)
[INFO][2021-06-02 15:46:31,905][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,905][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 43.0 in stage 14.0 (TID 174) in 42 ms on localhost (executor driver) (41/75)
[INFO][2021-06-02 15:46:31,906][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 45.0 in stage 14.0 (TID 176) in 39 ms on localhost (executor driver) (42/75)
[INFO][2021-06-02 15:46:31,906][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 40.0 in stage 14.0 (TID 171) in 46 ms on localhost (executor driver) (43/75)
[INFO][2021-06-02 15:46:31,906][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 14.0 (TID 170) in 48 ms on localhost (executor driver) (44/75)
[INFO][2021-06-02 15:46:31,906][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 50.0 in stage 14.0 (TID 181) in 32 ms on localhost (executor driver) (45/75)
[INFO][2021-06-02 15:46:31,907][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 14.0 (TID 193, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,907][org.apache.spark.executor.Executor:54] - Running task 62.0 in stage 14.0 (TID 193)
[INFO][2021-06-02 15:46:31,907][org.apache.spark.executor.Executor:54] - Finished task 59.0 in stage 14.0 (TID 190). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,907][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 14.0 (TID 194, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,908][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.0 in stage 14.0 (TID 173) in 46 ms on localhost (executor driver) (46/75)
[INFO][2021-06-02 15:46:31,908][org.apache.spark.executor.Executor:54] - Running task 63.0 in stage 14.0 (TID 194)
[INFO][2021-06-02 15:46:31,909][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,909][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 14.0 (TID 195, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,909][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,909][org.apache.spark.executor.Executor:54] - Running task 64.0 in stage 14.0 (TID 195)
[INFO][2021-06-02 15:46:31,909][org.apache.spark.executor.Executor:54] - Finished task 60.0 in stage 14.0 (TID 191). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,909][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 14.0 (TID 196, localhost, executor driver, partition 190, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,910][org.apache.spark.executor.Executor:54] - Running task 65.0 in stage 14.0 (TID 196)
[INFO][2021-06-02 15:46:31,910][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 14.0 (TID 197, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,910][org.apache.spark.executor.Executor:54] - Running task 66.0 in stage 14.0 (TID 197)
[INFO][2021-06-02 15:46:31,911][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 14.0 (TID 198, localhost, executor driver, partition 192, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,912][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,912][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,912][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,912][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,912][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,912][org.apache.spark.executor.Executor:54] - Running task 67.0 in stage 14.0 (TID 198)
[INFO][2021-06-02 15:46:31,912][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 14.0 (TID 199, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,912][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,913][org.apache.spark.executor.Executor:54] - Running task 68.0 in stage 14.0 (TID 199)
[INFO][2021-06-02 15:46:31,914][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,915][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,915][org.apache.spark.executor.Executor:54] - Finished task 61.0 in stage 14.0 (TID 192). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,913][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 14.0 (TID 200, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,916][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,913][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,916][org.apache.spark.executor.Executor:54] - Running task 69.0 in stage 14.0 (TID 200)
[INFO][2021-06-02 15:46:31,916][org.apache.spark.executor.Executor:54] - Finished task 63.0 in stage 14.0 (TID 194). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,916][org.apache.spark.executor.Executor:54] - Finished task 62.0 in stage 14.0 (TID 193). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,916][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,916][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 14.0 (TID 201, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,917][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
[INFO][2021-06-02 15:46:31,918][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 47.0 in stage 14.0 (TID 178) in 47 ms on localhost (executor driver) (47/75)
[INFO][2021-06-02 15:46:31,918][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 44.0 in stage 14.0 (TID 175) in 54 ms on localhost (executor driver) (48/75)
[INFO][2021-06-02 15:46:31,918][org.apache.spark.executor.Executor:54] - Running task 70.0 in stage 14.0 (TID 201)
[INFO][2021-06-02 15:46:31,919][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,919][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,919][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.0 in stage 14.0 (TID 182) in 44 ms on localhost (executor driver) (49/75)
[INFO][2021-06-02 15:46:31,920][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,921][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 14.0 (TID 202, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,920][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,920][org.apache.spark.executor.Executor:54] - Finished task 66.0 in stage 14.0 (TID 197). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,922][org.apache.spark.executor.Executor:54] - Running task 71.0 in stage 14.0 (TID 202)
[INFO][2021-06-02 15:46:31,920][org.apache.spark.executor.Executor:54] - Finished task 64.0 in stage 14.0 (TID 195). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,922][org.apache.spark.executor.Executor:54] - Finished task 67.0 in stage 14.0 (TID 198). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,922][org.apache.spark.executor.Executor:54] - Finished task 65.0 in stage 14.0 (TID 196). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,922][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 14.0 (TID 203, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,923][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,923][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,924][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 14.0 (TID 204, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,924][org.apache.spark.executor.Executor:54] - Running task 72.0 in stage 14.0 (TID 203)
[INFO][2021-06-02 15:46:31,926][org.apache.spark.executor.Executor:54] - Running task 73.0 in stage 14.0 (TID 204)
[INFO][2021-06-02 15:46:31,926][org.apache.spark.executor.Executor:54] - Finished task 69.0 in stage 14.0 (TID 200). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,926][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,925][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 14.0 (TID 205, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
[INFO][2021-06-02 15:46:31,927][org.apache.spark.executor.Executor:54] - Finished task 68.0 in stage 14.0 (TID 199). 3140 bytes result sent to driver
[INFO][2021-06-02 15:46:31,926][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,927][org.apache.spark.executor.Executor:54] - Running task 74.0 in stage 14.0 (TID 205)
[INFO][2021-06-02 15:46:31,927][org.apache.spark.executor.Executor:54] - Finished task 70.0 in stage 14.0 (TID 201). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,928][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 46.0 in stage 14.0 (TID 177) in 60 ms on localhost (executor driver) (50/75)
[INFO][2021-06-02 15:46:31,928][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 49.0 in stage 14.0 (TID 180) in 55 ms on localhost (executor driver) (51/75)
[INFO][2021-06-02 15:46:31,929][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,929][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.0 in stage 14.0 (TID 179) in 57 ms on localhost (executor driver) (52/75)
[INFO][2021-06-02 15:46:31,929][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,929][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,929][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
[INFO][2021-06-02 15:46:31,929][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 57.0 in stage 14.0 (TID 188) in 39 ms on localhost (executor driver) (53/75)
[INFO][2021-06-02 15:46:31,930][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 52.0 in stage 14.0 (TID 183) in 47 ms on localhost (executor driver) (54/75)
[INFO][2021-06-02 15:46:31,930][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 58.0 in stage 14.0 (TID 189) in 39 ms on localhost (executor driver) (55/75)
[INFO][2021-06-02 15:46:31,930][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 2 blocks
[INFO][2021-06-02 15:46:31,931][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
[INFO][2021-06-02 15:46:31,931][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 55.0 in stage 14.0 (TID 186) in 43 ms on localhost (executor driver) (56/75)
[INFO][2021-06-02 15:46:31,931][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 53.0 in stage 14.0 (TID 184) in 48 ms on localhost (executor driver) (57/75)
[INFO][2021-06-02 15:46:31,931][org.apache.spark.executor.Executor:54] - Finished task 71.0 in stage 14.0 (TID 202). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,932][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.0 in stage 14.0 (TID 185) in 48 ms on localhost (executor driver) (58/75)
[INFO][2021-06-02 15:46:31,932][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 56.0 in stage 14.0 (TID 187) in 43 ms on localhost (executor driver) (59/75)
[INFO][2021-06-02 15:46:31,932][org.apache.spark.executor.Executor:54] - Finished task 73.0 in stage 14.0 (TID 204). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,933][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 59.0 in stage 14.0 (TID 190) in 32 ms on localhost (executor driver) (60/75)
[INFO][2021-06-02 15:46:31,932][org.apache.spark.executor.Executor:54] - Finished task 72.0 in stage 14.0 (TID 203). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,933][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 63.0 in stage 14.0 (TID 194) in 26 ms on localhost (executor driver) (61/75)
[INFO][2021-06-02 15:46:31,933][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 60.0 in stage 14.0 (TID 191) in 32 ms on localhost (executor driver) (62/75)
[INFO][2021-06-02 15:46:31,934][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 62.0 in stage 14.0 (TID 193) in 28 ms on localhost (executor driver) (63/75)
[INFO][2021-06-02 15:46:31,934][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.0 in stage 14.0 (TID 197) in 24 ms on localhost (executor driver) (64/75)
[INFO][2021-06-02 15:46:31,934][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 61.0 in stage 14.0 (TID 192) in 30 ms on localhost (executor driver) (65/75)
[INFO][2021-06-02 15:46:31,934][org.apache.spark.executor.Executor:54] - Finished task 74.0 in stage 14.0 (TID 205). 3097 bytes result sent to driver
[INFO][2021-06-02 15:46:31,934][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 64.0 in stage 14.0 (TID 195) in 25 ms on localhost (executor driver) (66/75)
[INFO][2021-06-02 15:46:31,935][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 67.0 in stage 14.0 (TID 198) in 25 ms on localhost (executor driver) (67/75)
[INFO][2021-06-02 15:46:31,935][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 65.0 in stage 14.0 (TID 196) in 26 ms on localhost (executor driver) (68/75)
[INFO][2021-06-02 15:46:31,935][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 68.0 in stage 14.0 (TID 199) in 23 ms on localhost (executor driver) (69/75)
[INFO][2021-06-02 15:46:31,935][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 69.0 in stage 14.0 (TID 200) in 22 ms on localhost (executor driver) (70/75)
[INFO][2021-06-02 15:46:31,936][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 70.0 in stage 14.0 (TID 201) in 20 ms on localhost (executor driver) (71/75)
[INFO][2021-06-02 15:46:31,936][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 73.0 in stage 14.0 (TID 204) in 13 ms on localhost (executor driver) (72/75)
[INFO][2021-06-02 15:46:31,936][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 71.0 in stage 14.0 (TID 202) in 15 ms on localhost (executor driver) (73/75)
[INFO][2021-06-02 15:46:31,936][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.0 in stage 14.0 (TID 203) in 14 ms on localhost (executor driver) (74/75)
[INFO][2021-06-02 15:46:31,936][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 74.0 in stage 14.0 (TID 205) in 12 ms on localhost (executor driver) (75/75)
[INFO][2021-06-02 15:46:31,936][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 15:46:31,936][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 14 (show at UnionALLRegisterTempView.java:29) finished in 0.134 s
[INFO][2021-06-02 15:46:31,937][org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: show at UnionALLRegisterTempView.java:29, took 0.147138 s
[INFO][2021-06-02 15:46:31,960][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 15:46:31,969][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@c9413d8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 15:46:31,972][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 15:46:31,986][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 15:46:32,056][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 15:46:32,057][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 15:46:32,059][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 15:46:32,061][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 15:46:32,065][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 15:46:32,066][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 15:46:32,067][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-a9f875ec-29ba-44a6-8cbf-b41cc7b3f86d
[INFO][2021-06-02 22:26:01,544][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 22:26:01,875][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 22:26:01,895][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 22:26:01,896][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 22:26:01,896][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 22:26:01,897][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 22:26:01,898][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 22:26:03,232][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 58883.
[INFO][2021-06-02 22:26:03,248][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 22:26:03,262][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 22:26:03,265][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 22:26:03,266][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 22:26:03,274][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-be8a76c2-7b44-48ff-b339-7dcb5331b0a3
[INFO][2021-06-02 22:26:03,287][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 22:26:03,322][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 22:26:03,381][org.spark_project.jetty.util.log:192] - Logging initialized @3675ms
[INFO][2021-06-02 22:26:03,451][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 22:26:03,467][org.spark_project.jetty.server.Server:403] - Started @3763ms
[INFO][2021-06-02 22:26:03,489][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@c9413d8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 22:26:03,489][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 22:26:03,508][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@552518c3{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,510][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44a2b17b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,510][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2a76b80a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,511][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61d9efe0{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,512][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e6516e{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,513][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43ed0ff3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,514][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a50b09c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,516][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e5c7f0b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,517][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4de025bf{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,518][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1eef9aef{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,519][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5db99216{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,519][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c1bd44c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,520][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18cc679e{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,521][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c4ca0f9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,522][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7df587ef{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,523][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2755d705{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,523][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@740abb5{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,525][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fe8b721{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,525][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@578524c3{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,526][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e094740{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,532][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc547a{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,533][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61a002b1{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,534][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@780ec4a5{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,535][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ac85b0c{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,535][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3aa3193a{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,537][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 22:26:03,598][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 22:26:03,625][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58924.
[INFO][2021-06-02 22:26:03,626][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:58924
[INFO][2021-06-02 22:26:03,627][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 22:26:03,628][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 58924, None)
[INFO][2021-06-02 22:26:03,631][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:58924 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 58924, None)
[INFO][2021-06-02 22:26:03,634][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 58924, None)
[INFO][2021-06-02 22:26:03,635][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 58924, None)
[INFO][2021-06-02 22:26:03,780][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@66d57c1b{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,807][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 22:26:03,822][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 22:26:03,822][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 22:26:03,827][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5488b5c5{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,827][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@712ca57b{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,828][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27abb83e{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,829][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1a1ed4e5{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:03,830][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7af1cd63{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:26:04,293][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 22:26:04,715][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 22:26:04,760][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 22:26:05,624][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 22:26:05,709][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/4de51033-5cb8-4915-9e98-f3b96c46eff4_resources
[INFO][2021-06-02 22:26:05,715][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/4de51033-5cb8-4915-9e98-f3b96c46eff4
[INFO][2021-06-02 22:26:05,718][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/4de51033-5cb8-4915-9e98-f3b96c46eff4
[INFO][2021-06-02 22:26:05,722][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/4de51033-5cb8-4915-9e98-f3b96c46eff4/_tmp_space.db
[INFO][2021-06-02 22:26:05,725][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 22:26:05,938][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/fd62dc14-e85e-4fe7-95ac-ce5addfb0a9f_resources
[INFO][2021-06-02 22:26:05,942][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/fd62dc14-e85e-4fe7-95ac-ce5addfb0a9f
[INFO][2021-06-02 22:26:05,946][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/fd62dc14-e85e-4fe7-95ac-ce5addfb0a9f
[INFO][2021-06-02 22:26:05,951][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/fd62dc14-e85e-4fe7-95ac-ce5addfb0a9f/_tmp_space.db
[INFO][2021-06-02 22:26:05,953][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 22:26:05,993][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 22:26:05,998][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: SELECT  * from adp_cfg.`info_this_jjjz_etf` WHERE rq = '20210419' LIMIT 10
[INFO][2021-06-02 22:26:06,369][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:06,380][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:06,381][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:06,381][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:06,382][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:06,383][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:06,384][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:06,384][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:06,384][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:06,385][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:06,385][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:06,386][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:06,386][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:06,386][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:06,387][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:06,387][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:06,387][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:06,388][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:06,388][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:06,388][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:06,388][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:06,389][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:06,389][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:06,389][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:06,390][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:06,390][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:06,400][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 22:26:08,036][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf
[INFO][2021-06-02 22:26:08,152][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: drop table if exists adp_usr.ids_tmp_sparkJjjzEtf
[WARN][2021-06-02 22:26:08,275][org.apache.spark.sql.execution.command.DropTableCommand:206] - org.apache.spark.sql.AnalysisException: Table or view not found: `adp_usr`.`ids_tmp_sparkJjjzEtf`;;
'UnresolvedRelation `adp_usr`.`ids_tmp_sparkJjjzEtf`

org.apache.spark.sql.AnalysisException: Table or view not found: `adp_usr`.`ids_tmp_sparkJjjzEtf`;;
'UnresolvedRelation `adp_usr`.`ids_tmp_sparkJjjzEtf`

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:82)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:618)
	at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:203)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at com.apex.bigdata.template.SparkRuntime.exec(SparkRuntime.java:123)
	at com.apex.bigdata.template.SparkRuntime.createHiveTempTable(SparkRuntime.java:138)
	at com.apex.bigdata.spark_01.CreateHiveTempTable.main(CreateHiveTempTable.java:18)
[INFO][2021-06-02 22:26:08,292][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: create table adp_usr.ids_tmp_sparkJjjzEtf  as select * from sparkJjjzEtf
[INFO][2021-06-02 22:26:08,737][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:08,738][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:08,739][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:08,739][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:08,740][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:08,741][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:08,741][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:08,742][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:08,743][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:08,744][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:08,745][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:08,747][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:08,748][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:08,749][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:08,750][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:08,751][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:08,752][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:08,753][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:08,755][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:08,756][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:08,757][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:08,758][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:08,759][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:08,760][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:08,761][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:08,762][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:09,090][org.apache.hadoop.hive.common.FileUtils:501] - Creating directory if it doesn't exist: hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/.hive-staging_hive_2021-06-02_22-26-09_088_869028963339481372-1
[INFO][2021-06-02 22:26:09,478][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-02 22:26:09,485][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: isnotnull(rq#0),(rq#0 = 20210419)
[INFO][2021-06-02 22:26:09,489][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<rq: int, jys: string, jjdm: string, jjlx: decimal(12,0), jjjz: decimal(10,4) ... 23 more fields>
[INFO][2021-06-02 22:26:09,516][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: IsNotNull(rq),EqualTo(rq,20210419)
[INFO][2021-06-02 22:26:09,638][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:09,639][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:09,639][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:09,640][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:09,641][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:09,641][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:09,642][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:09,643][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:09,644][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:09,644][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:09,645][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:09,646][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:09,646][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:09,647][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:09,648][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:09,648][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:09,649][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:09,650][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:09,650][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:09,651][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:09,651][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:09,652][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:09,652][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:09,653][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:09,653][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:09,654][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:10,031][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2021-06-02 22:26:10,678][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 346.6261 ms
[INFO][2021-06-02 22:26:10,804][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 242.5 KB, free 3.8 GB)
[INFO][2021-06-02 22:26:11,109][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.2 KB, free 3.8 GB)
[INFO][2021-06-02 22:26:11,112][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:58924 (size: 24.2 KB, free: 3.8 GB)
[INFO][2021-06-02 22:26:11,115][org.apache.spark.SparkContext:54] - Created broadcast 0 from sql at SparkRuntime.java:123
[INFO][2021-06-02 22:26:11,127][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-02 22:26:11,391][org.apache.spark.SparkContext:54] - Starting job: sql at SparkRuntime.java:123
[INFO][2021-06-02 22:26:11,417][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (sql at SparkRuntime.java:123)
[INFO][2021-06-02 22:26:11,420][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (sql at SparkRuntime.java:123) with 1 output partitions
[INFO][2021-06-02 22:26:11,421][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (sql at SparkRuntime.java:123)
[INFO][2021-06-02 22:26:11,422][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
[INFO][2021-06-02 22:26:11,424][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
[INFO][2021-06-02 22:26:11,432][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at sql at SparkRuntime.java:123), which has no missing parents
[INFO][2021-06-02 22:26:11,543][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 31.0 KB, free 3.8 GB)
[INFO][2021-06-02 22:26:11,552][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.3 KB, free 3.8 GB)
[INFO][2021-06-02 22:26:11,553][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:58924 (size: 11.3 KB, free: 3.8 GB)
[INFO][2021-06-02 22:26:11,553][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 22:26:11,569][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at sql at SparkRuntime.java:123) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[INFO][2021-06-02 22:26:11,570][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 4 tasks
[INFO][2021-06-02 22:26:11,626][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5327 bytes)
[INFO][2021-06-02 22:26:11,630][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5327 bytes)
[INFO][2021-06-02 22:26:11,631][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5327 bytes)
[INFO][2021-06-02 22:26:11,633][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5327 bytes)
[INFO][2021-06-02 22:26:11,648][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-06-02 22:26:11,648][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 0.0 (TID 3)
[INFO][2021-06-02 22:26:11,648][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-02 22:26:11,648][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
[INFO][2021-06-02 22:26:11,783][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210422/000000_0, range: 0-61127, partition values: [20210422]
[INFO][2021-06-02 22:26:11,783][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210419/000000_0, range: 0-71668, partition values: [20210419]
[INFO][2021-06-02 22:26:11,783][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210421/000000_0, range: 0-61127, partition values: [20210421]
[INFO][2021-06-02 22:26:11,783][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210420/000000_0, range: 0-66403, partition values: [20210420]
[INFO][2021-06-02 22:26:13,124][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:26:13,124][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:26:13,124][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:26:13,124][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:26:13,219][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1403 bytes result sent to driver
[INFO][2021-06-02 22:26:13,219][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 1403 bytes result sent to driver
[INFO][2021-06-02 22:26:13,219][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 0.0 (TID 3). 1403 bytes result sent to driver
[INFO][2021-06-02 22:26:13,251][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1621 ms on localhost (executor driver) (1/4)
[INFO][2021-06-02 22:26:13,252][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 1621 ms on localhost (executor driver) (2/4)
[INFO][2021-06-02 22:26:13,252][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 1620 ms on localhost (executor driver) (3/4)
[INFO][2021-06-02 22:26:13,318][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1575 bytes result sent to driver
[INFO][2021-06-02 22:26:13,322][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1715 ms on localhost (executor driver) (4/4)
[INFO][2021-06-02 22:26:13,323][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (sql at SparkRuntime.java:123) finished in 1.736 s
[INFO][2021-06-02 22:26:13,323][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 22:26:13,324][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 22:26:13,325][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 22:26:13,325][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
[INFO][2021-06-02 22:26:13,325][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 22:26:13,328][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at sql at SparkRuntime.java:123), which has no missing parents
[INFO][2021-06-02 22:26:13,348][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 154.6 KB, free 3.8 GB)
[INFO][2021-06-02 22:26:13,353][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 57.8 KB, free 3.8 GB)
[INFO][2021-06-02 22:26:13,354][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.52.10:58924 (size: 57.8 KB, free: 3.8 GB)
[INFO][2021-06-02 22:26:13,355][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 22:26:13,356][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at sql at SparkRuntime.java:123) (first 15 tasks are for partitions Vector(0))
[INFO][2021-06-02 22:26:13,356][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
[INFO][2021-06-02 22:26:13,358][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
[INFO][2021-06-02 22:26:13,358][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 4)
[INFO][2021-06-02 22:26:13,388][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 4 blocks
[INFO][2021-06-02 22:26:13,390][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
[INFO][2021-06-02 22:26:13,403][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2021-06-02 22:26:13,619][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.52.10:58924 in memory (size: 11.3 KB, free: 3.8 GB)
[INFO][2021-06-02 22:26:13,933][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:439] - Saved output of task 'attempt_20210602222613_0001_m_000000_0' to hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/.hive-staging_hive_2021-06-02_22-26-09_088_869028963339481372-1/-ext-10000/_temporary/0/task_20210602222613_0001_m_000000
[INFO][2021-06-02 22:26:13,935][org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20210602222613_0001_m_000000_0: Committed
[INFO][2021-06-02 22:26:13,944][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 4). 1609 bytes result sent to driver
[INFO][2021-06-02 22:26:13,955][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 4) in 598 ms on localhost (executor driver) (1/1)
[INFO][2021-06-02 22:26:13,956][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 22:26:13,956][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (sql at SparkRuntime.java:123) finished in 0.600 s
[INFO][2021-06-02 22:26:13,964][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: sql at SparkRuntime.java:123, took 2.573266 s
[INFO][2021-06-02 22:26:14,006][org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
[INFO][2021-06-02 22:26:14,115][hive.ql.metadata.Hive:2641] - Replacing src:hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/.hive-staging_hive_2021-06-02_22-26-09_088_869028963339481372-1/-ext-10000/part-00000-13b0ea8d-2886-45b2-9d44-76b8f6db9055-c000, dest: hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/part-00000-13b0ea8d-2886-45b2-9d44-76b8f6db9055-c000, Status:true
[INFO][2021-06-02 22:26:14,259][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: `adp_usr`.`ids_tmp_sparkjjjzetf`
[INFO][2021-06-02 22:26:14,427][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,428][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,429][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,429][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:14,430][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,431][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,431][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,432][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,433][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,434][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,435][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,436][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,437][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,438][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,439][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,441][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:14,442][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,443][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,443][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,444][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,445][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,446][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,447][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:14,447][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,448][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:14,449][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,484][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:26:14,514][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,514][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,514][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,515][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:14,515][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,515][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,516][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,516][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,516][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,516][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,516][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,516][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,517][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,517][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,517][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,517][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:14,517][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,517][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,517][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,518][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,518][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,518][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,518][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:14,518][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,519][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:14,519][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,533][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf
[INFO][2021-06-02 22:26:14,536][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:26:14,569][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,569][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,569][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,570][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:14,570][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,570][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,570][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,570][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,570][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,570][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,571][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,571][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,571][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,571][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,571][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,571][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:14,571][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,572][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:14,573][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:14,603][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,604][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,605][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,605][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,605][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:14,606][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,606][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,606][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,606][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,606][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:14,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:14,607][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,618][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:26:14,651][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,652][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,652][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:26:14,652][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:26:14,653][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,653][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,653][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,653][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,654][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,654][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,655][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,656][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,656][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,657][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,657][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,657][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:26:14,658][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,658][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,658][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,658][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:26:14,659][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:26:14,659][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:26:14,659][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:26:14,659][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:26:14,660][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:26:14,660][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:26:14,684][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 22:26:14,688][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@c9413d8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 22:26:14,690][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 22:26:14,700][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 22:26:14,717][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 22:26:14,717][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 22:26:14,719][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 22:26:14,721][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 22:26:14,724][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 22:26:14,724][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 22:26:14,725][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-15a7c05f-b917-4213-acf9-b57b8ff7f0dc
[INFO][2021-06-02 22:30:21,766][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 22:30:22,128][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 22:30:22,146][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 22:30:22,146][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 22:30:22,147][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 22:30:22,147][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 22:30:22,149][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 22:30:23,080][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 58990.
[INFO][2021-06-02 22:30:23,101][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 22:30:23,120][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 22:30:23,123][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 22:30:23,123][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 22:30:23,133][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-7f4b10d1-1f8a-4e6c-a866-3d575a588c80
[INFO][2021-06-02 22:30:23,145][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 22:30:23,180][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 22:30:23,236][org.spark_project.jetty.util.log:192] - Logging initialized @3168ms
[INFO][2021-06-02 22:30:23,311][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 22:30:23,332][org.spark_project.jetty.server.Server:403] - Started @3265ms
[INFO][2021-06-02 22:30:23,368][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@1ef8566a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 22:30:23,369][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 22:30:23,406][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@25f9407e{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,407][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db534f2{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,409][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a56812e{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,411][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f4854d6{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,412][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e70bd39{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,414][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6de54b40{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,415][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@388ffbc2{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,417][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2187fff7{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,418][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21d5c1a0{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,420][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@538613b3{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,421][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11389053{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,422][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ec11999{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,423][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9f46d94{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,424][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e77b8cf{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,425][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@67ef029{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,427][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e57e95e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,428][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@56db847e{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,430][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@560cbf1a{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,431][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@551a20d6{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,432][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64c2b546{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,443][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a11c4c7{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,444][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f093abe{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,446][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4eeea57d{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,448][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72c927f1{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,448][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3dd69f5a{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,451][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 22:30:23,530][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 22:30:23,555][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59032.
[INFO][2021-06-02 22:30:23,556][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:59032
[INFO][2021-06-02 22:30:23,558][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 22:30:23,560][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 59032, None)
[INFO][2021-06-02 22:30:23,564][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:59032 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 59032, None)
[INFO][2021-06-02 22:30:23,569][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 59032, None)
[INFO][2021-06-02 22:30:23,570][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 59032, None)
[INFO][2021-06-02 22:30:23,751][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a9030{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,777][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 22:30:23,792][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 22:30:23,793][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 22:30:23,797][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7bc9e6ab{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,797][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4248ed58{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,797][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@45673f68{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,798][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69e308c6{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:23,799][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1623bbe5{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:30:24,257][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 22:30:24,681][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 22:30:24,738][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 22:30:25,620][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 22:30:25,720][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/31cea60f-2ce8-48ff-b9db-05fed20b0d13_resources
[INFO][2021-06-02 22:30:25,727][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/31cea60f-2ce8-48ff-b9db-05fed20b0d13
[INFO][2021-06-02 22:30:25,730][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/31cea60f-2ce8-48ff-b9db-05fed20b0d13
[INFO][2021-06-02 22:30:25,732][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/31cea60f-2ce8-48ff-b9db-05fed20b0d13/_tmp_space.db
[INFO][2021-06-02 22:30:25,736][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 22:30:25,882][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/37e02d27-3955-4b25-a7b0-b6f017b0cdbd_resources
[INFO][2021-06-02 22:30:25,885][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/37e02d27-3955-4b25-a7b0-b6f017b0cdbd
[INFO][2021-06-02 22:30:25,888][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/37e02d27-3955-4b25-a7b0-b6f017b0cdbd
[INFO][2021-06-02 22:30:25,891][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/37e02d27-3955-4b25-a7b0-b6f017b0cdbd/_tmp_space.db
[INFO][2021-06-02 22:30:25,893][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 22:30:25,934][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 22:30:25,939][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:30:26,267][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:30:26,274][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:30:26,275][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:30:26,276][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:30:26,277][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:30:26,278][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:30:26,279][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:30:26,280][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:30:26,280][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:30:26,280][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:30:26,282][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:30:26,282][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:30:26,283][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:30:26,283][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:30:26,284][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:30:26,284][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:30:26,285][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:30:26,286][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:30:26,286][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:30:26,287][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:30:26,287][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:30:26,288][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:30:26,288][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:30:26,289][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:30:26,289][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:30:26,290][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:30:26,304][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 22:30:28,515][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 199.4903 ms
[INFO][2021-06-02 22:30:29,127][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 228.8 KB, free 3.8 GB)
[INFO][2021-06-02 22:30:29,408][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.6 KB, free 3.8 GB)
[INFO][2021-06-02 22:30:29,412][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:59032 (size: 22.6 KB, free: 3.8 GB)
[INFO][2021-06-02 22:30:29,414][org.apache.spark.SparkContext:54] - Created broadcast 0 from 
[INFO][2021-06-02 22:30:29,722][org.apache.hadoop.mapred.FileInputFormat:247] - Total input paths to process : 1
[INFO][2021-06-02 22:30:29,745][org.apache.spark.SparkContext:54] - Starting job: show at CreateHiveTempTable.java:19
[INFO][2021-06-02 22:30:29,769][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at CreateHiveTempTable.java:19) with 1 output partitions
[INFO][2021-06-02 22:30:29,770][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at CreateHiveTempTable.java:19)
[INFO][2021-06-02 22:30:29,771][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-02 22:30:29,773][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 22:30:29,781][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[4] at show at CreateHiveTempTable.java:19), which has no missing parents
[INFO][2021-06-02 22:30:29,856][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 13.5 KB, free 3.8 GB)
[INFO][2021-06-02 22:30:29,872][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KB, free 3.8 GB)
[INFO][2021-06-02 22:30:29,872][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:59032 (size: 6.3 KB, free: 3.8 GB)
[INFO][2021-06-02 22:30:29,873][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 22:30:29,889][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at CreateHiveTempTable.java:19) (first 15 tasks are for partitions Vector(0))
[INFO][2021-06-02 22:30:29,890][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
[INFO][2021-06-02 22:30:30,007][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4942 bytes)
[INFO][2021-06-02 22:30:30,023][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-02 22:30:30,120][org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/part-00000-13b0ea8d-2886-45b2-9d44-76b8f6db9055-c000:0+1290
[INFO][2021-06-02 22:30:30,261][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 28.5554 ms
[INFO][2021-06-02 22:30:30,979][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1198 bytes result sent to driver
[INFO][2021-06-02 22:30:30,993][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1050 ms on localhost (executor driver) (1/1)
[INFO][2021-06-02 22:30:30,998][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 22:30:31,003][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at CreateHiveTempTable.java:19) finished in 1.081 s
[INFO][2021-06-02 22:30:31,007][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at CreateHiveTempTable.java:19, took 1.261599 s
[INFO][2021-06-02 22:30:31,038][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 22:30:31,045][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@1ef8566a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 22:30:31,047][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 22:30:31,057][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 22:30:31,069][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 22:30:31,069][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 22:30:31,077][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 22:30:31,080][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 22:30:31,087][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 22:30:31,088][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 22:30:31,089][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-c891894a-8b9f-4435-9f40-71be303f440e
[INFO][2021-06-02 22:32:44,510][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 22:32:45,029][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 22:32:45,053][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 22:32:45,054][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 22:32:45,056][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 22:32:45,056][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 22:32:45,057][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 22:32:47,091][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51150.
[INFO][2021-06-02 22:32:47,125][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 22:32:47,150][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 22:32:47,154][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 22:32:47,155][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 22:32:47,168][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-7752bcf7-5209-4be5-934e-ce2b46b80858
[INFO][2021-06-02 22:32:47,185][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 22:32:47,234][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 22:32:47,314][org.spark_project.jetty.util.log:192] - Logging initialized @5406ms
[INFO][2021-06-02 22:32:47,385][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 22:32:47,401][org.spark_project.jetty.server.Server:403] - Started @5495ms
[INFO][2021-06-02 22:32:47,431][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@49341a83{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 22:32:47,431][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 22:32:47,454][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@552518c3{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,455][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44a2b17b{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,456][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2a76b80a{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,457][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61d9efe0{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,458][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e6516e{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,459][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43ed0ff3{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,460][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a50b09c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,461][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e5c7f0b{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,462][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4de025bf{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,463][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1eef9aef{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,464][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5db99216{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,464][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c1bd44c{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,465][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18cc679e{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,466][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c4ca0f9{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,467][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7df587ef{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,468][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2755d705{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,469][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@740abb5{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,470][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fe8b721{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,471][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@578524c3{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,472][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e094740{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,479][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc547a{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,480][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61a002b1{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,482][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@780ec4a5{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,483][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ac85b0c{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,484][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3aa3193a{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,486][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 22:32:47,564][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 22:32:47,599][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51191.
[INFO][2021-06-02 22:32:47,600][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:51191
[INFO][2021-06-02 22:32:47,602][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 22:32:47,604][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 51191, None)
[INFO][2021-06-02 22:32:47,607][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:51191 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 51191, None)
[INFO][2021-06-02 22:32:47,611][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 51191, None)
[INFO][2021-06-02 22:32:47,611][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 51191, None)
[INFO][2021-06-02 22:32:47,783][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@66d57c1b{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,820][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 22:32:47,842][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 22:32:47,842][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 22:32:47,850][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@712ca57b{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,851][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@54534abf{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,852][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1a1ed4e5{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,852][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c6e0a08{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:47,854][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3c2772d1{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 22:32:48,337][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 22:32:48,751][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 22:32:48,808][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 22:32:49,842][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 22:32:49,953][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/f39196fd-80dd-4090-8e91-f6277fd83127_resources
[INFO][2021-06-02 22:32:49,960][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/f39196fd-80dd-4090-8e91-f6277fd83127
[INFO][2021-06-02 22:32:49,964][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/f39196fd-80dd-4090-8e91-f6277fd83127
[INFO][2021-06-02 22:32:49,969][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/f39196fd-80dd-4090-8e91-f6277fd83127/_tmp_space.db
[INFO][2021-06-02 22:32:49,972][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 22:32:50,151][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/44b06891-3b26-41c8-84dc-c89379f24f59_resources
[INFO][2021-06-02 22:32:50,154][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/44b06891-3b26-41c8-84dc-c89379f24f59
[INFO][2021-06-02 22:32:50,157][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/44b06891-3b26-41c8-84dc-c89379f24f59
[INFO][2021-06-02 22:32:50,161][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/44b06891-3b26-41c8-84dc-c89379f24f59/_tmp_space.db
[INFO][2021-06-02 22:32:50,164][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 22:32:50,201][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 22:32:50,207][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: SELECT  * from adp_cfg.`info_this_jjjz_etf` WHERE rq = '20210419' LIMIT 10
[INFO][2021-06-02 22:32:50,532][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:50,544][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:50,545][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:50,545][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:50,545][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:50,547][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:50,547][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:50,548][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:50,548][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:50,549][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:50,549][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:50,550][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:50,550][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:50,550][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:50,551][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:50,551][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:50,551][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:50,552][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:50,552][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:50,552][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:50,553][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:50,553][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:50,553][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:50,554][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:50,554][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:50,554][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:50,564][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 22:32:52,149][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf
[INFO][2021-06-02 22:32:52,266][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: drop table if exists adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:32:52,348][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:52,349][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:52,350][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:52,350][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:52,351][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,352][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,353][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,354][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,354][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,355][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,356][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,358][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,359][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,360][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,361][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:52,362][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:52,363][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,364][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,365][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,366][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,367][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,367][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,369][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:52,369][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:52,371][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:52,372][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:52,434][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:52,436][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:52,437][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:52,438][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:52,438][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,439][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,440][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,440][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,441][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,441][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,442][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,442][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,442][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,443][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,444][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:52,444][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:52,445][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,445][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,445][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,446][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:52,446][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:52,446][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:52,447][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:52,447][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:52,448][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:52,448][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:52,595][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: create table adp_usr.ids_tmp_sparkJjjzEtf  as select * from sparkJjjzEtf
[INFO][2021-06-02 22:32:53,068][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:53,069][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:53,069][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:53,070][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:53,071][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,072][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,072][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,073][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,074][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,075][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,075][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,076][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,077][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,078][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,079][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:53,080][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:53,080][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,081][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,082][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,082][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,083][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,083][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,084][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:53,085][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:53,085][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:53,086][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:53,311][org.apache.hadoop.hive.common.FileUtils:501] - Creating directory if it doesn't exist: hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/.hive-staging_hive_2021-06-02_22-32-53_308_8977523623423086186-1
[INFO][2021-06-02 22:32:53,556][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
[INFO][2021-06-02 22:32:53,560][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: isnotnull(rq#0),(rq#0 = 20210419)
[INFO][2021-06-02 22:32:53,562][org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<rq: int, jys: string, jjdm: string, jjlx: decimal(12,0), jjjz: decimal(10,4) ... 23 more fields>
[INFO][2021-06-02 22:32:53,571][org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: IsNotNull(rq),EqualTo(rq,20210419)
[INFO][2021-06-02 22:32:53,669][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:53,670][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:53,671][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:53,672][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:53,672][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:53,673][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,674][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,675][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,676][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,676][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,677][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,678][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,678][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,679][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,680][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,680][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:53,681][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:53,681][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,682][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,683][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,684][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:53,684][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:53,685][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:53,686][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:53,686][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:53,687][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:54,105][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2021-06-02 22:32:54,650][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 299.739 ms
[INFO][2021-06-02 22:32:54,766][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 242.5 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:55,003][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.2 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:55,016][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.52.10:51191 (size: 24.2 KB, free: 3.8 GB)
[INFO][2021-06-02 22:32:55,019][org.apache.spark.SparkContext:54] - Created broadcast 0 from sql at SparkRuntime.java:123
[INFO][2021-06-02 22:32:55,028][org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO][2021-06-02 22:32:55,208][org.apache.spark.SparkContext:54] - Starting job: sql at SparkRuntime.java:123
[INFO][2021-06-02 22:32:55,224][org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (sql at SparkRuntime.java:123)
[INFO][2021-06-02 22:32:55,228][org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (sql at SparkRuntime.java:123) with 1 output partitions
[INFO][2021-06-02 22:32:55,229][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (sql at SparkRuntime.java:123)
[INFO][2021-06-02 22:32:55,229][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
[INFO][2021-06-02 22:32:55,233][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
[INFO][2021-06-02 22:32:55,242][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at sql at SparkRuntime.java:123), which has no missing parents
[INFO][2021-06-02 22:32:55,340][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 31.0 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:55,351][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.3 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:55,352][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.52.10:51191 (size: 11.3 KB, free: 3.8 GB)
[INFO][2021-06-02 22:32:55,353][org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 22:32:55,366][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at sql at SparkRuntime.java:123) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[INFO][2021-06-02 22:32:55,367][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 4 tasks
[INFO][2021-06-02 22:32:55,396][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5327 bytes)
[INFO][2021-06-02 22:32:55,398][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5327 bytes)
[INFO][2021-06-02 22:32:55,398][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5327 bytes)
[INFO][2021-06-02 22:32:55,399][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5327 bytes)
[INFO][2021-06-02 22:32:55,412][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-06-02 22:32:55,412][org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
[INFO][2021-06-02 22:32:55,412][org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 0.0 (TID 3)
[INFO][2021-06-02 22:32:55,412][org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-06-02 22:32:55,526][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210422/000000_0, range: 0-61127, partition values: [20210422]
[INFO][2021-06-02 22:32:55,526][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210421/000000_0, range: 0-61127, partition values: [20210421]
[INFO][2021-06-02 22:32:55,526][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210420/000000_0, range: 0-66403, partition values: [20210420]
[INFO][2021-06-02 22:32:55,526][org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://node01:8020/user/hive/warehouse/adp_cfg.db/info_this_jjjz_etf/jzrq=20210419/000000_0, range: 0-71668, partition values: [20210419]
[INFO][2021-06-02 22:32:56,807][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:32:56,807][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:32:56,807][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:32:56,807][org.apache.parquet.filter2.compat.FilterCompat:71] - Filtering using predicate: and(noteq(rq, null), eq(rq, 20210419))
[INFO][2021-06-02 22:32:56,934][org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 0.0 (TID 3). 1403 bytes result sent to driver
[INFO][2021-06-02 22:32:56,934][org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1360 bytes result sent to driver
[INFO][2021-06-02 22:32:56,934][org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 1360 bytes result sent to driver
[INFO][2021-06-02 22:32:56,984][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 1584 ms on localhost (executor driver) (1/4)
[INFO][2021-06-02 22:32:56,985][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 1587 ms on localhost (executor driver) (2/4)
[INFO][2021-06-02 22:32:56,986][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1588 ms on localhost (executor driver) (3/4)
[INFO][2021-06-02 22:32:57,052][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1575 bytes result sent to driver
[INFO][2021-06-02 22:32:57,055][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1665 ms on localhost (executor driver) (4/4)
[INFO][2021-06-02 22:32:57,056][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 22:32:57,058][org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (sql at SparkRuntime.java:123) finished in 1.677 s
[INFO][2021-06-02 22:32:57,060][org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
[INFO][2021-06-02 22:32:57,060][org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
[INFO][2021-06-02 22:32:57,061][org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
[INFO][2021-06-02 22:32:57,062][org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
[INFO][2021-06-02 22:32:57,067][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at sql at SparkRuntime.java:123), which has no missing parents
[INFO][2021-06-02 22:32:57,091][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 154.6 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:57,094][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 57.8 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:57,096][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.52.10:51191 (size: 57.8 KB, free: 3.8 GB)
[INFO][2021-06-02 22:32:57,097][org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 22:32:57,098][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at sql at SparkRuntime.java:123) (first 15 tasks are for partitions Vector(0))
[INFO][2021-06-02 22:32:57,098][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
[INFO][2021-06-02 22:32:57,099][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
[INFO][2021-06-02 22:32:57,100][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 4)
[INFO][2021-06-02 22:32:57,137][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 4 blocks
[INFO][2021-06-02 22:32:57,139][org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
[INFO][2021-06-02 22:32:57,149][org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO][2021-06-02 22:32:57,413][org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.52.10:51191 in memory (size: 11.3 KB, free: 3.8 GB)
[INFO][2021-06-02 22:32:57,421][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:439] - Saved output of task 'attempt_20210602223257_0001_m_000000_0' to hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/.hive-staging_hive_2021-06-02_22-32-53_308_8977523623423086186-1/-ext-10000/_temporary/0/task_20210602223257_0001_m_000000
[INFO][2021-06-02 22:32:57,421][org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20210602223257_0001_m_000000_0: Committed
[INFO][2021-06-02 22:32:57,424][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 4). 1609 bytes result sent to driver
[INFO][2021-06-02 22:32:57,429][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 4) in 331 ms on localhost (executor driver) (1/1)
[INFO][2021-06-02 22:32:57,429][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 22:32:57,429][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (sql at SparkRuntime.java:123) finished in 0.331 s
[INFO][2021-06-02 22:32:57,432][org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: sql at SparkRuntime.java:123, took 2.224385 s
[INFO][2021-06-02 22:32:57,452][org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
[INFO][2021-06-02 22:32:57,496][hive.ql.metadata.Hive:2641] - Replacing src:hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/.hive-staging_hive_2021-06-02_22-32-53_308_8977523623423086186-1/-ext-10000/part-00000-7ee6e7ba-b1d3-4196-97fa-9dffc65440a6-c000, dest: hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/part-00000-7ee6e7ba-b1d3-4196-97fa-9dffc65440a6-c000, Status:true
[INFO][2021-06-02 22:32:57,571][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: `adp_usr`.`ids_tmp_sparkjjjzetf`
[INFO][2021-06-02 22:32:57,723][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,724][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,725][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,725][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:57,726][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,726][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,726][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,727][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,728][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,729][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,729][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,730][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,730][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,730][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,731][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,731][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:57,732][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,733][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,734][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,735][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,736][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,736][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,737][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:57,738][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,738][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:57,739][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,771][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:32:57,799][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,799][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,800][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,800][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:57,800][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,800][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,800][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,800][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,800][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:57,801][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,802][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,802][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,802][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,803][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,803][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,803][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:57,803][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,803][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:57,803][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,818][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: sparkJjjzEtf
[INFO][2021-06-02 22:32:57,822][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:32:57,848][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,849][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,849][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,849][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:57,849][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,850][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,851][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,852][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,852][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:57,852][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,852][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:57,852][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,876][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,876][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,876][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,876][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:57,876][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,877][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,878][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:57,879][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,879][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:57,879][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,888][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:32:57,914][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,915][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,915][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,915][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:57,915][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,915][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,915][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,915][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,916][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,916][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,916][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,916][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,917][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,917][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,917][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,917][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:57,917][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,918][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,918][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,918][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,918][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,919][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,919][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:57,919][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,919][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:57,919][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,948][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,948][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,949][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,950][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,950][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,950][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,950][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,950][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:57,950][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,950][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:57,951][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,959][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select * from adp_usr.ids_tmp_sparkJjjzEtf
[INFO][2021-06-02 22:32:57,985][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:57,985][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,985][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 22:32:57,985][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 22:32:57,985][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,985][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,986][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,986][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,986][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,986][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,986][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,986][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,986][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 22:32:57,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 22:32:57,988][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 22:32:57,988][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 22:32:57,988][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 22:32:57,988][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 22:32:57,988][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 22:32:57,988][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 22:32:58,067][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 31.7226 ms
[INFO][2021-06-02 22:32:58,083][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 228.8 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:58,092][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.6 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:58,093][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.52.10:51191 (size: 22.6 KB, free: 3.8 GB)
[INFO][2021-06-02 22:32:58,093][org.apache.spark.SparkContext:54] - Created broadcast 3 from 
[INFO][2021-06-02 22:32:58,126][org.apache.hadoop.mapred.FileInputFormat:247] - Total input paths to process : 1
[INFO][2021-06-02 22:32:58,133][org.apache.spark.SparkContext:54] - Starting job: show at CreateHiveTempTable.java:21
[INFO][2021-06-02 22:32:58,133][org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateHiveTempTable.java:21) with 1 output partitions
[INFO][2021-06-02 22:32:58,133][org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at CreateHiveTempTable.java:21)
[INFO][2021-06-02 22:32:58,133][org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
[INFO][2021-06-02 22:32:58,134][org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
[INFO][2021-06-02 22:32:58,134][org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[11] at show at CreateHiveTempTable.java:21), which has no missing parents
[INFO][2021-06-02 22:32:58,140][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 13.5 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:58,143][org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.3 KB, free 3.8 GB)
[INFO][2021-06-02 22:32:58,145][org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.52.10:51191 (size: 6.3 KB, free: 3.8 GB)
[INFO][2021-06-02 22:32:58,145][org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO][2021-06-02 22:32:58,145][org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at show at CreateHiveTempTable.java:21) (first 15 tasks are for partitions Vector(0))
[INFO][2021-06-02 22:32:58,146][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
[INFO][2021-06-02 22:32:58,151][org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 4942 bytes)
[INFO][2021-06-02 22:32:58,152][org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 5)
[INFO][2021-06-02 22:32:58,158][org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://node01:8020/user/hive/warehouse/adp_usr.db/ids_tmp_sparkjjjzetf/part-00000-7ee6e7ba-b1d3-4196-97fa-9dffc65440a6-c000:0+1290
[INFO][2021-06-02 22:32:58,199][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 16.7659 ms
[INFO][2021-06-02 22:32:58,213][org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 5). 1198 bytes result sent to driver
[INFO][2021-06-02 22:32:58,213][org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 5) in 67 ms on localhost (executor driver) (1/1)
[INFO][2021-06-02 22:32:58,213][org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-06-02 22:32:58,214][org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at CreateHiveTempTable.java:21) finished in 0.068 s
[INFO][2021-06-02 22:32:58,214][org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateHiveTempTable.java:21, took 0.080570 s
[INFO][2021-06-02 22:32:58,229][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 22:32:58,234][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@49341a83{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 22:32:58,235][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 22:32:58,244][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 22:32:58,258][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 22:32:58,259][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 22:32:58,260][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 22:32:58,262][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 22:32:58,265][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 22:32:58,266][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 22:32:58,266][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-d6da0c5f-9996-4887-9907-1aeb3cbfde3d
[INFO][2021-06-02 23:31:19,756][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 23:31:20,167][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 23:31:20,191][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 23:31:20,191][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 23:31:20,192][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 23:31:20,193][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 23:31:20,194][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 23:31:21,720][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 53778.
[INFO][2021-06-02 23:31:21,737][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 23:31:21,756][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 23:31:21,760][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 23:31:21,761][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 23:31:21,776][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-4c9b392a-89df-42b0-b3c7-d10bf19f645f
[INFO][2021-06-02 23:31:21,801][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 23:31:21,851][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 23:31:21,917][org.spark_project.jetty.util.log:192] - Logging initialized @4709ms
[INFO][2021-06-02 23:31:21,996][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 23:31:22,010][org.spark_project.jetty.server.Server:403] - Started @4804ms
[INFO][2021-06-02 23:31:22,033][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@c650a93{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 23:31:22,033][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 23:31:22,057][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@25f9407e{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,058][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db534f2{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,059][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a56812e{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,060][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f4854d6{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,061][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e70bd39{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,061][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6de54b40{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,062][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@388ffbc2{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,063][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2187fff7{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,064][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21d5c1a0{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,064][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@538613b3{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,065][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11389053{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,065][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ec11999{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,066][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9f46d94{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,067][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e77b8cf{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,068][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@67ef029{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,069][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e57e95e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,069][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@56db847e{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,070][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@560cbf1a{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,071][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@551a20d6{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,072][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64c2b546{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,079][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a11c4c7{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,079][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f093abe{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,081][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4eeea57d{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,081][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72c927f1{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,082][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3dd69f5a{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,084][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 23:31:22,162][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 23:31:22,198][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53819.
[INFO][2021-06-02 23:31:22,199][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:53819
[INFO][2021-06-02 23:31:22,201][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 23:31:22,203][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 53819, None)
[INFO][2021-06-02 23:31:22,207][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:53819 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 53819, None)
[INFO][2021-06-02 23:31:22,211][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 53819, None)
[INFO][2021-06-02 23:31:22,211][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 53819, None)
[INFO][2021-06-02 23:31:22,404][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a9030{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,433][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 23:31:22,450][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 23:31:22,450][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 23:31:22,454][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4248ed58{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,455][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4564e94b{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,455][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69e308c6{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,456][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@667e34b1{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,458][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4351171a{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:31:22,899][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 23:31:23,314][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 23:31:23,384][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 23:31:24,350][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 23:31:24,548][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/085ce5fd-4632-4d62-9008-a3612b695867_resources
[INFO][2021-06-02 23:31:24,558][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/085ce5fd-4632-4d62-9008-a3612b695867
[INFO][2021-06-02 23:31:24,562][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/085ce5fd-4632-4d62-9008-a3612b695867
[INFO][2021-06-02 23:31:24,565][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/085ce5fd-4632-4d62-9008-a3612b695867/_tmp_space.db
[INFO][2021-06-02 23:31:24,570][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 23:31:24,799][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/a4a761aa-4676-4365-aa57-bdc1517b4cac_resources
[INFO][2021-06-02 23:31:24,802][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/a4a761aa-4676-4365-aa57-bdc1517b4cac
[INFO][2021-06-02 23:31:24,805][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/a4a761aa-4676-4365-aa57-bdc1517b4cac
[INFO][2021-06-02 23:31:24,808][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/a4a761aa-4676-4365-aa57-bdc1517b4cac/_tmp_space.db
[INFO][2021-06-02 23:31:24,810][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 23:31:24,846][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 23:31:24,851][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: desc adp_cfg.info_this_jjjz_etf
[INFO][2021-06-02 23:31:26,384][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 23:31:26,399][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 23:31:26,400][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 23:31:26,401][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 23:31:26,401][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 23:31:26,403][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:31:26,404][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:31:26,405][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:31:26,406][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:31:26,407][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:31:26,408][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:31:26,408][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:31:26,409][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:31:26,410][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:31:26,410][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:31:26,411][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 23:31:26,413][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 23:31:26,414][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:31:26,415][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:31:26,416][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:31:26,416][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:31:26,417][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:31:26,418][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:31:26,419][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 23:31:26,419][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 23:31:26,420][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 23:31:26,440][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 23:31:27,120][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 232.1201 ms
[INFO][2021-06-02 23:31:27,129][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 23:31:27,134][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@c650a93{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 23:31:27,136][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 23:31:27,144][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 23:31:27,151][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 23:31:27,152][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 23:31:27,156][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 23:31:27,161][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 23:31:27,163][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 23:31:27,164][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 23:31:27,164][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-7d5c6e5c-ac0a-461d-bea8-91d630f84a17
[INFO][2021-06-02 23:37:56,093][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 23:37:56,608][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 23:37:56,638][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 23:37:56,639][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 23:37:56,640][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 23:37:56,640][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 23:37:56,641][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 23:37:58,011][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52049.
[INFO][2021-06-02 23:37:58,026][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 23:37:58,047][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 23:37:58,050][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 23:37:58,050][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 23:37:58,057][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-94a51426-c76e-4f6e-b3af-7bd3517e8695
[INFO][2021-06-02 23:37:58,069][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 23:37:58,109][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 23:37:58,176][org.spark_project.jetty.util.log:192] - Logging initialized @4622ms
[INFO][2021-06-02 23:37:58,231][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 23:37:58,243][org.spark_project.jetty.server.Server:403] - Started @4690ms
[INFO][2021-06-02 23:37:58,263][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@46074492{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 23:37:58,264][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 23:37:58,284][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59aa20b3{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,284][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2a76b80a{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,285][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f4854d6{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,286][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e6516e{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,287][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43ed0ff3{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,287][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a50b09c{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,288][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6691490c{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,289][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4de025bf{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,290][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1eef9aef{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,291][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5db99216{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,291][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c1bd44c{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,292][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18cc679e{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,292][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c4ca0f9{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,293][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7df587ef{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,294][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2755d705{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,295][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@740abb5{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,296][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fe8b721{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,296][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@578524c3{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,297][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e094740{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,298][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc547a{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,304][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4152d38d{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,304][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@780ec4a5{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,305][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f70f32f{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,306][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3aa3193a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,307][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59a67c3a{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,309][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 23:37:58,365][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 23:37:58,386][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52090.
[INFO][2021-06-02 23:37:58,386][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:52090
[INFO][2021-06-02 23:37:58,388][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 23:37:58,389][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 52090, None)
[INFO][2021-06-02 23:37:58,392][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:52090 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 52090, None)
[INFO][2021-06-02 23:37:58,395][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 52090, None)
[INFO][2021-06-02 23:37:58,395][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 52090, None)
[INFO][2021-06-02 23:37:58,518][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@d59970a{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,547][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 23:37:58,562][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 23:37:58,563][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 23:37:58,568][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@712ca57b{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,569][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@54534abf{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,570][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1a1ed4e5{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,570][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c6e0a08{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:58,572][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3c2772d1{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:37:59,008][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 23:37:59,424][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 23:37:59,453][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 23:38:00,139][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 23:38:00,231][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/559a4e86-346f-4865-9fc5-f5ecf8afe8c8_resources
[INFO][2021-06-02 23:38:00,240][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/559a4e86-346f-4865-9fc5-f5ecf8afe8c8
[INFO][2021-06-02 23:38:00,245][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/559a4e86-346f-4865-9fc5-f5ecf8afe8c8
[INFO][2021-06-02 23:38:00,249][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/559a4e86-346f-4865-9fc5-f5ecf8afe8c8/_tmp_space.db
[INFO][2021-06-02 23:38:00,252][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 23:38:00,519][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/ad1d7922-cebd-4ecc-8bb0-f69a6f8d7a21_resources
[INFO][2021-06-02 23:38:00,523][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/ad1d7922-cebd-4ecc-8bb0-f69a6f8d7a21
[INFO][2021-06-02 23:38:00,526][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/ad1d7922-cebd-4ecc-8bb0-f69a6f8d7a21
[INFO][2021-06-02 23:38:00,530][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/ad1d7922-cebd-4ecc-8bb0-f69a6f8d7a21/_tmp_space.db
[INFO][2021-06-02 23:38:00,532][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 23:38:00,576][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 23:38:00,581][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: desc adp_cfg.info_this_jjjz_etf
[INFO][2021-06-02 23:38:02,089][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 23:38:02,103][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 23:38:02,103][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 23:38:02,105][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 23:38:02,105][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 23:38:02,108][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:38:02,108][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:38:02,109][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:38:02,109][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:38:02,109][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:38:02,109][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:38:02,110][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:38:02,111][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:38:02,112][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:38:02,113][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:38:02,113][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 23:38:02,114][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 23:38:02,116][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:38:02,117][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:38:02,117][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:38:02,118][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:38:02,119][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:38:02,121][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:38:02,121][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 23:38:02,122][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 23:38:02,122][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 23:38:02,146][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 23:38:02,721][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 189.8468 ms
[INFO][2021-06-02 23:38:02,726][com.apex.bigdata.spark_01.CacheHiveTableSchema:90] - cacheHiveTableSchema table:adp_cfg.info_this_jjjz_etffailed!
[ERROR][2021-06-02 23:38:02,726][com.apex.bigdata.spark_01.CacheHiveTableSchema:91] - Running Error : 
java.lang.ArrayIndexOutOfBoundsException: 3
	at com.apex.bigdata.spark_01.CacheHiveTableSchema.cacheHiveTableSchema(CacheHiveTableSchema.java:91)
	at com.apex.bigdata.spark_01.CacheHiveTableSchema.main(CacheHiveTableSchema.java:60)
[INFO][2021-06-02 23:38:02,731][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 23:38:02,739][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@46074492{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 23:38:02,741][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 23:38:02,750][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 23:38:02,762][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 23:38:02,763][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 23:38:02,771][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 23:38:02,777][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 23:38:02,779][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 23:38:02,780][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 23:38:02,780][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-e1b31859-c185-4d00-bd84-44a0494f26fc
[INFO][2021-06-02 23:43:42,733][org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
[INFO][2021-06-02 23:43:43,232][org.apache.spark.SparkContext:54] - Submitted application: iDS-Spark::IdsRunner
[INFO][2021-06-02 23:43:43,258][org.apache.spark.SecurityManager:54] - Changing view acls to: hello
[INFO][2021-06-02 23:43:43,259][org.apache.spark.SecurityManager:54] - Changing modify acls to: hello
[INFO][2021-06-02 23:43:43,259][org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
[INFO][2021-06-02 23:43:43,260][org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
[INFO][2021-06-02 23:43:43,261][org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hello); groups with view permissions: Set(); users  with modify permissions: Set(hello); groups with modify permissions: Set()
[INFO][2021-06-02 23:43:44,889][org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 53603.
[INFO][2021-06-02 23:43:44,931][org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
[INFO][2021-06-02 23:43:44,966][org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
[INFO][2021-06-02 23:43:44,973][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-06-02 23:43:44,975][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
[INFO][2021-06-02 23:43:44,985][org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\14226\AppData\Local\Temp\blockmgr-f123fcc6-9289-4eaa-b4ca-f943feea3f2d
[INFO][2021-06-02 23:43:44,998][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 3.8 GB
[INFO][2021-06-02 23:43:45,043][org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
[INFO][2021-06-02 23:43:45,119][org.spark_project.jetty.util.log:192] - Logging initialized @5116ms
[INFO][2021-06-02 23:43:45,197][org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
[INFO][2021-06-02 23:43:45,222][org.spark_project.jetty.server.Server:403] - Started @5219ms
[INFO][2021-06-02 23:43:45,246][org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@71fe6934{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 23:43:45,247][org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
[INFO][2021-06-02 23:43:45,265][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1a69561c{/jobs,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,266][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a56812e{/jobs/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,267][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7eb01b12{/jobs/job,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,268][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e70bd39{/jobs/job/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,269][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6de54b40{/stages,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,269][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@388ffbc2{/stages/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,270][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4da855dd{/stages/stage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,271][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21d5c1a0{/stages/stage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,272][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@538613b3{/stages/pool,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,273][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11389053{/stages/pool/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,273][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ec11999{/storage,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,274][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9f46d94{/storage/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,274][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e77b8cf{/storage/rdd,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,275][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@67ef029{/storage/rdd/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,276][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e57e95e{/environment,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,278][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@56db847e{/environment/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,278][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@560cbf1a{/executors,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,279][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@551a20d6{/executors/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,280][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64c2b546{/executors/threadDump,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,281][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a11c4c7{/executors/threadDump/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,290][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7555b920{/static,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,291][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4eeea57d{/,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,293][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e24ddd0{/api,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,294][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3dd69f5a{/jobs/job/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,295][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ee4730{/stages/stage/kill,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,297][org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.52.10:4040
[INFO][2021-06-02 23:43:45,357][org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
[INFO][2021-06-02 23:43:45,386][org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53644.
[INFO][2021-06-02 23:43:45,387][org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.52.10:53644
[INFO][2021-06-02 23:43:45,389][org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-06-02 23:43:45,390][org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.52.10, 53644, None)
[INFO][2021-06-02 23:43:45,393][org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.52.10:53644 with 3.8 GB RAM, BlockManagerId(driver, 192.168.52.10, 53644, None)
[INFO][2021-06-02 23:43:45,397][org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.52.10, 53644, None)
[INFO][2021-06-02 23:43:45,397][org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.52.10, 53644, None)
[INFO][2021-06-02 23:43:45,532][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27494e46{/metrics/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,563][org.apache.spark.sql.internal.SharedState:54] - loading hive config file: file:/D:/develop/IntelliJ%20IDEA%202019.1.2/ideaWorkspace/spark-181205/target/classes/hive-site.xml
[INFO][2021-06-02 23:43:45,584][org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs://node01:8020/user/hive/warehouse').
[INFO][2021-06-02 23:43:45,585][org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'hdfs://node01:8020/user/hive/warehouse'.
[INFO][2021-06-02 23:43:45,591][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4564e94b{/SQL,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,591][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51745f40{/SQL/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,592][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@667e34b1{/SQL/execution,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,593][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6dba847b{/SQL/execution/json,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:45,595][org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@37d00a23{/static/sql,null,AVAILABLE,@Spark}
[INFO][2021-06-02 23:43:46,063][org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO][2021-06-02 23:43:46,483][hive.metastore:376] - Trying to connect to metastore with URI thrift://node01:9083
[INFO][2021-06-02 23:43:46,514][hive.metastore:472] - Connected to metastore.
[WARN][2021-06-02 23:43:47,604][org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:117] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
[INFO][2021-06-02 23:43:47,765][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/168ae93b-ca05-49d8-949d-b1ec484a6aa6_resources
[INFO][2021-06-02 23:43:47,778][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/168ae93b-ca05-49d8-949d-b1ec484a6aa6
[INFO][2021-06-02 23:43:47,784][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/168ae93b-ca05-49d8-949d-b1ec484a6aa6
[INFO][2021-06-02 23:43:47,790][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/168ae93b-ca05-49d8-949d-b1ec484a6aa6/_tmp_space.db
[INFO][2021-06-02 23:43:47,797][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 23:43:48,033][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/8eedf98b-47f7-4463-8be0-80d669b84649_resources
[INFO][2021-06-02 23:43:48,036][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/8eedf98b-47f7-4463-8be0-80d669b84649
[INFO][2021-06-02 23:43:48,038][org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/14226/AppData/Local/Temp/hello/8eedf98b-47f7-4463-8be0-80d669b84649
[INFO][2021-06-02 23:43:48,042][org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/hello/8eedf98b-47f7-4463-8be0-80d669b84649/_tmp_space.db
[INFO][2021-06-02 23:43:48,044][org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is hdfs://node01:8020/user/hive/warehouse
[INFO][2021-06-02 23:43:48,083][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
[INFO][2021-06-02 23:43:48,093][org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: desc adp_cfg.info_this_jjjz_etf
[INFO][2021-06-02 23:43:49,961][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 23:43:49,968][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: int
[INFO][2021-06-02 23:43:49,968][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 23:43:49,969][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: string
[INFO][2021-06-02 23:43:49,969][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(12,0)
[INFO][2021-06-02 23:43:49,970][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:43:49,971][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:43:49,971][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:43:49,972][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:43:49,972][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:43:49,972][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:43:49,973][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:43:49,973][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:43:49,974][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:43:49,974][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:43:49,974][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 23:43:49,975][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,3)
[INFO][2021-06-02 23:43:49,975][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:43:49,975][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:43:49,976][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:43:49,976][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(16,2)
[INFO][2021-06-02 23:43:49,976][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,4)
[INFO][2021-06-02 23:43:49,977][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,4)
[INFO][2021-06-02 23:43:49,977][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(10,6)
[INFO][2021-06-02 23:43:49,977][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,6)
[INFO][2021-06-02 23:43:49,978][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: decimal(9,2)
[INFO][2021-06-02 23:43:49,987][org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: array<string>
[INFO][2021-06-02 23:43:50,331][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 175.2705 ms
[INFO][2021-06-02 23:43:50,346][org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
[INFO][2021-06-02 23:43:50,354][org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@71fe6934{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[INFO][2021-06-02 23:43:50,356][org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.52.10:4040
[INFO][2021-06-02 23:43:50,370][org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-06-02 23:43:50,383][org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
[INFO][2021-06-02 23:43:50,384][org.apache.spark.storage.BlockManager:54] - BlockManager stopped
[INFO][2021-06-02 23:43:50,391][org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
[INFO][2021-06-02 23:43:50,396][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
[INFO][2021-06-02 23:43:50,401][org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
[INFO][2021-06-02 23:43:50,401][org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
[INFO][2021-06-02 23:43:50,402][org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\14226\AppData\Local\Temp\spark-596a833e-2c11-47e9-8d24-7c52274a0160
